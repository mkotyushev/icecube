{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Training DynEdge\n","\n","Now that both database and selection is ready, everything is in place to begin training. DynEdge is a GNN implemented in GraphNeT - it represents IceCube events as 3D point clouds and leverages techniques from segmentation analysis in computer vision to reconstruct events. You can find technical details on the model in [this paper](https://iopscience.iop.org/article/10.1088/1748-0221/17/11/P11003). The model and training configuration shown below is nearly identical to what's presented in the paper. Note that this configuration was originally meant for low energy, so it's possible that some adjustments might improve performance."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T08:35:50.269123Z","iopub.status.busy":"2023-02-03T08:35:50.268509Z","iopub.status.idle":"2023-02-03T08:35:51.121812Z","shell.execute_reply":"2023-02-03T08:35:51.120342Z","shell.execute_reply.started":"2023-02-03T08:35:50.269085Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-01 12:08:05 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230301-120805.log\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/root/miniconda3/envs/graphnet/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-03-01 12:08:12 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from copy import deepcopy\n","from graphnet.data.constants import FEATURES, TRUTH\n","\n","from icecube_utils import (\n","    load_pretrained_model,\n","    inference, \n",")\n","from wasserstein_ensemble import get_histogram, _get_neuron_importance_histogram, update_model\n","from parameters import get_parser\n","from train_large import config as config_large\n","\n","features = FEATURES.KAGGLE\n","truth = TRUTH.KAGGLE"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from collections import OrderedDict\n","import ot\n","import torch\n","import numpy as np\n","from ground_metric import GroundMetric\n","\n","def get_wassersteinized_layers_modularized(args, networks, T_vars_pre_computed=None, activations=None, eps=1e-7, test_loader=None):\n","    '''\n","    Two neural networks that have to be averaged in geometric manner (i.e. layerwise).\n","    The 1st network is aligned with respect to the other via wasserstein distance.\n","    Also this assumes that all the layers are either fully connected or convolutional *(with no bias)*\n","\n","    :param networks: list of networks\n","    :param activations: If not None, use it to build the activation histograms.\n","    Otherwise assumes uniform distribution over neurons in a layer.\n","    :return: list of layer weights 'wassersteinized', list of transport maps\n","    '''\n","\n","    # simple_model_0, simple_model_1 = networks[0], networks[1]\n","    # simple_model_0 = get_trained_model(0, model='simplenet')\n","    # simple_model_1 = get_trained_model(1, model='simplenet')\n","\n","    avg_aligned_layers, aligned_layers, T_vars = [], OrderedDict(), []\n","    # cumulative_T_var = None\n","    T_var = None\n","    # print(list(networks[0].parameters()))\n","    previous_layer_shape = None\n","    ground_metric_object = GroundMetric(args)\n","\n","    if args.eval_aligned:\n","        model0_aligned_layers = []\n","\n","    if args.gpu_id==-1:\n","        device = torch.device('cpu')\n","    else:\n","        device = torch.device('cuda:{}'.format(args.gpu_id))\n","\n","    network0_named_parameters = networks[0].named_parameters()\n","    network1_named_parameters = networks[1].named_parameters()\n","\n","    num_layers = len(list(zip(networks[0].parameters(), networks[1].parameters())))\n","    for idx, ((layer0_name, fc_layer0_weight), (layer1_name, fc_layer1_weight)) in \\\n","            enumerate(zip(network0_named_parameters, network1_named_parameters)):\n","\n","        # assert fc_layer0_weight.shape == fc_layer1_weight.shape\n","        print(\"Previous layer shape is \", previous_layer_shape)\n","        previous_layer_shape = fc_layer1_weight.shape\n","\n","        mu_cardinality = fc_layer0_weight.shape[0]\n","        nu_cardinality = fc_layer1_weight.shape[0]\n","\n","        # mu = np.ones(fc_layer0_weight.shape[0])/fc_layer0_weight.shape[0]\n","        # nu = np.ones(fc_layer1_weight.shape[0])/fc_layer1_weight.shape[0]\n","\n","        layer_shape = fc_layer0_weight.shape\n","        if len(layer_shape) > 2:\n","            is_conv = True\n","            # For convolutional layers, it is (#out_channels, #in_channels, height, width)\n","            fc_layer0_weight_data = fc_layer0_weight.data.view(fc_layer0_weight.shape[0], fc_layer0_weight.shape[1], -1)\n","            fc_layer1_weight_data = fc_layer1_weight.data.view(fc_layer1_weight.shape[0], fc_layer1_weight.shape[1], -1)\n","        else:\n","            is_conv = False\n","            if len(layer_shape) < 2:  # bias\n","                fc_layer0_weight_data = fc_layer0_weight.data[None, :]\n","                fc_layer1_weight_data = fc_layer1_weight.data[None, :]\n","                # fc_layer0_weight_data = fc_layer0_weight.data[:, None]\n","                # fc_layer1_weight_data = fc_layer1_weight.data[:, None]\n","            else:\n","                fc_layer0_weight_data = fc_layer0_weight.data\n","                fc_layer1_weight_data = fc_layer1_weight.data\n","\n","        print(f'Layer {layer0_name, layer1_name} shape is {fc_layer0_weight_data.shape} and {fc_layer1_weight_data.shape}')\n","\n","        if idx == 0:\n","            if is_conv:\n","                M = ground_metric_object.process(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n","                                fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n","                # M = cost_matrix(fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1),\n","                #                 fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n","            else:\n","                # print(\"layer data is \", fc_layer0_weight_data, fc_layer1_weight_data)\n","                M = ground_metric_object.process(fc_layer0_weight_data, fc_layer1_weight_data)\n","                # M = cost_matrix(fc_layer0_weight, fc_layer1_weight)\n","\n","            aligned_wt = fc_layer0_weight_data\n","        else:\n","\n","            print(\"shape of layer: model 0\", fc_layer0_weight_data.shape)\n","            print(\"shape of layer: model 1\", fc_layer1_weight_data.shape)\n","            print(\"shape of previous transport map\", T_var.shape)\n","\n","            # aligned_wt = None, this caches the tensor and causes OOM\n","            if is_conv:\n","                T_var_conv = T_var.unsqueeze(0).repeat(fc_layer0_weight_data.shape[2], 1, 1)\n","                aligned_wt = torch.bmm(fc_layer0_weight_data.permute(2, 0, 1), T_var_conv).permute(1, 2, 0)\n","\n","                M = ground_metric_object.process(\n","                    aligned_wt.contiguous().view(aligned_wt.shape[0], -1),\n","                    fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1)\n","                )\n","            else:\n","                print(f'fc_layer0_weight_data.shape = {fc_layer0_weight_data.shape}')\n","                print(f'T_var.shape = {T_var.shape}')\n","                if '_post_processing.0' in layer0_name and 'weight' in layer0_name:\n","                    T_var = torch.block_diag(*[val for name, val in T_vars if 'nn.2.weight' in name])\n","                    print(f'T_var.shape = {T_var.shape}')\n","                    aligned_wt = torch.cat([\n","                        fc_layer0_weight_data[:, :17],\n","                        torch.matmul(fc_layer0_weight_data[:, 17:], T_var)\n","                    ], dim=1)\n","                elif 'bias' in layer0_name:\n","                    aligned_wt = torch.matmul(fc_layer0_weight_data, T_var).t()\n","                elif fc_layer0_weight_data.shape[1] != T_var.shape[0]:\n","                    print(fc_layer0_weight_data.shape, T_var.shape)\n","                    # Handles the switch from convolutional layers to fc layers\n","                    fc_layer0_unflattened = fc_layer0_weight_data.view(fc_layer0_weight.shape[0], T_var.shape[0], -1).permute(2, 0, 1)\n","                    aligned_wt = torch.bmm(\n","                        fc_layer0_unflattened,\n","                        T_var.unsqueeze(0).repeat(fc_layer0_unflattened.shape[0], 1, 1)\n","                    ).permute(1, 2, 0)\n","                    aligned_wt = aligned_wt.contiguous().view(aligned_wt.shape[0], -1)\n","                \n","                else:\n","                    # print(\"layer data (aligned) is \", aligned_wt, fc_layer1_weight_data)\n","                    aligned_wt = torch.matmul(fc_layer0_weight_data, T_var)\n","                print(f'aligned_wt shape is {aligned_wt.shape}')\n","                print(f'fc_layer1_weight_data shape is {fc_layer1_weight_data.shape}')\n","                # M = cost_matrix(aligned_wt, fc_layer1_weight)\n","                M = ground_metric_object.process(aligned_wt, fc_layer1_weight_data)\n","                print(\"ground metric is \", M)\n","            if args.skip_last_layer and idx == (num_layers - 1):\n","                print(\"Simple averaging of last layer weights. NO transport map needs to be computed\")\n","                if args.ensemble_step != 0.5:\n","                    avg_aligned_layers.append((1 - args.ensemble_step) * aligned_wt +\n","                                          args.ensemble_step * fc_layer1_weight)\n","                else:\n","                    avg_aligned_layers.append((aligned_wt + fc_layer1_weight)/2)\n","                T_vars.append((layer0_name, torch.diag(torch.ones(fc_layer1_weight.shape[1]))))\n","                aligned_layers[layer0_name] = aligned_wt\n","                return avg_aligned_layers\n","\n","        if args.importance is None or (idx == num_layers -1):\n","            mu = get_histogram(args, 0, mu_cardinality, layer0_name)\n","            nu = get_histogram(args, 1, nu_cardinality, layer1_name)\n","        else:\n","            # mu = _get_neuron_importance_histogram(args, aligned_wt, is_conv)\n","            mu = _get_neuron_importance_histogram(args, fc_layer0_weight_data, is_conv)\n","            nu = _get_neuron_importance_histogram(args, fc_layer1_weight_data, is_conv)\n","            print(mu, nu)\n","            assert args.proper_marginals\n","\n","        if 'bias' in layer0_name:\n","            pass\n","        elif T_vars_pre_computed is None:\n","            cpuM = M.data.cpu().numpy()\n","            print(f'mu shape is {mu.shape}')\n","            print(f'nu shape is {nu.shape}')\n","            print(f'cpuM shape is {cpuM.shape}')\n","            if args.exact:\n","                T = ot.emd(mu, nu, cpuM)\n","            else:\n","                T = ot.bregman.sinkhorn(mu, nu, cpuM, reg=args.reg)\n","            # T = ot.emd(mu, nu, log_cpuM)\n","\n","            if args.gpu_id!=-1:\n","                T_var = torch.from_numpy(T).cuda(args.gpu_id).float()\n","            else:\n","                T_var = torch.from_numpy(T).float()\n","\n","            # torch.set_printoptions(profile=\"full\")\n","            print(\"the transport map is \", T_var)\n","            # torch.set_printoptions(profile=\"default\")\n","\n","            if args.correction:\n","                if not args.proper_marginals:\n","                    # think of it as m x 1, scaling weights for m linear combinations of points in X\n","                    if args.gpu_id != -1:\n","                        # marginals = torch.mv(T_var.t(), torch.ones(T_var.shape[0]).cuda(args.gpu_id))  # T.t().shape[1] = T.shape[0]\n","                        marginals = torch.ones(T_var.shape[0]).cuda(args.gpu_id) / T_var.shape[0]\n","                    else:\n","                        # marginals = torch.mv(T_var.t(),\n","                        #                      torch.ones(T_var.shape[0]))  # T.t().shape[1] = T.shape[0]\n","                        marginals = torch.ones(T_var.shape[0]) / T_var.shape[0]\n","                    marginals = torch.diag(1.0/(marginals + eps))  # take inverse\n","                    T_var = torch.matmul(T_var, marginals)\n","                else:\n","                    # marginals_alpha = T_var @ torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)\n","                    marginals_beta = T_var.t() @ torch.ones(T_var.shape[0], dtype=T_var.dtype).to(device)\n","\n","                    marginals = (1 / (marginals_beta + eps))\n","                    print(\"shape of inverse marginals beta is \", marginals_beta.shape)\n","                    print(\"inverse marginals beta is \", marginals_beta)\n","\n","                    T_var = T_var * marginals\n","                    # i.e., how a neuron of 2nd model is constituted by the neurons of 1st model\n","                    # this should all be ones, and number equal to number of neurons in 2nd model\n","                    print(T_var.sum(dim=0))\n","                    # assert (T_var.sum(dim=0) == torch.ones(T_var.shape[1], dtype=T_var.dtype).to(device)).all()\n","        else:\n","            T_var = T_vars_pre_computed[idx]\n","        \n","        if args.debug:\n","            if idx == (num_layers - 1):\n","                print(\"there goes the last transport map: \\n \", T_var)\n","            else:\n","                print(\"there goes the transport map at layer {}: \\n \".format(idx), T_var)\n","\n","            print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n","\n","        print(\"Ratio of trace to the matrix sum: \", torch.trace(T_var) / torch.sum(T_var))\n","        print(\"Here, trace is {} and matrix sum is {} \".format(torch.trace(T_var), torch.sum(T_var)))\n","        setattr(args, 'trace_sum_ratio_{}'.format(layer0_name), (torch.trace(T_var) / torch.sum(T_var)).item())\n","\n","        if args.past_correction:\n","            print(\"this is past correction for weight mode\")\n","            print(\"Shape of aligned wt is \", aligned_wt.shape)\n","            print(\"Shape of fc_layer0_weight_data is \", fc_layer0_weight_data.shape)\n","            if 'bias' in layer0_name:\n","                # here, no need to align so as T_var is not changed\n","                t_fc0_model = aligned_wt\n","            else:\n","                t_fc0_model = torch.matmul(T_var.t(), aligned_wt.contiguous().view(aligned_wt.shape[0], -1))\n","        else:\n","            t_fc0_model = torch.matmul(T_var.t(), fc_layer0_weight_data.view(fc_layer0_weight_data.shape[0], -1))\n","\n","        # Average the weights of aligned first layers\n","        if args.ensemble_step != 0.5:\n","            geometric_fc = ((1-args.ensemble_step) * t_fc0_model +\n","                            args.ensemble_step * fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))\n","        else:\n","            geometric_fc = (t_fc0_model + fc_layer1_weight_data.view(fc_layer1_weight_data.shape[0], -1))/2\n","        if is_conv and layer_shape != geometric_fc.shape:\n","            geometric_fc = geometric_fc.view(layer_shape)\n","        avg_aligned_layers.append(geometric_fc)\n","        aligned_layers[layer0_name] = t_fc0_model\n","        T_vars.append((layer0_name, T_var))\n","\n","        # get the performance of the model 0 aligned with respect to the model 1\n","        if args.eval_aligned:\n","            if is_conv and layer_shape != t_fc0_model.shape:\n","                t_fc0_model = t_fc0_model.view(layer_shape)\n","            model0_aligned_layers.append(t_fc0_model)\n","            _, acc = update_model(args, networks[0], model0_aligned_layers, test=True,\n","                                  test_loader=test_loader, idx=0)\n","            print(\"For layer idx {}, accuracy of the updated model is {}\".format(idx, acc))\n","            setattr(args, 'model0_aligned_acc_layer_{}'.format(str(idx)), acc)\n","            if idx == (num_layers - 1):\n","                setattr(args, 'model0_aligned_acc', acc)\n","\n","    return avg_aligned_layers, aligned_layers, T_vars"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T08:35:51.127385Z","iopub.status.busy":"2023-02-03T08:35:51.127064Z","iopub.status.idle":"2023-02-03T08:35:51.137833Z","shell.execute_reply":"2023-02-03T08:35:51.136719Z","shell.execute_reply.started":"2023-02-03T08:35:51.127354Z"},"trusted":true},"outputs":[],"source":["# Configuration\n","config_small = {\n","        \"path\": '/workspace/icecube/data/batch_1.db',\n","        \"inference_database_path\": '/workspace/icecube/data/batch_51.db',\n","        \"pulsemap\": 'pulse_table',\n","        \"truth_table\": 'meta_table',\n","        \"features\": features,\n","        \"truth\": truth,\n","        \"index_column\": 'event_id',\n","        \"run_name_tag\": 'my_example',\n","        \"batch_size\": 200,\n","        \"num_workers\": 8,\n","        \"target\": 'direction',\n","        \"early_stopping_patience\": 5,\n","        \"fit\": {\n","                \"max_epochs\": 50,\n","                'max_steps': 0,\n","                \"gpus\": [0],\n","                \"distribution_strategy\": None,\n","                },\n","        'train_selection': '/workspace/icecube/data/train_selection_max_200_pulses.csv',\n","        'validate_selection': '/workspace/icecube/data/validate_selection_max_200_pulses.csv',\n","        'test_selection': None,\n","        'base_dir': 'training',\n","        'dynedge': {}\n","}"]},{"cell_type":"markdown","metadata":{},"source":["In the cell below, you can choose between training dynedge from scratch on the batch_1 database or loading in a pretrained model that has trained on batches 1 to 50."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["model_small = load_pretrained_model(config=config_small, state_dict_path='/workspace/icecube/weights/dynedge_pretrained_batch_1_to_50/state_dict.pth')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_large = load_pretrained_model(config=config_large, state_dict_path='/workspace/icecube/weights/dynedge_pretrained_large/state_dict.pth')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["StandardModel(\n","  (_detector): IceCubeKaggle(\n","    (_graph_builder): KNNGraphBuilder()\n","  )\n","  (_gnn): DynEdge(\n","    (_activation): LeakyReLU(negative_slope=0.01)\n","    (_conv_layers): ModuleList(\n","      (0): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=34, out_features=128, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=128, out_features=256, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (1): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=512, out_features=336, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=336, out_features=256, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (2): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=512, out_features=336, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=336, out_features=256, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (3): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=512, out_features=336, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=336, out_features=256, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","    )\n","    (_post_processing): Sequential(\n","      (0): Linear(in_features=1041, out_features=336, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=336, out_features=256, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","    (_readout): Sequential(\n","      (0): Linear(in_features=768, out_features=128, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (_tasks): ModuleList(\n","    (0): DirectionReconstructionWithKappa(\n","      (_loss_function): VonMisesFisher3DLoss()\n","      (_affine): Linear(in_features=128, out_features=3, bias=True)\n","    )\n","  )\n",")"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model_small"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["StandardModel(\n","  (_detector): IceCubeKaggle(\n","    (_graph_builder): KNNGraphBuilder()\n","  )\n","  (_gnn): DynEdge(\n","    (_activation): LeakyReLU(negative_slope=0.01)\n","    (_conv_layers): ModuleList(\n","      (0): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=34, out_features=256, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=256, out_features=512, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (1): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=1024, out_features=672, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=672, out_features=512, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (2): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=1024, out_features=672, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=672, out_features=512, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","      (3): DynEdgeConv(nn=Sequential(\n","        (0): Linear(in_features=1024, out_features=672, bias=True)\n","        (1): LeakyReLU(negative_slope=0.01)\n","        (2): Linear(in_features=672, out_features=512, bias=True)\n","        (3): LeakyReLU(negative_slope=0.01)\n","      ))\n","    )\n","    (_post_processing): Sequential(\n","      (0): Linear(in_features=2065, out_features=336, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=336, out_features=256, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","    (_readout): Sequential(\n","      (0): Linear(in_features=768, out_features=128, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n","  (_tasks): ModuleList(\n","    (0): DirectionReconstructionWithKappa(\n","      (_loss_function): VonMisesFisher3DLoss()\n","      (_affine): Linear(in_features=128, out_features=3, bias=True)\n","    )\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model_large"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["parser = get_parser()\n","args = parser.parse_args('--gpu-id 1 --model-name mlpnet --n-epochs 10 --save-result-file sample.csv \\\n","--sweep-name exp_sample --exact --correction --ground-metric euclidean --weight-stats \\\n","--activation-histograms --activation-mode raw --geom-ensemble-type acts --sweep-id 21 \\\n","--act-num-samples 200 --ground-metric-normalize none --activation-seed 21 \\\n","--prelu-acts --recheck-acc --load-models ./mnist_models --ckpt-type final \\\n","--past-correction --not-squared --dist-normalize --print-distances --to-download'.split())\n","\n","args.gpu_id = 0\n","args.proper_marginals = True\n","args.skip_last_layer = False"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Previous layer shape is  None\n","Layer ('_gnn._conv_layers.0.nn.0.weight', '_gnn._conv_layers.0.nn.0.weight') shape is torch.Size([128, 34]) and torch.Size([256, 34])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","returns a uniform measure of cardinality:  128\n","returns a uniform measure of cardinality:  256\n","mu shape is (128,)\n","nu shape is (256,)\n","cpuM shape is (128, 256)\n","the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        ...,\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n","       device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([256])\n","inverse marginals beta is  tensor([0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039], device='cuda:0')\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([128, 34])\n","Shape of fc_layer0_weight_data is  torch.Size([128, 34])\n","Previous layer shape is  torch.Size([256, 34])\n","Layer ('_gnn._conv_layers.0.nn.0.bias', '_gnn._conv_layers.0.nn.0.bias') shape is torch.Size([1, 128]) and torch.Size([1, 256])\n","shape of layer: model 0 torch.Size([1, 128])\n","shape of layer: model 1 torch.Size([1, 256])\n","shape of previous transport map torch.Size([128, 256])\n","fc_layer0_weight_data.shape = torch.Size([1, 128])\n","T_var.shape = torch.Size([128, 256])\n","aligned_wt shape is torch.Size([256, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 256])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.2723],\n","        [1.6805],\n","        [1.8960],\n","        [3.2288],\n","        [2.4894],\n","        [1.8039],\n","        [1.6815],\n","        [2.4521],\n","        [1.7248],\n","        [3.6621],\n","        [2.1654],\n","        [1.7127],\n","        [2.1453],\n","        [2.4032],\n","        [3.7231],\n","        [1.7116],\n","        [1.7943],\n","        [3.5728],\n","        [1.6683],\n","        [1.7028],\n","        [1.7604],\n","        [2.0724],\n","        [2.4894],\n","        [4.6704],\n","        [1.7307],\n","        [3.8961],\n","        [1.7239],\n","        [2.7301],\n","        [2.5359],\n","        [2.8568],\n","        [3.1255],\n","        [1.7397],\n","        [2.1088],\n","        [1.6787],\n","        [1.8960],\n","        [2.0335],\n","        [1.7600],\n","        [1.8087],\n","        [3.8961],\n","        [2.3996],\n","        [1.8247],\n","        [2.5855],\n","        [4.3951],\n","        [2.5606],\n","        [2.3209],\n","        [1.8354],\n","        [1.6683],\n","        [1.6983],\n","        [2.3485],\n","        [1.6828],\n","        [2.7295],\n","        [1.7004],\n","        [2.0724],\n","        [4.3469],\n","        [1.6640],\n","        [1.8247],\n","        [2.6803],\n","        [1.7362],\n","        [1.7604],\n","        [3.1044],\n","        [4.1851],\n","        [1.7028],\n","        [3.0854],\n","        [3.1255],\n","        [2.4906],\n","        [3.0194],\n","        [2.3209],\n","        [3.8031],\n","        [4.1851],\n","        [2.1697],\n","        [1.7331],\n","        [1.8415],\n","        [1.7600],\n","        [1.6660],\n","        [3.6621],\n","        [1.7116],\n","        [2.4521],\n","        [1.7700],\n","        [1.7918],\n","        [1.7918],\n","        [2.7244],\n","        [2.4036],\n","        [2.1335],\n","        [2.0477],\n","        [2.4036],\n","        [3.7365],\n","        [4.4235],\n","        [3.1373],\n","        [1.8166],\n","        [3.9999],\n","        [2.5040],\n","        [1.8184],\n","        [1.6727],\n","        [1.7591],\n","        [2.5040],\n","        [1.6798],\n","        [1.8184],\n","        [2.6803],\n","        [3.6760],\n","        [1.6660],\n","        [2.3535],\n","        [3.0194],\n","        [1.6787],\n","        [1.6853],\n","        [2.5908],\n","        [3.8031],\n","        [2.1088],\n","        [3.6160],\n","        [1.9314],\n","        [1.7307],\n","        [1.7362],\n","        [2.2884],\n","        [1.8415],\n","        [2.1697],\n","        [5.1226],\n","        [2.7227],\n","        [1.6690],\n","        [1.7052],\n","        [2.5064],\n","        [2.6931],\n","        [3.5728],\n","        [1.7052],\n","        [4.3382],\n","        [3.9999],\n","        [2.9792],\n","        [2.5606],\n","        [1.6650],\n","        [4.3951],\n","        [2.8729],\n","        [2.5064],\n","        [2.5116],\n","        [2.0299],\n","        [2.0335],\n","        [4.6704],\n","        [2.1453],\n","        [1.7248],\n","        [1.6690],\n","        [2.3582],\n","        [1.8354],\n","        [4.3469],\n","        [1.6640],\n","        [1.6642],\n","        [1.7409],\n","        [2.7227],\n","        [1.6805],\n","        [1.9314],\n","        [2.5577],\n","        [2.7301],\n","        [2.9930],\n","        [2.3535],\n","        [1.7629],\n","        [1.8087],\n","        [2.5116],\n","        [2.2658],\n","        [2.3996],\n","        [2.5908],\n","        [1.6727],\n","        [2.0582],\n","        [1.7409],\n","        [1.6853],\n","        [1.6652],\n","        [4.1841],\n","        [1.7700],\n","        [1.7591],\n","        [1.7943],\n","        [2.9930],\n","        [2.1060],\n","        [2.1371],\n","        [2.2658],\n","        [4.0421],\n","        [2.1335],\n","        [1.7204],\n","        [1.9512],\n","        [3.6160],\n","        [2.0853],\n","        [1.6815],\n","        [2.6128],\n","        [2.4906],\n","        [2.0299],\n","        [1.9971],\n","        [2.1719],\n","        [3.1044],\n","        [2.8568],\n","        [5.1017],\n","        [3.1373],\n","        [2.1719],\n","        [1.6652],\n","        [2.4289],\n","        [2.6013],\n","        [3.0991],\n","        [2.6128],\n","        [2.1654],\n","        [2.5855],\n","        [1.7127],\n","        [1.7397],\n","        [3.1873],\n","        [2.0853],\n","        [1.6798],\n","        [2.8729],\n","        [4.4235],\n","        [4.1841],\n","        [3.4092],\n","        [1.6828],\n","        [3.0854],\n","        [1.7331],\n","        [2.7244],\n","        [2.0773],\n","        [1.7629],\n","        [5.1226],\n","        [3.4092],\n","        [2.0773],\n","        [1.6983],\n","        [2.3485],\n","        [2.4289],\n","        [4.3382],\n","        [4.6054],\n","        [2.1371],\n","        [2.5577],\n","        [2.7295],\n","        [7.2449],\n","        [1.7204],\n","        [2.1060],\n","        [1.7048],\n","        [7.2449],\n","        [1.9576],\n","        [3.7365],\n","        [2.9792],\n","        [4.0421],\n","        [2.2884],\n","        [2.0728],\n","        [2.4032],\n","        [1.7239],\n","        [2.5359],\n","        [4.6054],\n","        [1.9576],\n","        [5.1017],\n","        [1.7004],\n","        [2.0477],\n","        [3.7231],\n","        [1.9512],\n","        [3.6760],\n","        [2.6931],\n","        [2.3582],\n","        [2.0728],\n","        [2.6013],\n","        [3.1873],\n","        [1.8039],\n","        [1.8166],\n","        [1.9971],\n","        [1.7048],\n","        [3.0991],\n","        [2.0582],\n","        [2.2723],\n","        [1.6642],\n","        [3.2288],\n","        [1.6650]], device='cuda:0')\n","returns a uniform measure of cardinality:  128\n","returns a uniform measure of cardinality:  256\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 128])\n","Previous layer shape is  torch.Size([256])\n","Layer ('_gnn._conv_layers.0.nn.2.weight', '_gnn._conv_layers.0.nn.2.weight') shape is torch.Size([256, 128]) and torch.Size([512, 256])\n","shape of layer: model 0 torch.Size([256, 128])\n","shape of layer: model 1 torch.Size([512, 256])\n","shape of previous transport map torch.Size([128, 256])\n","fc_layer0_weight_data.shape = torch.Size([256, 128])\n","T_var.shape = torch.Size([128, 256])\n","aligned_wt shape is torch.Size([256, 256])\n","fc_layer1_weight_data shape is torch.Size([512, 256])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.0302, 0.9373, 0.9634,  ..., 0.9906, 0.9606, 0.9584],\n","        [2.2827, 2.3381, 2.3049,  ..., 2.2950, 2.3595, 2.3590],\n","        [2.3569, 2.3441, 2.3632,  ..., 2.3232, 2.3140, 2.2995],\n","        ...,\n","        [1.2097, 1.2491, 1.1884,  ..., 1.2840, 1.2730, 1.2160],\n","        [1.0778, 1.1347, 1.1060,  ..., 1.1141, 1.1335, 1.0462],\n","        [2.2175, 2.1709, 2.1369,  ..., 2.1160, 2.2039, 2.2061]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","mu shape is (256,)\n","nu shape is (512,)\n","cpuM shape is (256, 512)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([512])\n","inverse marginals beta is  tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n","       device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999],\n","       device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 256])\n","Shape of fc_layer0_weight_data is  torch.Size([256, 128])\n","Previous layer shape is  torch.Size([512, 256])\n","Layer ('_gnn._conv_layers.0.nn.2.bias', '_gnn._conv_layers.0.nn.2.bias') shape is torch.Size([1, 256]) and torch.Size([1, 512])\n","shape of layer: model 0 torch.Size([1, 256])\n","shape of layer: model 1 torch.Size([1, 512])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([1, 256])\n","T_var.shape = torch.Size([256, 512])\n","aligned_wt shape is torch.Size([512, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 512])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[ 2.4355],\n","        [ 2.6425],\n","        [ 0.8669],\n","        [ 0.9543],\n","        [ 2.5723],\n","        [ 0.8320],\n","        [ 1.2958],\n","        [ 7.1601],\n","        [ 1.2893],\n","        [ 3.8866],\n","        [ 7.1009],\n","        [ 1.3796],\n","        [ 3.3461],\n","        [ 4.8078],\n","        [ 3.1962],\n","        [ 0.9543],\n","        [ 6.7674],\n","        [ 4.0819],\n","        [ 2.5128],\n","        [ 1.4260],\n","        [ 1.8745],\n","        [ 1.7175],\n","        [ 3.4109],\n","        [ 4.2727],\n","        [ 1.1571],\n","        [ 1.2548],\n","        [ 3.6085],\n","        [ 2.5467],\n","        [ 2.6618],\n","        [ 5.3217],\n","        [ 4.0592],\n","        [ 2.8200],\n","        [ 1.2548],\n","        [ 6.7674],\n","        [ 1.4701],\n","        [ 7.3630],\n","        [ 4.2817],\n","        [ 5.3158],\n","        [ 1.1329],\n","        [ 2.3542],\n","        [ 4.1765],\n","        [ 1.8588],\n","        [ 1.7149],\n","        [ 4.0178],\n","        [ 5.6226],\n","        [ 0.9989],\n","        [ 5.6288],\n","        [ 4.1635],\n","        [ 3.8367],\n","        [ 6.7370],\n","        [ 2.6856],\n","        [ 0.9827],\n","        [ 2.4642],\n","        [ 1.3630],\n","        [ 0.9753],\n","        [ 2.3581],\n","        [ 4.5817],\n","        [ 1.0464],\n","        [ 2.3491],\n","        [ 2.5342],\n","        [ 4.9497],\n","        [ 1.0449],\n","        [ 5.8122],\n","        [ 1.1989],\n","        [ 1.3630],\n","        [ 1.2893],\n","        [ 0.8694],\n","        [ 3.8432],\n","        [ 2.3853],\n","        [ 1.2768],\n","        [ 1.3457],\n","        [ 3.0478],\n","        [ 5.8122],\n","        [ 1.3064],\n","        [ 1.8879],\n","        [ 2.1995],\n","        [ 1.9503],\n","        [ 2.6383],\n","        [ 1.6340],\n","        [ 2.7165],\n","        [ 4.8222],\n","        [ 2.0778],\n","        [ 0.8475],\n","        [ 5.1406],\n","        [ 5.3217],\n","        [ 1.8371],\n","        [ 2.6411],\n","        [ 2.6020],\n","        [ 4.2727],\n","        [ 2.6311],\n","        [ 3.2993],\n","        [ 1.0771],\n","        [ 1.2717],\n","        [ 0.8315],\n","        [ 2.1202],\n","        [ 5.3369],\n","        [ 5.1824],\n","        [ 0.8523],\n","        [ 1.2768],\n","        [ 0.9294],\n","        [ 0.8475],\n","        [ 0.8236],\n","        [ 1.0820],\n","        [ 2.5029],\n","        [ 4.1630],\n","        [ 6.0088],\n","        [ 0.9827],\n","        [ 1.0387],\n","        [ 3.4459],\n","        [ 2.4642],\n","        [ 2.1312],\n","        [ 1.0464],\n","        [ 4.1765],\n","        [ 3.3461],\n","        [ 1.0226],\n","        [ 1.6814],\n","        [ 1.8775],\n","        [ 5.1824],\n","        [ 1.7149],\n","        [ 3.1085],\n","        [ 1.7017],\n","        [ 3.2993],\n","        [ 4.6614],\n","        [ 1.1973],\n","        [ 0.8912],\n","        [ 3.3143],\n","        [ 1.6349],\n","        [ 5.0764],\n","        [ 2.4981],\n","        [ 0.9990],\n","        [ 2.3491],\n","        [ 1.0693],\n","        [ 3.2362],\n","        [ 2.8745],\n","        [ 0.8814],\n","        [ 0.9473],\n","        [ 6.0183],\n","        [ 4.0819],\n","        [ 1.1764],\n","        [ 1.0666],\n","        [ 1.2545],\n","        [ 3.4368],\n","        [ 2.3509],\n","        [ 2.5338],\n","        [ 4.2259],\n","        [ 1.3173],\n","        [ 6.7370],\n","        [ 1.6665],\n","        [ 0.8372],\n","        [ 4.0956],\n","        [ 3.7827],\n","        [ 1.1329],\n","        [ 1.9503],\n","        [ 0.8232],\n","        [ 3.8866],\n","        [ 2.7294],\n","        [ 1.4864],\n","        [ 1.6086],\n","        [ 3.8432],\n","        [ 1.7421],\n","        [ 1.6081],\n","        [ 1.0570],\n","        [ 1.9039],\n","        [ 3.9871],\n","        [ 2.1063],\n","        [ 4.8078],\n","        [ 4.0178],\n","        [ 1.0666],\n","        [ 3.1967],\n","        [ 0.8577],\n","        [ 2.1312],\n","        [ 6.0183],\n","        [ 0.8873],\n","        [ 1.0706],\n","        [ 2.6020],\n","        [ 0.8258],\n","        [ 0.9455],\n","        [ 0.9455],\n","        [ 3.1967],\n","        [ 2.7051],\n","        [ 0.8236],\n","        [ 3.3725],\n","        [ 0.9473],\n","        [ 1.3949],\n","        [ 6.9319],\n","        [ 2.7511],\n","        [ 2.3916],\n","        [ 4.3305],\n","        [ 4.0816],\n","        [ 4.9497],\n","        [ 6.5224],\n","        [ 3.0443],\n","        [ 3.7827],\n","        [ 1.2717],\n","        [ 7.1009],\n","        [ 2.2516],\n","        [ 1.8775],\n","        [ 0.9841],\n","        [ 1.1030],\n","        [ 1.0530],\n","        [ 1.5792],\n","        [ 0.8669],\n","        [ 4.6655],\n","        [ 2.6577],\n","        [ 3.2362],\n","        [ 0.8826],\n","        [ 5.1970],\n","        [ 1.1679],\n","        [ 4.3420],\n","        [ 1.5885],\n","        [ 1.1617],\n","        [ 3.8367],\n","        [ 2.4697],\n","        [ 3.3167],\n","        [ 1.1962],\n","        [ 2.7165],\n","        [ 1.8879],\n","        [ 2.4697],\n","        [ 2.5093],\n","        [ 0.9993],\n","        [ 2.8745],\n","        [ 0.8912],\n","        [ 3.0443],\n","        [ 0.8577],\n","        [ 4.8383],\n","        [ 4.2817],\n","        [ 1.3147],\n","        [ 3.9879],\n","        [ 0.9631],\n","        [ 2.2516],\n","        [ 4.8383],\n","        [ 1.3617],\n","        [ 2.3058],\n","        [ 3.3143],\n","        [ 1.0820],\n","        [ 1.1679],\n","        [ 3.6988],\n","        [ 0.9631],\n","        [ 1.3147],\n","        [ 1.1764],\n","        [ 0.8523],\n","        [ 1.1617],\n","        [ 1.0599],\n","        [ 2.5342],\n","        [ 4.6158],\n","        [ 1.8475],\n","        [ 1.0906],\n","        [ 1.1973],\n","        [ 1.7913],\n","        [ 1.5976],\n","        [ 1.8371],\n","        [ 4.5817],\n","        [ 1.1962],\n","        [ 4.8222],\n","        [ 2.6411],\n","        [ 1.2926],\n","        [ 1.8588],\n","        [ 3.4109],\n","        [ 3.3744],\n","        [ 1.5542],\n","        [ 5.6372],\n","        [ 3.3744],\n","        [ 1.0570],\n","        [ 2.6593],\n","        [ 2.0778],\n","        [ 0.8313],\n","        [ 6.0088],\n","        [ 1.7175],\n","        [ 1.3949],\n","        [ 0.9333],\n","        [ 0.8826],\n","        [ 2.3598],\n","        [ 2.6856],\n","        [ 2.0239],\n","        [ 2.5338],\n","        [ 6.3773],\n","        [ 3.6983],\n","        [ 4.6655],\n","        [ 3.1962],\n","        [ 4.5869],\n","        [ 4.3420],\n","        [ 5.3158],\n","        [ 2.3130],\n","        [ 1.2640],\n","        [ 1.2958],\n","        [ 0.8313],\n","        [ 3.4368],\n","        [ 1.8745],\n","        [ 2.6383],\n","        [ 1.6340],\n","        [ 2.3542],\n","        [ 3.4534],\n","        [ 1.0906],\n","        [ 2.3058],\n","        [ 4.4060],\n","        [ 1.8475],\n","        [ 3.7456],\n","        [ 1.7751],\n","        [ 0.8806],\n","        [ 2.5215],\n","        [ 5.7513],\n","        [ 5.6372],\n","        [ 1.5976],\n","        [ 1.9474],\n","        [ 0.9548],\n","        [ 2.1210],\n","        [ 1.4044],\n","        [ 4.3305],\n","        [ 1.6081],\n","        [ 3.4459],\n","        [ 2.3598],\n","        [ 1.1605],\n","        [ 4.0816],\n","        [ 2.6618],\n","        [ 1.6814],\n","        [ 2.3916],\n","        [ 0.9534],\n","        [ 2.1202],\n","        [ 2.2845],\n","        [ 2.1063],\n","        [ 1.6086],\n","        [ 1.4085],\n","        [ 0.8372],\n","        [ 1.1989],\n","        [ 2.3853],\n","        [ 1.3064],\n","        [ 2.3581],\n","        [ 0.8258],\n","        [ 0.8358],\n","        [ 1.5542],\n","        [ 5.1970],\n","        [ 3.2899],\n","        [ 1.7913],\n","        [ 1.2250],\n","        [ 1.2260],\n","        [ 2.8806],\n","        [ 1.2926],\n","        [ 3.5042],\n","        [ 1.1865],\n","        [ 4.5869],\n","        [ 0.9989],\n","        [ 1.2545],\n","        [ 1.1873],\n","        [ 1.0449],\n","        [ 5.4562],\n","        [ 4.4060],\n","        [ 2.0435],\n","        [ 1.0530],\n","        [ 2.8133],\n","        [ 2.2845],\n","        [ 5.6226],\n","        [ 2.0292],\n","        [ 2.3932],\n","        [ 0.9990],\n","        [ 5.9774],\n","        [ 2.0239],\n","        [ 1.5792],\n","        [ 2.5093],\n","        [ 2.3509],\n","        [ 8.0444],\n","        [ 5.9774],\n","        [ 0.9294],\n","        [ 1.7421],\n","        [ 1.0771],\n","        [ 2.1863],\n","        [ 8.3136],\n","        [ 4.6158],\n","        [ 8.3136],\n","        [ 1.1735],\n","        [ 2.5432],\n","        [ 2.9288],\n","        [ 2.5215],\n","        [ 0.8358],\n","        [ 2.6311],\n","        [ 1.6686],\n","        [ 5.4562],\n","        [ 1.4085],\n","        [ 1.0702],\n","        [ 1.0387],\n","        [ 1.2260],\n","        [ 2.1534],\n","        [ 3.9871],\n","        [ 5.6288],\n","        [ 1.1605],\n","        [ 2.5432],\n","        [ 2.8133],\n","        [ 2.6425],\n","        [ 1.1735],\n","        [ 2.1210],\n","        [ 1.9039],\n","        [ 2.1534],\n","        [ 1.2250],\n","        [ 3.6988],\n","        [ 1.9474],\n","        [ 1.5967],\n","        [ 3.3167],\n","        [ 0.8873],\n","        [ 1.3796],\n","        [ 5.0764],\n","        [ 5.6460],\n","        [ 1.4701],\n","        [ 3.9879],\n","        [ 1.5967],\n","        [ 2.5128],\n","        [ 2.8200],\n","        [ 6.5224],\n","        [ 1.6665],\n","        [ 1.7751],\n","        [ 1.1873],\n","        [ 2.8806],\n","        [12.5451],\n","        [ 8.0444],\n","        [ 0.8694],\n","        [ 6.3773],\n","        [ 0.8287],\n","        [ 3.5949],\n","        [ 4.1635],\n","        [ 0.8315],\n","        [ 5.3369],\n","        [ 2.5723],\n","        [ 0.8287],\n","        [ 1.7017],\n","        [ 1.3457],\n","        [ 1.1865],\n","        [ 0.9534],\n","        [ 2.5467],\n","        [ 6.4540],\n","        [ 3.3725],\n","        [ 0.9841],\n","        [ 0.8320],\n","        [ 2.2410],\n","        [ 3.4926],\n","        [ 2.7051],\n","        [ 1.1406],\n","        [ 4.0956],\n","        [ 2.7511],\n","        [ 4.6614],\n","        [ 0.9753],\n","        [ 3.6983],\n","        [ 5.1406],\n","        [ 3.1085],\n","        [ 2.1369],\n","        [ 2.5029],\n","        [ 1.1030],\n","        [ 1.0706],\n","        [ 2.4981],\n","        [ 2.1863],\n","        [ 4.1630],\n","        [ 0.8275],\n","        [ 3.5949],\n","        [ 2.3130],\n","        [ 1.4044],\n","        [ 3.5042],\n","        [ 2.7294],\n","        [ 1.1406],\n","        [ 1.6686],\n","        [ 2.9288],\n","        [ 0.8676],\n","        [ 6.4540],\n","        [ 5.7513],\n","        [ 5.6460],\n","        [ 0.8275],\n","        [ 3.1385],\n","        [ 2.6577],\n","        [ 0.8769],\n","        [ 2.0435],\n","        [ 1.5885],\n","        [ 0.8232],\n","        [ 1.0226],\n","        [ 3.0478],\n","        [ 3.4534],\n","        [ 1.8195],\n","        [ 1.0702],\n","        [ 3.2899],\n","        [ 1.3617],\n","        [ 0.8814],\n","        [ 1.8195],\n","        [ 4.0592],\n","        [ 0.9457],\n","        [ 2.6593],\n","        [ 1.2640],\n","        [ 0.9993],\n","        [ 2.1369],\n","        [ 1.1221],\n","        [ 1.4260],\n","        [ 7.3630],\n","        [ 7.1601],\n","        [ 2.4355],\n","        [ 1.4864],\n","        [ 1.0599],\n","        [ 3.6085],\n","        [ 0.8676],\n","        [ 1.3173],\n","        [ 3.1385],\n","        [ 0.9548],\n","        [ 2.2410],\n","        [ 0.9333],\n","        [ 0.9457],\n","        [ 2.3932],\n","        [ 6.9319],\n","        [ 2.0292],\n","        [ 1.1571],\n","        [ 2.1995],\n","        [ 1.1221],\n","        [ 3.7456],\n","        [12.5451],\n","        [ 3.4926],\n","        [ 4.2259],\n","        [ 0.8806],\n","        [ 1.6349],\n","        [ 1.0693],\n","        [ 0.8769]], device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([512, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 256])\n","Previous layer shape is  torch.Size([512])\n","Layer ('_gnn._conv_layers.1.nn.0.weight', '_gnn._conv_layers.1.nn.0.weight') shape is torch.Size([336, 512]) and torch.Size([672, 1024])\n","shape of layer: model 0 torch.Size([336, 512])\n","shape of layer: model 1 torch.Size([672, 1024])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([336, 512])\n","T_var.shape = torch.Size([256, 512])\n","torch.Size([336, 512]) torch.Size([256, 512])\n","aligned_wt shape is torch.Size([336, 1024])\n","fc_layer1_weight_data shape is torch.Size([672, 1024])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.6158, 2.6267, 2.6221,  ..., 2.5865, 2.6108, 2.5660],\n","        [1.7192, 1.7150, 1.7058,  ..., 1.6902, 1.6887, 1.6821],\n","        [3.7287, 3.7040, 3.7030,  ..., 3.6944, 3.7119, 3.7037],\n","        ...,\n","        [2.1899, 2.2069, 2.1775,  ..., 2.1974, 2.1732, 2.1912],\n","        [2.5645, 2.5886, 2.5788,  ..., 2.5736, 2.5782, 2.5466],\n","        [2.7811, 2.7639, 2.7578,  ..., 2.7614, 2.7708, 2.7430]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","mu shape is (336,)\n","nu shape is (672,)\n","cpuM shape is (336, 672)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([672])\n","inverse marginals beta is  tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015], device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0030, device='cuda:0')\n","Here, trace is 1.9998656511306763 and matrix sum is 671.9548950195312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([336, 1024])\n","Shape of fc_layer0_weight_data is  torch.Size([336, 512])\n","Previous layer shape is  torch.Size([672, 1024])\n","Layer ('_gnn._conv_layers.1.nn.0.bias', '_gnn._conv_layers.1.nn.0.bias') shape is torch.Size([1, 336]) and torch.Size([1, 672])\n","shape of layer: model 0 torch.Size([1, 336])\n","shape of layer: model 1 torch.Size([1, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([1, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([672, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[11.8611],\n","        [ 2.0772],\n","        [ 8.2575],\n","        [ 2.0593],\n","        [ 0.7132],\n","        [14.5546],\n","        [15.0427],\n","        [ 4.6537],\n","        [ 3.0829],\n","        [ 1.1941],\n","        [20.9927],\n","        [ 0.4641],\n","        [ 6.7187],\n","        [ 5.8408],\n","        [ 8.2996],\n","        [ 3.6817],\n","        [20.0694],\n","        [ 2.7114],\n","        [ 5.1354],\n","        [ 9.0327],\n","        [ 1.0528],\n","        [ 1.8178],\n","        [11.2963],\n","        [ 0.4703],\n","        [14.0168],\n","        [ 4.6661],\n","        [ 5.9974],\n","        [11.6630],\n","        [ 5.0300],\n","        [ 1.6655],\n","        [ 1.4616],\n","        [11.8660],\n","        [17.5693],\n","        [ 1.0160],\n","        [11.1082],\n","        [ 0.6173],\n","        [ 3.4309],\n","        [58.0853],\n","        [ 2.9592],\n","        [17.5693],\n","        [ 8.0300],\n","        [ 0.4680],\n","        [ 2.3650],\n","        [ 3.0152],\n","        [ 3.9268],\n","        [ 2.4207],\n","        [ 6.7068],\n","        [10.2155],\n","        [ 3.7845],\n","        [ 7.0005],\n","        [ 6.7730],\n","        [ 2.3049],\n","        [16.5068],\n","        [12.3454],\n","        [ 3.5068],\n","        [17.3598],\n","        [ 5.0841],\n","        [ 0.6173],\n","        [ 9.9695],\n","        [11.9880],\n","        [ 8.3336],\n","        [ 4.3797],\n","        [ 0.7610],\n","        [ 5.5369],\n","        [ 9.1982],\n","        [ 2.5064],\n","        [ 2.0397],\n","        [ 1.8919],\n","        [ 1.8038],\n","        [13.4814],\n","        [ 3.9104],\n","        [ 7.4361],\n","        [ 1.3443],\n","        [17.9835],\n","        [ 5.9974],\n","        [ 2.9592],\n","        [ 1.8038],\n","        [ 2.6620],\n","        [24.3990],\n","        [ 9.9695],\n","        [ 2.0298],\n","        [ 8.7954],\n","        [14.5546],\n","        [12.3839],\n","        [ 0.4808],\n","        [18.7143],\n","        [ 4.6865],\n","        [ 5.0896],\n","        [ 0.5097],\n","        [ 6.7625],\n","        [12.0636],\n","        [ 3.2845],\n","        [ 3.1561],\n","        [ 1.1959],\n","        [ 8.2575],\n","        [23.8281],\n","        [20.4381],\n","        [ 9.3738],\n","        [11.8802],\n","        [ 3.3776],\n","        [ 8.3649],\n","        [ 2.0645],\n","        [11.1135],\n","        [ 2.6827],\n","        [ 2.8035],\n","        [20.4381],\n","        [56.3071],\n","        [ 7.7569],\n","        [ 9.7191],\n","        [ 1.7693],\n","        [ 3.8416],\n","        [ 4.0888],\n","        [ 1.1341],\n","        [11.1082],\n","        [15.6807],\n","        [ 6.2127],\n","        [ 0.7947],\n","        [ 9.6613],\n","        [ 1.1652],\n","        [ 3.1422],\n","        [ 3.9104],\n","        [ 4.7644],\n","        [13.5483],\n","        [ 2.4628],\n","        [12.1117],\n","        [ 4.4499],\n","        [ 8.0300],\n","        [ 2.7376],\n","        [ 1.7031],\n","        [ 1.0523],\n","        [17.3598],\n","        [ 5.5873],\n","        [ 3.2845],\n","        [11.8660],\n","        [ 1.0550],\n","        [11.7181],\n","        [ 1.7209],\n","        [ 8.0462],\n","        [12.0088],\n","        [ 2.7704],\n","        [ 4.1082],\n","        [ 3.5115],\n","        [ 1.4538],\n","        [ 1.0348],\n","        [23.7096],\n","        [ 0.7132],\n","        [11.8643],\n","        [ 8.7330],\n","        [ 3.8950],\n","        [ 5.0896],\n","        [ 5.2690],\n","        [ 2.3741],\n","        [ 4.3617],\n","        [ 0.4675],\n","        [ 4.0290],\n","        [45.2995],\n","        [10.8769],\n","        [ 2.4207],\n","        [ 1.6812],\n","        [ 1.4891],\n","        [ 4.7734],\n","        [ 2.6315],\n","        [ 4.6537],\n","        [ 8.5586],\n","        [ 7.4253],\n","        [ 8.5586],\n","        [23.8281],\n","        [ 1.6847],\n","        [ 7.5350],\n","        [ 8.1804],\n","        [21.6316],\n","        [ 3.1561],\n","        [12.7264],\n","        [ 2.0397],\n","        [ 5.9657],\n","        [ 1.7045],\n","        [ 7.5350],\n","        [10.4841],\n","        [ 1.0457],\n","        [45.2995],\n","        [ 3.0662],\n","        [ 8.6377],\n","        [ 1.0918],\n","        [19.6694],\n","        [ 8.3336],\n","        [ 3.3591],\n","        [ 1.7222],\n","        [11.3662],\n","        [ 0.8621],\n","        [20.5299],\n","        [ 9.6936],\n","        [ 2.3650],\n","        [ 3.2103],\n","        [20.1316],\n","        [ 0.4820],\n","        [ 8.2996],\n","        [ 9.0637],\n","        [ 0.8881],\n","        [10.0401],\n","        [13.8006],\n","        [ 5.5974],\n","        [18.8671],\n","        [ 5.5760],\n","        [13.1234],\n","        [ 1.0523],\n","        [ 9.9848],\n","        [ 4.8682],\n","        [13.6429],\n","        [ 6.8840],\n","        [ 1.7031],\n","        [ 6.3917],\n","        [ 7.0248],\n","        [15.9296],\n","        [21.6316],\n","        [15.6807],\n","        [ 1.3826],\n","        [ 0.4675],\n","        [ 7.6608],\n","        [ 1.9015],\n","        [ 2.3276],\n","        [ 3.4223],\n","        [ 6.1453],\n","        [ 1.7222],\n","        [ 5.7320],\n","        [ 5.4595],\n","        [ 7.4361],\n","        [14.5073],\n","        [ 9.0327],\n","        [ 4.3533],\n","        [ 2.2636],\n","        [ 7.2071],\n","        [ 8.1804],\n","        [ 1.1534],\n","        [13.4011],\n","        [ 1.4154],\n","        [23.9557],\n","        [14.7825],\n","        [ 5.7320],\n","        [19.8567],\n","        [10.2155],\n","        [11.7181],\n","        [ 6.1001],\n","        [ 8.7289],\n","        [ 6.3666],\n","        [ 8.7580],\n","        [20.9927],\n","        [ 0.4680],\n","        [ 2.0593],\n","        [ 6.9427],\n","        [ 2.8764],\n","        [17.9835],\n","        [ 5.7080],\n","        [ 5.0992],\n","        [12.5234],\n","        [ 0.6476],\n","        [10.3629],\n","        [ 3.8950],\n","        [13.0984],\n","        [11.3596],\n","        [ 1.8916],\n","        [ 5.1682],\n","        [ 5.6140],\n","        [15.0427],\n","        [12.0609],\n","        [14.4504],\n","        [ 4.2324],\n","        [10.0367],\n","        [ 5.7080],\n","        [18.1157],\n","        [ 0.4703],\n","        [11.9337],\n","        [ 2.4628],\n","        [ 0.4808],\n","        [ 2.0592],\n","        [14.7825],\n","        [ 9.5285],\n","        [ 1.3513],\n","        [ 4.4499],\n","        [ 6.4976],\n","        [ 7.6608],\n","        [ 7.8561],\n","        [ 4.8682],\n","        [ 4.1749],\n","        [13.1234],\n","        [ 5.8133],\n","        [ 0.5808],\n","        [13.5483],\n","        [ 1.9162],\n","        [ 5.8744],\n","        [16.0750],\n","        [ 6.1033],\n","        [ 6.9427],\n","        [ 4.3617],\n","        [20.8277],\n","        [ 1.7720],\n","        [12.7130],\n","        [ 3.4223],\n","        [ 0.7412],\n","        [ 4.8664],\n","        [ 0.8357],\n","        [ 8.6377],\n","        [11.8611],\n","        [18.8671],\n","        [15.5773],\n","        [10.4776],\n","        [ 3.1422],\n","        [ 1.4299],\n","        [16.1979],\n","        [ 1.7045],\n","        [ 7.1530],\n","        [ 3.7845],\n","        [ 6.6537],\n","        [14.7010],\n","        [ 1.3513],\n","        [ 1.2694],\n","        [19.8567],\n","        [ 6.2568],\n","        [ 4.0603],\n","        [ 7.6356],\n","        [ 1.1534],\n","        [12.1117],\n","        [ 9.7149],\n","        [ 7.4794],\n","        [12.7130],\n","        [ 3.5068],\n","        [ 2.3756],\n","        [ 1.1306],\n","        [11.4923],\n","        [22.8732],\n","        [ 3.0829],\n","        [13.0984],\n","        [ 8.6366],\n","        [56.3071],\n","        [12.3454],\n","        [ 0.8621],\n","        [ 6.9832],\n","        [10.4841],\n","        [ 1.2694],\n","        [ 0.7947],\n","        [ 5.5760],\n","        [ 2.7418],\n","        [ 7.1530],\n","        [ 4.2324],\n","        [ 1.7693],\n","        [ 4.8870],\n","        [16.0750],\n","        [ 9.7149],\n","        [12.3839],\n","        [ 0.5419],\n","        [ 4.6865],\n","        [ 5.2690],\n","        [15.8243],\n","        [ 9.2935],\n","        [ 5.0264],\n","        [ 1.4299],\n","        [ 1.0918],\n","        [13.2432],\n","        [ 8.7330],\n","        [ 5.8198],\n","        [ 5.5369],\n","        [ 9.4238],\n","        [25.1825],\n","        [12.0636],\n","        [20.0694],\n","        [23.7096],\n","        [ 5.0992],\n","        [ 9.2766],\n","        [ 7.8533],\n","        [21.9210],\n","        [ 3.2103],\n","        [ 8.7954],\n","        [ 9.3312],\n","        [29.2547],\n","        [16.5068],\n","        [ 8.6790],\n","        [ 0.8392],\n","        [ 1.1335],\n","        [14.4504],\n","        [ 5.9657],\n","        [ 4.1749],\n","        [ 9.2766],\n","        [ 0.8229],\n","        [ 6.1001],\n","        [ 5.8968],\n","        [ 2.8035],\n","        [ 6.9832],\n","        [ 3.5367],\n","        [ 1.6367],\n","        [ 5.0068],\n","        [ 7.0248],\n","        [ 8.6366],\n","        [ 2.8764],\n","        [ 6.9074],\n","        [ 2.5064],\n","        [11.5445],\n","        [ 1.8178],\n","        [ 6.6821],\n","        [ 5.6140],\n","        [18.1096],\n","        [ 9.5285],\n","        [14.0168],\n","        [ 0.4684],\n","        [16.9881],\n","        [ 6.8840],\n","        [ 4.8254],\n","        [ 6.2127],\n","        [ 5.8744],\n","        [ 6.1525],\n","        [12.5234],\n","        [22.8732],\n","        [ 2.5190],\n","        [ 2.6457],\n","        [ 7.1166],\n","        [ 2.2110],\n","        [ 3.8198],\n","        [14.4659],\n","        [ 8.9687],\n","        [ 3.5115],\n","        [ 2.0592],\n","        [ 6.8512],\n","        [ 5.1682],\n","        [ 0.8392],\n","        [14.7010],\n","        [ 4.7734],\n","        [15.7787],\n","        [ 9.2657],\n","        [12.0760],\n","        [16.9881],\n","        [ 9.3738],\n","        [20.8277],\n","        [11.9880],\n","        [ 6.7730],\n","        [ 2.7114],\n","        [ 1.3443],\n","        [ 2.7912],\n","        [ 6.6357],\n","        [ 7.4253],\n","        [ 9.3312],\n","        [ 5.8198],\n","        [ 9.2396],\n","        [ 7.3391],\n","        [ 7.0005],\n","        [ 7.3391],\n","        [11.2963],\n","        [11.1089],\n","        [ 5.3135],\n","        [13.6429],\n","        [ 9.2657],\n","        [ 5.8968],\n","        [ 6.6537],\n","        [ 1.1341],\n","        [11.1089],\n","        [ 0.7610],\n","        [ 6.6357],\n","        [11.4923],\n","        [18.1096],\n","        [ 3.4309],\n","        [14.4266],\n","        [16.5655],\n","        [ 0.6487],\n","        [ 1.6847],\n","        [11.1758],\n","        [18.1157],\n","        [ 6.8512],\n","        [ 4.0603],\n","        [ 9.4238],\n","        [ 9.7191],\n","        [ 1.8252],\n","        [12.0609],\n","        [11.9337],\n","        [ 0.9701],\n","        [ 5.0068],\n","        [10.4776],\n","        [ 5.2541],\n","        [16.7758],\n","        [ 6.7625],\n","        [11.9790],\n","        [ 2.0929],\n","        [ 2.7912],\n","        [13.2058],\n","        [ 1.0550],\n","        [ 8.0295],\n","        [ 2.3741],\n","        [ 5.5873],\n","        [ 3.3776],\n","        [16.7758],\n","        [10.0401],\n","        [14.5073],\n","        [ 4.6661],\n","        [ 1.6367],\n","        [ 1.1941],\n","        [ 1.1821],\n","        [ 5.5513],\n","        [ 5.5974],\n","        [ 2.2110],\n","        [ 0.8005],\n","        [ 6.1646],\n","        [13.2432],\n","        [ 7.2322],\n","        [ 1.8916],\n","        [ 1.0348],\n","        [ 8.9687],\n","        [15.7787],\n","        [20.1316],\n","        [25.1825],\n","        [ 9.2935],\n","        [13.4011],\n","        [16.5655],\n","        [13.2058],\n","        [ 1.3826],\n","        [ 1.4154],\n","        [ 7.2071],\n","        [ 1.0160],\n","        [ 7.2322],\n","        [ 2.2636],\n","        [ 6.1033],\n","        [ 9.6613],\n","        [ 7.7569],\n","        [ 0.4820],\n","        [13.7437],\n","        [ 2.0645],\n","        [ 3.9268],\n","        [13.7437],\n","        [ 2.6827],\n","        [ 6.3917],\n","        [11.9148],\n","        [ 5.8408],\n","        [ 5.5513],\n","        [12.7264],\n","        [ 1.9015],\n","        [ 0.4684],\n","        [ 3.8416],\n","        [ 9.2396],\n","        [ 4.7644],\n","        [ 3.6817],\n","        [ 6.4976],\n","        [ 1.1652],\n","        [14.4659],\n","        [58.0853],\n","        [ 0.5097],\n","        [ 1.4891],\n","        [ 6.1646],\n","        [ 3.8198],\n","        [ 3.3591],\n","        [ 2.7418],\n","        [11.9790],\n","        [ 0.5419],\n","        [ 7.6356],\n","        [ 5.0841],\n","        [ 2.0772],\n","        [13.7136],\n","        [ 2.6315],\n","        [11.6630],\n","        [ 2.6620],\n","        [11.1758],\n","        [11.8802],\n","        [ 2.8618],\n","        [14.4266],\n","        [11.7345],\n","        [12.0760],\n","        [ 2.3049],\n","        [ 1.1959],\n","        [ 0.8229],\n","        [15.5773],\n","        [13.8006],\n","        [ 2.8618],\n","        [ 0.8881],\n","        [ 8.6790],\n","        [ 5.3135],\n","        [ 4.8870],\n","        [ 3.5367],\n","        [21.9210],\n","        [ 9.1982],\n","        [ 4.3533],\n","        [11.3662],\n","        [11.1135],\n","        [11.3596],\n","        [10.0367],\n","        [ 3.0662],\n","        [ 4.0888],\n","        [ 0.9701],\n","        [15.9296],\n","        [ 2.6457],\n","        [ 2.0929],\n","        [ 5.4595],\n","        [ 5.6529],\n","        [ 0.4641],\n","        [ 5.1354],\n","        [20.5299],\n","        [ 4.0290],\n","        [ 6.9074],\n","        [17.2513],\n","        [ 2.3756],\n","        [ 1.0457],\n","        [ 7.1166],\n","        [15.1527],\n","        [ 2.5190],\n","        [ 2.7376],\n","        [ 6.6821],\n","        [ 1.6655],\n","        [ 1.8252],\n","        [ 4.8664],\n","        [13.7637],\n","        [ 5.6529],\n","        [ 1.6812],\n","        [ 8.7580],\n","        [ 6.7068],\n","        [ 1.7720],\n","        [ 1.9162],\n","        [12.0088],\n","        [ 8.7289],\n","        [ 2.2470],\n","        [ 8.0462],\n","        [ 6.1525],\n","        [16.1979],\n","        [ 6.7187],\n","        [19.6694],\n","        [ 1.1821],\n","        [ 8.0295],\n","        [13.7637],\n","        [ 7.4794],\n","        [ 7.8561],\n","        [ 5.8133],\n","        [ 5.0300],\n","        [29.2547],\n","        [ 0.7412],\n","        [ 0.8005],\n","        [10.3629],\n","        [ 2.7704],\n","        [15.8243],\n","        [11.5445],\n","        [ 7.8533],\n","        [ 1.0528],\n","        [11.9148],\n","        [23.9557],\n","        [ 1.7209],\n","        [ 9.9848],\n","        [15.1527],\n","        [13.7136],\n","        [ 5.2541],\n","        [ 0.6487],\n","        [ 3.0152],\n","        [ 1.8919],\n","        [ 1.4538],\n","        [ 0.6476],\n","        [ 9.0637],\n","        [ 8.3649],\n","        [ 2.2470],\n","        [ 6.2568],\n","        [10.8769],\n","        [ 0.8357],\n","        [24.3990],\n","        [ 6.1453],\n","        [ 1.1306],\n","        [ 8.0565],\n","        [17.2513],\n","        [ 2.3276],\n","        [ 6.3666],\n","        [ 0.5808],\n","        [ 4.3797],\n","        [ 8.0565],\n","        [11.7345],\n","        [ 2.0298],\n","        [ 4.8254],\n","        [13.4814],\n","        [11.8643],\n","        [ 1.1335],\n","        [18.7143],\n","        [ 5.0264],\n","        [ 9.6936],\n","        [ 4.1082],\n","        [ 1.4616]], device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","Ratio of trace to the matrix sum:  tensor(0.0030, device='cuda:0')\n","Here, trace is 1.9998656511306763 and matrix sum is 671.9548950195312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([672, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 336])\n","Previous layer shape is  torch.Size([672])\n","Layer ('_gnn._conv_layers.1.nn.2.weight', '_gnn._conv_layers.1.nn.2.weight') shape is torch.Size([256, 336]) and torch.Size([512, 672])\n","shape of layer: model 0 torch.Size([256, 336])\n","shape of layer: model 1 torch.Size([512, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([256, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([256, 672])\n","fc_layer1_weight_data shape is torch.Size([512, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.8759, 2.9099, 2.8789,  ..., 2.8886, 2.8578, 2.8771],\n","        [3.1977, 3.2650, 3.2183,  ..., 3.2185, 3.2058, 3.2109],\n","        [3.4066, 3.4131, 3.4129,  ..., 3.4276, 3.3950, 3.4074],\n","        ...,\n","        [2.5265, 2.5768, 2.5345,  ..., 2.5589, 2.5462, 2.5559],\n","        [2.4843, 2.5108, 2.4857,  ..., 2.4728, 2.4463, 2.4560],\n","        [2.7915, 2.7507, 2.7629,  ..., 2.7607, 2.7530, 2.7845]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","mu shape is (256,)\n","nu shape is (512,)\n","cpuM shape is (256, 512)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([512])\n","inverse marginals beta is  tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n","       device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999],\n","       device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 672])\n","Shape of fc_layer0_weight_data is  torch.Size([256, 336])\n","Previous layer shape is  torch.Size([512, 672])\n","Layer ('_gnn._conv_layers.1.nn.2.bias', '_gnn._conv_layers.1.nn.2.bias') shape is torch.Size([1, 256]) and torch.Size([1, 512])\n","shape of layer: model 0 torch.Size([1, 256])\n","shape of layer: model 1 torch.Size([1, 512])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([1, 256])\n","T_var.shape = torch.Size([256, 512])\n","aligned_wt shape is torch.Size([512, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 512])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[ 1.0719],\n","        [ 3.8178],\n","        [ 2.9230],\n","        [ 0.8735],\n","        [ 0.6225],\n","        [ 3.9548],\n","        [ 4.6946],\n","        [ 0.8911],\n","        [ 8.5759],\n","        [13.8277],\n","        [ 4.8843],\n","        [12.2789],\n","        [ 3.3697],\n","        [ 1.2764],\n","        [ 0.7008],\n","        [12.2789],\n","        [ 2.7117],\n","        [13.1038],\n","        [ 6.2841],\n","        [ 4.2201],\n","        [ 9.7803],\n","        [ 4.6488],\n","        [ 0.5880],\n","        [ 2.6401],\n","        [ 0.7644],\n","        [ 1.9981],\n","        [ 8.6804],\n","        [ 1.2132],\n","        [ 0.8972],\n","        [ 1.1788],\n","        [10.7859],\n","        [ 4.5039],\n","        [ 0.5932],\n","        [ 2.4956],\n","        [ 4.7144],\n","        [ 1.0935],\n","        [11.3902],\n","        [ 0.5580],\n","        [ 2.7408],\n","        [13.3660],\n","        [ 1.2156],\n","        [ 0.5458],\n","        [16.3322],\n","        [ 5.8650],\n","        [12.5500],\n","        [ 4.3520],\n","        [ 9.5457],\n","        [10.0413],\n","        [ 1.6490],\n","        [13.3660],\n","        [ 1.6564],\n","        [ 6.2841],\n","        [ 0.6064],\n","        [10.1426],\n","        [11.0782],\n","        [ 0.5226],\n","        [ 0.7053],\n","        [ 0.7741],\n","        [ 0.6187],\n","        [ 1.8444],\n","        [ 3.3285],\n","        [ 3.5235],\n","        [ 3.0035],\n","        [ 3.5781],\n","        [ 7.0570],\n","        [ 3.1714],\n","        [ 3.2403],\n","        [11.9070],\n","        [ 9.3469],\n","        [ 6.0949],\n","        [10.7859],\n","        [11.2929],\n","        [ 2.3627],\n","        [ 0.5282],\n","        [ 0.5353],\n","        [ 3.3834],\n","        [ 4.0545],\n","        [ 2.2300],\n","        [16.9873],\n","        [11.0782],\n","        [ 0.8739],\n","        [ 7.9386],\n","        [ 0.7644],\n","        [13.5330],\n","        [ 0.6575],\n","        [ 4.0777],\n","        [11.2929],\n","        [ 7.0196],\n","        [ 8.7090],\n","        [10.5317],\n","        [ 2.2088],\n","        [ 9.9504],\n","        [ 6.5876],\n","        [ 0.6022],\n","        [ 1.2764],\n","        [ 9.7803],\n","        [ 1.0274],\n","        [ 3.7571],\n","        [ 5.9420],\n","        [ 0.6590],\n","        [ 0.6225],\n","        [ 2.2475],\n","        [ 0.6337],\n","        [ 1.0705],\n","        [ 1.6459],\n","        [ 1.4471],\n","        [ 7.6256],\n","        [11.3099],\n","        [ 0.5380],\n","        [ 0.7862],\n","        [ 1.9448],\n","        [ 0.8534],\n","        [ 2.1440],\n","        [ 2.9689],\n","        [ 9.9504],\n","        [ 4.2703],\n","        [ 0.7008],\n","        [ 6.0512],\n","        [12.4046],\n","        [ 6.3873],\n","        [ 0.5847],\n","        [ 6.2283],\n","        [ 1.3912],\n","        [ 2.1072],\n","        [ 2.3480],\n","        [15.1675],\n","        [ 0.8735],\n","        [ 4.7759],\n","        [ 4.8956],\n","        [ 0.5126],\n","        [ 8.5759],\n","        [ 4.3373],\n","        [16.3322],\n","        [ 2.0646],\n","        [ 3.1151],\n","        [ 5.9386],\n","        [ 6.0512],\n","        [ 0.6963],\n","        [ 0.5670],\n","        [ 7.1056],\n","        [ 7.5493],\n","        [ 7.0168],\n","        [ 1.3912],\n","        [ 0.8739],\n","        [ 1.0852],\n","        [ 0.7714],\n","        [ 1.2475],\n","        [ 6.3873],\n","        [ 0.6418],\n","        [ 0.6963],\n","        [ 4.6537],\n","        [ 6.5102],\n","        [ 4.3022],\n","        [ 1.3684],\n","        [ 3.5235],\n","        [10.5317],\n","        [ 9.1911],\n","        [ 0.9878],\n","        [ 5.7794],\n","        [ 1.0652],\n","        [ 4.3355],\n","        [ 3.2103],\n","        [16.9873],\n","        [ 5.7428],\n","        [ 3.7571],\n","        [ 7.0570],\n","        [ 7.1056],\n","        [ 1.9904],\n","        [ 2.3379],\n","        [ 7.6256],\n","        [ 0.8911],\n","        [ 2.7408],\n","        [ 3.2403],\n","        [11.8976],\n","        [ 9.3469],\n","        [ 2.3147],\n","        [ 5.6952],\n","        [ 3.9564],\n","        [12.5500],\n","        [11.3099],\n","        [ 0.6399],\n","        [ 3.9548],\n","        [ 2.8783],\n","        [ 0.5226],\n","        [ 8.3834],\n","        [ 1.4748],\n","        [ 0.7714],\n","        [ 6.3616],\n","        [ 0.5670],\n","        [ 0.5230],\n","        [ 0.7099],\n","        [ 1.2156],\n","        [ 8.1156],\n","        [ 3.0032],\n","        [ 4.3022],\n","        [13.2546],\n","        [ 5.7758],\n","        [ 4.6488],\n","        [ 4.8268],\n","        [ 1.0719],\n","        [ 3.2407],\n","        [ 0.5391],\n","        [ 1.3684],\n","        [ 0.5391],\n","        [ 5.7794],\n","        [12.2991],\n","        [ 5.3846],\n","        [ 3.8627],\n","        [ 4.3355],\n","        [ 6.5876],\n","        [ 1.7445],\n","        [ 5.9489],\n","        [ 0.7683],\n","        [ 0.5830],\n","        [ 0.6349],\n","        [ 0.5830],\n","        [ 2.7117],\n","        [ 2.4617],\n","        [ 2.4644],\n","        [ 6.3779],\n","        [ 1.5156],\n","        [ 8.9941],\n","        [ 0.5308],\n","        [ 0.5353],\n","        [ 6.3616],\n","        [ 7.9056],\n","        [ 0.5282],\n","        [ 4.7759],\n","        [ 1.2089],\n","        [ 9.3157],\n","        [ 2.4648],\n","        [ 0.5230],\n","        [ 1.0274],\n","        [ 5.9573],\n","        [ 4.3282],\n","        [ 1.0078],\n","        [ 0.8879],\n","        [ 2.1499],\n","        [ 1.5465],\n","        [ 6.0949],\n","        [ 5.7428],\n","        [ 0.9495],\n","        [ 8.6804],\n","        [ 4.3282],\n","        [ 0.8791],\n","        [ 8.1156],\n","        [ 0.9036],\n","        [11.3902],\n","        [ 0.5458],\n","        [ 2.5788],\n","        [ 4.6946],\n","        [ 1.0256],\n","        [13.8401],\n","        [ 1.2089],\n","        [ 6.5102],\n","        [ 4.8843],\n","        [ 0.6588],\n","        [ 2.9230],\n","        [ 1.9379],\n","        [ 1.2124],\n","        [ 5.9489],\n","        [ 3.3834],\n","        [ 2.1072],\n","        [ 0.5410],\n","        [ 2.3872],\n","        [ 1.2132],\n","        [ 9.3081],\n","        [ 1.2426],\n","        [ 4.3343],\n","        [ 1.8444],\n","        [ 8.7138],\n","        [ 0.6337],\n","        [12.4046],\n","        [ 1.4657],\n","        [ 0.7053],\n","        [ 0.8879],\n","        [ 2.7624],\n","        [ 2.6120],\n","        [ 4.6537],\n","        [ 4.7144],\n","        [ 8.8726],\n","        [12.2991],\n","        [ 0.9036],\n","        [ 1.0078],\n","        [ 5.4568],\n","        [ 3.1714],\n","        [ 0.9368],\n","        [ 1.2124],\n","        [ 9.7185],\n","        [ 9.5457],\n","        [11.8976],\n","        [ 5.8475],\n","        [ 2.8783],\n","        [ 7.1318],\n","        [ 3.1794],\n","        [11.2662],\n","        [ 9.8428],\n","        [ 2.3872],\n","        [ 0.5308],\n","        [ 0.7683],\n","        [ 9.3081],\n","        [ 1.5465],\n","        [ 1.3940],\n","        [ 1.8092],\n","        [ 0.7862],\n","        [ 2.9657],\n","        [ 1.9379],\n","        [ 0.5858],\n","        [ 1.4471],\n","        [ 7.0168],\n","        [ 7.2807],\n","        [ 2.4644],\n","        [ 2.9657],\n","        [ 1.5825],\n","        [ 4.6039],\n","        [ 0.5129],\n","        [ 9.8428],\n","        [ 2.2088],\n","        [ 0.8972],\n","        [ 2.4087],\n","        [ 3.9564],\n","        [ 1.0705],\n","        [14.4179],\n","        [ 8.7090],\n","        [ 3.0035],\n","        [ 0.6349],\n","        [ 0.6588],\n","        [ 9.7092],\n","        [ 0.5332],\n","        [ 0.9040],\n","        [ 1.6564],\n","        [ 1.4748],\n","        [ 3.8178],\n","        [ 3.5781],\n","        [ 1.1788],\n","        [ 0.5932],\n","        [ 0.5858],\n","        [ 6.9029],\n","        [ 3.8627],\n","        [ 1.5156],\n","        [ 2.9689],\n","        [ 4.8268],\n","        [ 2.7624],\n","        [13.0136],\n","        [ 7.9386],\n","        [16.8325],\n","        [ 3.3308],\n","        [ 6.5258],\n","        [ 5.6952],\n","        [ 3.1151],\n","        [ 2.3480],\n","        [ 0.6590],\n","        [ 5.5244],\n","        [ 2.5788],\n","        [ 3.3697],\n","        [ 5.3846],\n","        [ 4.2201],\n","        [ 0.5580],\n","        [ 6.6956],\n","        [ 4.6039],\n","        [ 3.3708],\n","        [ 0.5847],\n","        [ 3.1794],\n","        [ 8.3864],\n","        [ 2.0646],\n","        [ 1.6459],\n","        [ 2.3379],\n","        [ 0.6988],\n","        [ 1.0256],\n","        [10.0413],\n","        [ 5.9420],\n","        [ 4.0777],\n","        [ 1.0652],\n","        [ 1.5825],\n","        [ 0.6594],\n","        [ 5.4568],\n","        [ 7.2372],\n","        [ 5.8475],\n","        [ 3.0032],\n","        [ 5.7758],\n","        [ 1.2426],\n","        [ 2.8109],\n","        [ 0.6418],\n","        [ 4.2703],\n","        [ 0.5213],\n","        [14.4179],\n","        [ 1.4336],\n","        [ 5.5244],\n","        [ 8.7868],\n","        [14.9174],\n","        [ 0.5410],\n","        [ 4.3373],\n","        [ 2.4648],\n","        [ 7.0196],\n","        [ 0.8534],\n","        [ 9.7092],\n","        [ 0.7741],\n","        [ 1.4336],\n","        [ 2.4956],\n","        [15.1675],\n","        [ 0.6256],\n","        [ 0.6575],\n","        [ 1.6490],\n","        [ 5.2298],\n","        [ 8.9941],\n","        [ 6.5258],\n","        [ 0.6022],\n","        [ 8.3864],\n","        [ 1.8092],\n","        [ 0.6266],\n","        [ 4.7039],\n","        [13.0136],\n","        [ 0.5332],\n","        [ 1.4657],\n","        [11.9070],\n","        [ 1.9981],\n","        [ 0.6256],\n","        [ 8.0085],\n","        [ 2.3147],\n","        [ 2.1440],\n","        [ 0.9495],\n","        [ 0.8791],\n","        [ 2.2475],\n","        [ 5.6230],\n","        [ 1.7445],\n","        [13.8401],\n","        [ 2.8109],\n","        [ 4.8956],\n","        [13.8277],\n","        [16.8325],\n","        [ 8.0085],\n","        [10.0496],\n","        [ 1.9448],\n","        [ 7.2372],\n","        [ 8.7138],\n","        [ 1.2475],\n","        [ 5.6230],\n","        [ 4.7039],\n","        [ 4.3520],\n","        [ 0.5129],\n","        [ 3.3308],\n","        [ 1.5599],\n","        [ 6.3779],\n","        [ 0.5213],\n","        [ 4.5039],\n","        [ 1.9140],\n","        [ 7.5493],\n","        [ 0.8885],\n","        [ 8.3834],\n","        [ 0.6187],\n","        [ 0.9878],\n","        [ 3.2103],\n","        [ 0.6988],\n","        [ 2.4617],\n","        [ 5.2298],\n","        [ 0.6399],\n","        [ 2.3627],\n","        [ 0.6594],\n","        [ 5.9573],\n","        [ 0.5380],\n","        [ 7.2807],\n","        [ 2.6401],\n","        [ 2.4424],\n","        [ 1.3253],\n","        [13.5330],\n","        [ 6.2283],\n","        [ 0.7997],\n","        [ 4.0545],\n","        [ 9.7185],\n","        [13.1038],\n","        [13.2546],\n","        [ 4.3343],\n","        [ 9.3157],\n","        [ 1.3253],\n","        [ 0.7099],\n","        [ 0.5126],\n","        [ 6.9029],\n","        [ 1.9140],\n","        [ 1.0935],\n","        [ 2.6120],\n","        [ 0.6975],\n","        [ 0.5880],\n","        [ 7.9056],\n","        [ 2.2300],\n","        [ 8.8726],\n","        [ 7.1318],\n","        [ 0.8885],\n","        [10.0496],\n","        [ 2.4424],\n","        [ 0.6266],\n","        [ 6.6956],\n","        [ 2.1499],\n","        [ 8.7868],\n","        [ 3.2407],\n","        [ 1.9904],\n","        [ 0.6064],\n","        [ 0.7997],\n","        [ 3.3708],\n","        [ 9.1911],\n","        [ 1.5599],\n","        [ 0.6975],\n","        [ 5.8650],\n","        [ 0.9040],\n","        [ 0.9368],\n","        [ 1.3940],\n","        [14.9174],\n","        [ 1.0852],\n","        [ 2.4087],\n","        [ 3.3285],\n","        [ 5.9386],\n","        [10.1426],\n","        [11.2662]], device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([512, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 256])\n","Previous layer shape is  torch.Size([512])\n","Layer ('_gnn._conv_layers.2.nn.0.weight', '_gnn._conv_layers.2.nn.0.weight') shape is torch.Size([336, 512]) and torch.Size([672, 1024])\n","shape of layer: model 0 torch.Size([336, 512])\n","shape of layer: model 1 torch.Size([672, 1024])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([336, 512])\n","T_var.shape = torch.Size([256, 512])\n","torch.Size([336, 512]) torch.Size([256, 512])\n","aligned_wt shape is torch.Size([336, 1024])\n","fc_layer1_weight_data shape is torch.Size([672, 1024])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.5539, 2.5305, 2.5426,  ..., 2.5223, 2.5270, 2.5318],\n","        [2.4076, 2.4048, 2.3791,  ..., 2.3924, 2.4121, 2.4199],\n","        [1.4539, 1.4077, 1.4171,  ..., 1.4234, 1.4071, 1.4193],\n","        ...,\n","        [2.7600, 2.7711, 2.7335,  ..., 2.7592, 2.7577, 2.7889],\n","        [2.9416, 2.9139, 2.9438,  ..., 2.9422, 2.9574, 2.9652],\n","        [3.2390, 3.2262, 3.2395,  ..., 3.2547, 3.2351, 3.2096]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","mu shape is (336,)\n","nu shape is (672,)\n","cpuM shape is (336, 672)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([672])\n","inverse marginals beta is  tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015], device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0015, device='cuda:0')\n","Here, trace is 0.9999328255653381 and matrix sum is 671.954833984375 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([336, 1024])\n","Shape of fc_layer0_weight_data is  torch.Size([336, 512])\n","Previous layer shape is  torch.Size([672, 1024])\n","Layer ('_gnn._conv_layers.2.nn.0.bias', '_gnn._conv_layers.2.nn.0.bias') shape is torch.Size([1, 336]) and torch.Size([1, 672])\n","shape of layer: model 0 torch.Size([1, 336])\n","shape of layer: model 1 torch.Size([1, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([1, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([672, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[4.8598],\n","        [1.2637],\n","        [1.9957],\n","        [0.9154],\n","        [0.8759],\n","        [3.1885],\n","        [1.9511],\n","        [1.8236],\n","        [0.9665],\n","        [0.4617],\n","        [0.9186],\n","        [0.6194],\n","        [1.9578],\n","        [0.5721],\n","        [0.4673],\n","        [2.5312],\n","        [1.3634],\n","        [0.8759],\n","        [0.4793],\n","        [1.5689],\n","        [1.8382],\n","        [0.7369],\n","        [1.0003],\n","        [0.8361],\n","        [1.0031],\n","        [0.5011],\n","        [3.3973],\n","        [0.4793],\n","        [1.5782],\n","        [1.8062],\n","        [1.2806],\n","        [0.8502],\n","        [0.6765],\n","        [1.2652],\n","        [0.5175],\n","        [0.8115],\n","        [2.2141],\n","        [0.4961],\n","        [0.9429],\n","        [1.6165],\n","        [0.4736],\n","        [2.6945],\n","        [0.4784],\n","        [1.8062],\n","        [0.6178],\n","        [0.4758],\n","        [0.5085],\n","        [0.4784],\n","        [0.9092],\n","        [1.7855],\n","        [1.5877],\n","        [1.2695],\n","        [0.4906],\n","        [2.4010],\n","        [3.8706],\n","        [1.7913],\n","        [1.2450],\n","        [3.8144],\n","        [3.1060],\n","        [1.7913],\n","        [2.6228],\n","        [0.5555],\n","        [2.8448],\n","        [1.4059],\n","        [1.6976],\n","        [1.0383],\n","        [1.6551],\n","        [1.0947],\n","        [1.4923],\n","        [0.5954],\n","        [0.8361],\n","        [3.3481],\n","        [4.7059],\n","        [0.9014],\n","        [0.9901],\n","        [1.7616],\n","        [1.0646],\n","        [1.8023],\n","        [0.6204],\n","        [3.2218],\n","        [1.2955],\n","        [2.9454],\n","        [0.5842],\n","        [3.0074],\n","        [3.1090],\n","        [1.3502],\n","        [0.4593],\n","        [1.5635],\n","        [0.5199],\n","        [0.4633],\n","        [2.1357],\n","        [0.9287],\n","        [1.1157],\n","        [2.5590],\n","        [0.5043],\n","        [0.8502],\n","        [1.8579],\n","        [1.5635],\n","        [1.7302],\n","        [0.4846],\n","        [2.5822],\n","        [0.5003],\n","        [0.8777],\n","        [1.9697],\n","        [0.8396],\n","        [1.5237],\n","        [1.4416],\n","        [0.5669],\n","        [0.7119],\n","        [0.4961],\n","        [0.4943],\n","        [0.5915],\n","        [0.4629],\n","        [0.9754],\n","        [0.5339],\n","        [0.4596],\n","        [0.5566],\n","        [2.5135],\n","        [0.7973],\n","        [1.2806],\n","        [1.8464],\n","        [0.4784],\n","        [4.3743],\n","        [1.3795],\n","        [1.1799],\n","        [4.7059],\n","        [0.5355],\n","        [1.3647],\n","        [2.4013],\n","        [0.8668],\n","        [0.5050],\n","        [1.2116],\n","        [0.8753],\n","        [1.5082],\n","        [3.8547],\n","        [1.3000],\n","        [2.2464],\n","        [1.4162],\n","        [0.5229],\n","        [1.2531],\n","        [1.6883],\n","        [1.8179],\n","        [1.7084],\n","        [3.8144],\n","        [4.1120],\n","        [1.6445],\n","        [0.5555],\n","        [1.2249],\n","        [0.8160],\n","        [0.7369],\n","        [0.4668],\n","        [0.8634],\n","        [0.5262],\n","        [0.5440],\n","        [2.4010],\n","        [0.4828],\n","        [0.6960],\n","        [1.8922],\n","        [1.1660],\n","        [0.8753],\n","        [0.5254],\n","        [2.2251],\n","        [0.4675],\n","        [0.5340],\n","        [1.4838],\n","        [0.5175],\n","        [0.6630],\n","        [1.4838],\n","        [1.7121],\n","        [0.6339],\n","        [0.9429],\n","        [0.6608],\n","        [1.2531],\n","        [2.0608],\n","        [1.0220],\n","        [0.4623],\n","        [1.7498],\n","        [1.1858],\n","        [0.6178],\n","        [0.8737],\n","        [1.5027],\n","        [0.4590],\n","        [0.7951],\n","        [1.6976],\n","        [1.9697],\n","        [0.6633],\n","        [1.8922],\n","        [2.1386],\n","        [1.3568],\n","        [1.6710],\n","        [3.2218],\n","        [0.6297],\n","        [2.2162],\n","        [1.0383],\n","        [1.0636],\n","        [0.5128],\n","        [0.8777],\n","        [1.0947],\n","        [3.4817],\n","        [0.4675],\n","        [0.9098],\n","        [1.8980],\n","        [0.5631],\n","        [1.9704],\n","        [5.9076],\n","        [1.0023],\n","        [1.5269],\n","        [1.2399],\n","        [0.7443],\n","        [0.8363],\n","        [0.4617],\n","        [3.1477],\n","        [0.5915],\n","        [0.6205],\n","        [0.7252],\n","        [2.6894],\n","        [0.4907],\n","        [2.5590],\n","        [2.1386],\n","        [2.0494],\n","        [0.4846],\n","        [1.9134],\n","        [1.2201],\n","        [0.9655],\n","        [2.0494],\n","        [0.9103],\n","        [0.7119],\n","        [1.2597],\n","        [1.6551],\n","        [1.5119],\n","        [0.7443],\n","        [1.2134],\n","        [1.3309],\n","        [1.2053],\n","        [1.9704],\n","        [1.7961],\n","        [2.7484],\n","        [1.1660],\n","        [0.5599],\n","        [2.5135],\n","        [0.5308],\n","        [1.3309],\n","        [1.7148],\n","        [0.4830],\n","        [1.7961],\n","        [1.7564],\n","        [3.1008],\n","        [0.9103],\n","        [2.6228],\n","        [0.5673],\n","        [0.4623],\n","        [0.4590],\n","        [1.0003],\n","        [0.5721],\n","        [2.6945],\n","        [3.7967],\n","        [1.8382],\n","        [1.2506],\n","        [0.4770],\n","        [1.2220],\n","        [0.4818],\n","        [6.5036],\n","        [0.5085],\n","        [2.2251],\n","        [1.3252],\n","        [0.8139],\n","        [1.0115],\n","        [1.6445],\n","        [1.9974],\n","        [3.1762],\n","        [1.5782],\n","        [2.4013],\n","        [0.8974],\n","        [0.9098],\n","        [1.4995],\n","        [0.5050],\n","        [1.0696],\n","        [0.8974],\n","        [0.4795],\n","        [0.9655],\n","        [0.6215],\n","        [1.0220],\n","        [0.4668],\n","        [1.2652],\n","        [0.4795],\n","        [0.7951],\n","        [0.6907],\n","        [1.2137],\n","        [0.8363],\n","        [0.5011],\n","        [1.2848],\n","        [3.1762],\n","        [0.8115],\n","        [1.4393],\n","        [1.0115],\n","        [0.6366],\n","        [1.1157],\n","        [0.6854],\n","        [2.8570],\n","        [1.2736],\n","        [4.6058],\n","        [0.8370],\n","        [1.0062],\n","        [0.4812],\n","        [0.6555],\n","        [2.2884],\n","        [5.2319],\n","        [1.1619],\n","        [1.3631],\n","        [1.0190],\n","        [2.2623],\n","        [1.0190],\n","        [4.0446],\n","        [1.4923],\n","        [0.4728],\n","        [0.7511],\n","        [2.5532],\n","        [0.9287],\n","        [1.8464],\n","        [0.5954],\n","        [0.6535],\n","        [1.1986],\n","        [1.6142],\n","        [1.5549],\n","        [1.0023],\n","        [1.8179],\n","        [1.6165],\n","        [7.2414],\n","        [2.7484],\n","        [0.6608],\n","        [0.6497],\n","        [0.5003],\n","        [1.1507],\n","        [1.2149],\n","        [2.4041],\n","        [0.4673],\n","        [0.5599],\n","        [0.4980],\n","        [1.4995],\n","        [1.2955],\n","        [1.9511],\n","        [1.2736],\n","        [0.4598],\n","        [1.8023],\n","        [1.3996],\n","        [0.4818],\n","        [1.8980],\n","        [0.5673],\n","        [0.9154],\n","        [0.4596],\n","        [2.8448],\n","        [0.5059],\n","        [1.0636],\n","        [0.6204],\n","        [1.2420],\n","        [1.2117],\n","        [0.5986],\n","        [0.4736],\n","        [0.5475],\n","        [1.1199],\n","        [1.2134],\n","        [0.4922],\n","        [1.2077],\n","        [0.5152],\n","        [0.5100],\n","        [1.7084],\n","        [3.1008],\n","        [1.5549],\n","        [0.4589],\n","        [2.0652],\n","        [2.8570],\n","        [0.5554],\n","        [1.7121],\n","        [1.1799],\n","        [0.4591],\n","        [2.4041],\n","        [1.9169],\n","        [0.4697],\n","        [1.7616],\n","        [1.7496],\n","        [0.8737],\n","        [2.4058],\n","        [0.5004],\n","        [1.3252],\n","        [0.7436],\n","        [0.9901],\n","        [2.2464],\n","        [0.6205],\n","        [1.8532],\n","        [0.4862],\n","        [3.1885],\n","        [4.4299],\n","        [0.5986],\n","        [0.6366],\n","        [1.0031],\n","        [0.5943],\n","        [1.4957],\n","        [0.4862],\n","        [2.1687],\n","        [2.2141],\n","        [1.7126],\n","        [0.4812],\n","        [1.3568],\n","        [0.4779],\n","        [3.8547],\n","        [0.8160],\n","        [0.7252],\n","        [0.5795],\n","        [1.3092],\n","        [0.4596],\n","        [1.0062],\n","        [0.4880],\n","        [0.8517],\n","        [1.2249],\n","        [3.3929],\n","        [0.4770],\n","        [0.4671],\n","        [1.9957],\n","        [2.2884],\n","        [0.7790],\n","        [0.5128],\n","        [5.9076],\n","        [1.2124],\n","        [1.2597],\n","        [0.6497],\n","        [0.5932],\n","        [0.4593],\n","        [0.8078],\n","        [1.6142],\n","        [0.5339],\n","        [0.8078],\n","        [0.4589],\n","        [1.9974],\n","        [1.2399],\n","        [1.0071],\n","        [0.4830],\n","        [1.6869],\n","        [0.6215],\n","        [1.2137],\n","        [1.3502],\n","        [2.2162],\n","        [0.6907],\n","        [0.8155],\n","        [0.6194],\n","        [2.7621],\n","        [3.4817],\n","        [0.4779],\n","        [2.3726],\n","        [1.7303],\n","        [1.8236],\n","        [3.7967],\n","        [4.0446],\n","        [0.7014],\n","        [0.6854],\n","        [1.2695],\n","        [0.6960],\n","        [1.2637],\n","        [1.3631],\n","        [0.5043],\n","        [2.2623],\n","        [0.7790],\n","        [0.9039],\n","        [7.2414],\n","        [1.6869],\n","        [0.8139],\n","        [1.5237],\n","        [1.1858],\n","        [1.2149],\n","        [0.6596],\n","        [0.5554],\n","        [0.7436],\n","        [1.2506],\n","        [0.8155],\n","        [1.1713],\n","        [1.5269],\n","        [3.3481],\n","        [1.7116],\n","        [1.8579],\n","        [2.7827],\n","        [1.0696],\n","        [0.5355],\n","        [2.5532],\n","        [0.4671],\n","        [0.4590],\n","        [0.5004],\n","        [0.4907],\n","        [0.5100],\n","        [3.0074],\n","        [1.3639],\n","        [1.1713],\n","        [4.8598],\n","        [2.4058],\n","        [0.4758],\n","        [0.5943],\n","        [3.3973],\n","        [0.8543],\n","        [1.7126],\n","        [1.0646],\n","        [2.5651],\n","        [1.3996],\n","        [2.1357],\n","        [0.4590],\n","        [1.5027],\n","        [0.4629],\n","        [1.7116],\n","        [0.7723],\n","        [0.4922],\n","        [0.4880],\n","        [0.6630],\n","        [0.8516],\n","        [0.7823],\n","        [0.4602],\n","        [0.6596],\n","        [1.2116],\n","        [1.3795],\n","        [0.8516],\n","        [1.0071],\n","        [0.7612],\n","        [0.4589],\n","        [0.5475],\n","        [1.3634],\n","        [0.7511],\n","        [0.9754],\n","        [2.8650],\n","        [4.6058],\n","        [1.5689],\n","        [0.5059],\n","        [0.8634],\n","        [3.1090],\n","        [0.6930],\n","        [0.6535],\n","        [0.7609],\n","        [0.9014],\n","        [1.2077],\n","        [0.9665],\n","        [2.5822],\n","        [1.2220],\n","        [0.5152],\n","        [0.5795],\n","        [1.5119],\n","        [1.9578],\n","        [1.6710],\n","        [4.3743],\n","        [3.1477],\n","        [0.9039],\n","        [6.5036],\n","        [0.8668],\n","        [1.4162],\n","        [1.2420],\n","        [2.6894],\n","        [1.7564],\n","        [0.6339],\n","        [0.8543],\n","        [0.5631],\n","        [1.4393],\n","        [1.8532],\n","        [1.6032],\n","        [0.7271],\n","        [2.6203],\n","        [0.9982],\n","        [3.0939],\n","        [1.3000],\n","        [0.4784],\n","        [3.2947],\n","        [2.6203],\n","        [0.5566],\n","        [0.4591],\n","        [2.0652],\n","        [0.5842],\n","        [0.5932],\n","        [0.7264],\n","        [0.8517],\n","        [4.4308],\n","        [0.5308],\n","        [1.3647],\n","        [0.4596],\n","        [0.9186],\n","        [2.9292],\n","        [0.5229],\n","        [3.1060],\n","        [5.2319],\n","        [1.1507],\n","        [0.5669],\n","        [0.5440],\n","        [0.7271],\n","        [0.4602],\n","        [0.4697],\n","        [0.9313],\n","        [0.7264],\n","        [0.7609],\n","        [1.5082],\n","        [0.8370],\n","        [4.4308],\n","        [1.4416],\n","        [1.6032],\n","        [0.4728],\n","        [2.9454],\n","        [1.6883],\n","        [1.7303],\n","        [1.7148],\n","        [3.2947],\n","        [2.9292],\n","        [0.5398],\n","        [1.7855],\n","        [1.2053],\n","        [0.4828],\n","        [0.5340],\n","        [1.1986],\n","        [1.4059],\n","        [2.3726],\n","        [0.9982],\n","        [0.7723],\n","        [0.6633],\n","        [1.2201],\n","        [0.6555],\n","        [1.2124],\n","        [4.1120],\n","        [0.5179],\n","        [0.4638],\n","        [2.1687],\n","        [0.9092],\n","        [0.5262],\n","        [3.8706],\n","        [1.1619],\n","        [0.8396],\n","        [1.9134],\n","        [1.7498],\n","        [0.4590],\n","        [0.5179],\n","        [0.4906],\n","        [1.2848],\n","        [0.5956],\n","        [0.7973],\n","        [0.4943],\n","        [0.9313],\n","        [0.4980],\n","        [1.7302],\n","        [1.3092],\n","        [0.4638],\n","        [0.4590],\n","        [2.5312],\n","        [0.4589],\n","        [0.7014],\n","        [1.2117],\n","        [1.7496],\n","        [0.5956],\n","        [4.4299],\n","        [1.4957],\n","        [3.3929],\n","        [0.5199],\n","        [0.7823],\n","        [0.7612],\n","        [1.2450],\n","        [1.3639],\n","        [0.5254],\n","        [2.7827],\n","        [0.4598],\n","        [1.9169],\n","        [1.5877],\n","        [0.6765],\n","        [3.0939],\n","        [0.4784],\n","        [0.4784],\n","        [2.8650],\n","        [0.6297],\n","        [2.0608],\n","        [0.4633],\n","        [2.5651],\n","        [1.1199],\n","        [2.7621],\n","        [0.5398],\n","        [0.6930]], device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","Ratio of trace to the matrix sum:  tensor(0.0015, device='cuda:0')\n","Here, trace is 0.9999328255653381 and matrix sum is 671.954833984375 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([672, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 336])\n","Previous layer shape is  torch.Size([672])\n","Layer ('_gnn._conv_layers.2.nn.2.weight', '_gnn._conv_layers.2.nn.2.weight') shape is torch.Size([256, 336]) and torch.Size([512, 672])\n","shape of layer: model 0 torch.Size([256, 336])\n","shape of layer: model 1 torch.Size([512, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([256, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([256, 672])\n","fc_layer1_weight_data shape is torch.Size([512, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.6730, 2.6228, 2.6622,  ..., 2.7074, 2.6507, 2.6790],\n","        [2.3762, 2.3900, 2.3731,  ..., 2.3990, 2.4030, 2.3916],\n","        [2.5027, 2.4865, 2.5070,  ..., 2.5152, 2.5148, 2.4784],\n","        ...,\n","        [2.1654, 2.1400, 2.2124,  ..., 2.2255, 2.2371, 2.1567],\n","        [2.7856, 2.7262, 2.7557,  ..., 2.7844, 2.7484, 2.7711],\n","        [2.4771, 2.4640, 2.4955,  ..., 2.4967, 2.5013, 2.4806]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","mu shape is (256,)\n","nu shape is (512,)\n","cpuM shape is (256, 512)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([512])\n","inverse marginals beta is  tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n","       device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999],\n","       device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 672])\n","Shape of fc_layer0_weight_data is  torch.Size([256, 336])\n","Previous layer shape is  torch.Size([512, 672])\n","Layer ('_gnn._conv_layers.2.nn.2.bias', '_gnn._conv_layers.2.nn.2.bias') shape is torch.Size([1, 256]) and torch.Size([1, 512])\n","shape of layer: model 0 torch.Size([1, 256])\n","shape of layer: model 1 torch.Size([1, 512])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([1, 256])\n","T_var.shape = torch.Size([256, 512])\n","aligned_wt shape is torch.Size([512, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 512])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[ 0.5882],\n","        [ 4.3767],\n","        [ 2.6397],\n","        [ 0.6170],\n","        [ 3.0873],\n","        [ 1.2727],\n","        [ 4.7596],\n","        [ 0.7169],\n","        [ 1.3552],\n","        [ 0.7828],\n","        [ 1.0073],\n","        [ 1.1424],\n","        [ 0.6834],\n","        [ 5.6205],\n","        [ 0.5162],\n","        [ 1.8129],\n","        [ 1.8942],\n","        [ 3.1834],\n","        [ 0.6373],\n","        [ 2.3401],\n","        [ 1.0484],\n","        [ 7.8283],\n","        [ 3.5541],\n","        [ 0.7917],\n","        [ 2.9733],\n","        [ 0.5882],\n","        [ 2.6128],\n","        [ 1.5260],\n","        [ 1.8551],\n","        [ 1.3333],\n","        [ 1.6356],\n","        [ 1.4066],\n","        [ 0.7819],\n","        [ 1.3101],\n","        [ 0.8957],\n","        [ 0.6141],\n","        [ 0.6141],\n","        [ 2.6397],\n","        [ 2.0211],\n","        [ 3.7711],\n","        [ 2.2995],\n","        [ 4.5756],\n","        [ 0.8615],\n","        [ 4.5378],\n","        [ 1.9460],\n","        [ 0.9012],\n","        [ 0.5163],\n","        [ 2.4361],\n","        [ 2.5530],\n","        [ 3.1834],\n","        [ 2.7355],\n","        [ 1.5565],\n","        [ 1.3333],\n","        [ 6.4021],\n","        [ 2.0573],\n","        [ 8.4584],\n","        [ 6.7705],\n","        [ 3.9736],\n","        [ 0.8443],\n","        [ 4.5378],\n","        [ 3.9736],\n","        [ 0.9012],\n","        [ 0.8258],\n","        [ 0.5187],\n","        [ 0.5592],\n","        [ 2.7355],\n","        [ 1.4031],\n","        [ 5.1501],\n","        [ 0.7485],\n","        [ 1.9870],\n","        [ 1.2180],\n","        [ 0.8561],\n","        [ 1.7671],\n","        [ 5.4928],\n","        [ 2.8598],\n","        [ 0.7265],\n","        [ 0.7906],\n","        [ 1.0230],\n","        [ 4.0071],\n","        [ 0.6968],\n","        [ 0.6577],\n","        [ 1.1810],\n","        [ 3.0873],\n","        [ 2.0927],\n","        [ 0.5197],\n","        [ 0.8725],\n","        [ 1.1568],\n","        [ 1.4448],\n","        [ 1.7074],\n","        [ 6.4980],\n","        [ 1.0770],\n","        [ 1.5152],\n","        [ 2.4892],\n","        [ 0.6373],\n","        [ 7.5206],\n","        [ 5.2705],\n","        [ 0.5728],\n","        [ 2.0564],\n","        [ 2.0819],\n","        [ 1.0147],\n","        [ 1.3980],\n","        [ 1.0663],\n","        [ 0.5163],\n","        [ 1.1167],\n","        [ 2.0927],\n","        [ 1.1881],\n","        [ 0.6801],\n","        [ 1.7671],\n","        [ 1.0663],\n","        [ 0.7094],\n","        [ 5.1501],\n","        [ 1.6468],\n","        [ 2.6780],\n","        [ 4.5944],\n","        [ 0.7871],\n","        [ 2.4541],\n","        [12.9764],\n","        [ 2.5530],\n","        [ 0.9766],\n","        [ 1.1305],\n","        [ 1.6125],\n","        [ 0.6532],\n","        [ 0.6968],\n","        [ 1.4789],\n","        [ 1.4427],\n","        [ 0.8298],\n","        [ 2.1123],\n","        [ 7.5206],\n","        [ 0.8716],\n","        [ 0.6584],\n","        [ 0.5364],\n","        [ 1.3347],\n","        [ 0.5267],\n","        [ 0.7788],\n","        [ 0.5432],\n","        [ 1.0356],\n","        [ 1.8393],\n","        [ 1.1803],\n","        [ 3.7711],\n","        [ 2.0819],\n","        [ 1.3111],\n","        [ 1.2602],\n","        [ 2.0648],\n","        [ 0.6178],\n","        [ 2.7420],\n","        [ 1.5152],\n","        [ 3.7151],\n","        [ 1.2602],\n","        [ 3.6442],\n","        [ 1.2249],\n","        [ 1.3261],\n","        [ 1.1803],\n","        [ 2.6780],\n","        [ 0.6178],\n","        [ 2.4541],\n","        [ 4.5944],\n","        [ 1.9735],\n","        [ 1.3347],\n","        [ 0.9354],\n","        [ 1.9757],\n","        [ 2.0211],\n","        [ 2.4609],\n","        [ 1.3138],\n","        [ 1.4276],\n","        [ 0.7838],\n","        [ 1.4929],\n","        [ 0.8677],\n","        [ 0.7169],\n","        [ 1.0236],\n","        [ 2.6700],\n","        [ 1.3672],\n","        [ 0.9876],\n","        [ 5.4928],\n","        [ 1.8437],\n","        [ 1.8216],\n","        [ 0.5290],\n","        [ 0.6260],\n","        [ 1.9460],\n","        [ 0.5303],\n","        [ 2.3191],\n","        [ 0.5197],\n","        [ 4.3767],\n","        [ 1.3980],\n","        [ 4.2611],\n","        [ 4.7612],\n","        [ 1.0521],\n","        [ 5.5859],\n","        [ 3.5090],\n","        [ 1.0849],\n","        [ 6.1716],\n","        [ 1.8437],\n","        [ 0.6827],\n","        [ 6.1716],\n","        [14.1956],\n","        [ 0.5267],\n","        [ 1.0533],\n","        [ 3.7151],\n","        [ 4.8241],\n","        [ 1.3993],\n","        [ 1.5744],\n","        [ 1.2977],\n","        [ 1.3993],\n","        [ 1.9038],\n","        [ 0.7458],\n","        [ 4.8760],\n","        [ 1.9715],\n","        [ 1.9038],\n","        [ 0.7294],\n","        [ 1.7074],\n","        [ 0.5919],\n","        [ 1.0236],\n","        [ 2.7570],\n","        [ 0.7906],\n","        [ 0.7838],\n","        [ 1.1167],\n","        [ 1.2626],\n","        [ 5.9198],\n","        [ 1.6856],\n","        [ 1.2977],\n","        [ 0.6836],\n","        [ 4.2611],\n","        [ 3.6778],\n","        [ 6.4980],\n","        [ 0.7819],\n","        [ 0.5305],\n","        [ 2.4609],\n","        [ 1.4031],\n","        [ 1.5725],\n","        [ 1.6927],\n","        [ 3.5573],\n","        [ 1.1568],\n","        [ 2.8598],\n","        [ 2.2371],\n","        [ 1.1305],\n","        [ 0.6836],\n","        [ 1.1424],\n","        [ 0.9004],\n","        [ 0.7871],\n","        [ 0.9766],\n","        [ 3.4179],\n","        [ 1.8942],\n","        [ 1.4066],\n","        [ 0.7294],\n","        [ 0.7016],\n","        [ 1.2566],\n","        [ 1.4074],\n","        [ 1.2558],\n","        [ 1.6468],\n","        [ 3.4179],\n","        [ 1.9870],\n","        [ 1.0169],\n","        [ 0.6603],\n","        [ 3.5090],\n","        [ 2.2221],\n","        [ 2.3250],\n","        [ 1.0521],\n","        [ 0.8957],\n","        [ 1.2626],\n","        [ 2.0955],\n","        [ 0.8258],\n","        [ 3.9296],\n","        [ 2.9087],\n","        [ 5.6205],\n","        [ 0.6170],\n","        [ 1.4941],\n","        [ 2.1545],\n","        [ 2.2466],\n","        [ 2.6435],\n","        [ 0.7265],\n","        [ 0.7094],\n","        [ 1.0073],\n","        [ 0.5290],\n","        [ 3.3780],\n","        [ 0.6532],\n","        [ 1.7695],\n","        [ 2.1608],\n","        [ 1.4789],\n","        [ 2.1123],\n","        [ 0.6827],\n","        [ 0.8561],\n","        [ 3.1142],\n","        [ 1.3649],\n","        [ 0.5728],\n","        [ 1.2180],\n","        [12.9764],\n","        [ 2.2371],\n","        [ 0.9880],\n","        [ 1.9715],\n","        [ 1.3138],\n","        [ 3.7011],\n","        [ 4.5756],\n","        [ 1.6031],\n","        [ 1.0601],\n","        [ 2.1545],\n","        [ 2.4892],\n","        [14.1956],\n","        [ 0.8298],\n","        [ 1.0484],\n","        [ 4.8760],\n","        [ 1.9757],\n","        [ 2.2221],\n","        [ 0.9880],\n","        [ 2.0933],\n","        [ 3.1999],\n","        [ 1.1810],\n","        [ 1.1042],\n","        [ 1.0356],\n","        [ 2.1608],\n","        [ 1.1075],\n","        [ 1.1151],\n","        [ 5.5859],\n","        [ 0.7788],\n","        [ 0.6584],\n","        [ 0.8443],\n","        [ 4.1434],\n","        [ 3.1142],\n","        [ 2.0648],\n","        [ 0.9357],\n","        [ 1.2140],\n","        [ 1.6856],\n","        [ 2.0034],\n","        [ 0.5592],\n","        [ 0.9622],\n","        [ 0.6432],\n","        [ 2.9590],\n","        [ 1.4427],\n","        [ 3.0059],\n","        [ 3.8208],\n","        [ 7.2866],\n","        [ 1.1167],\n","        [ 4.1434],\n","        [ 0.9622],\n","        [ 0.8677],\n","        [ 0.7485],\n","        [ 1.2900],\n","        [ 2.7570],\n","        [ 1.4929],\n","        [ 3.9296],\n","        [ 0.7462],\n","        [ 2.7870],\n","        [ 1.7630],\n","        [ 0.5303],\n","        [ 3.1327],\n","        [ 7.6509],\n","        [ 3.7011],\n","        [ 1.4276],\n","        [ 1.7630],\n","        [ 2.0933],\n","        [ 0.5187],\n","        [ 2.0564],\n","        [ 3.6778],\n","        [ 0.5343],\n","        [ 0.5163],\n","        [ 2.6435],\n","        [ 1.3672],\n","        [ 0.5163],\n","        [ 3.5573],\n","        [ 2.2528],\n","        [ 3.8208],\n","        [ 0.6110],\n","        [ 1.3261],\n","        [ 1.8420],\n","        [ 1.7695],\n","        [ 1.5744],\n","        [ 0.7828],\n","        [ 2.3191],\n","        [ 2.3693],\n","        [ 0.5364],\n","        [ 2.7420],\n","        [ 0.9962],\n","        [ 3.0059],\n","        [ 1.1042],\n","        [ 3.1999],\n","        [ 2.7870],\n","        [ 1.5725],\n","        [ 2.2466],\n","        [ 1.8551],\n","        [ 1.1881],\n","        [ 0.9354],\n","        [ 0.5162],\n","        [ 8.4584],\n","        [ 1.0230],\n","        [ 7.6509],\n","        [ 0.5432],\n","        [ 0.5175],\n","        [ 0.8725],\n","        [ 2.0573],\n","        [ 1.3101],\n","        [ 1.0424],\n","        [ 0.9004],\n","        [ 1.5084],\n","        [ 1.9735],\n","        [ 1.6125],\n","        [ 1.2727],\n","        [ 1.9773],\n","        [ 1.6031],\n","        [ 1.1167],\n","        [ 0.6260],\n","        [ 1.2558],\n","        [ 2.9087],\n","        [ 1.3552],\n","        [ 7.8283],\n","        [ 2.9590],\n","        [ 0.5305],\n","        [ 0.9876],\n","        [ 6.5842],\n","        [ 5.9198],\n","        [ 0.5919],\n","        [ 1.6356],\n","        [ 2.1710],\n","        [ 2.4361],\n","        [ 2.3693],\n","        [ 0.5356],\n","        [ 1.8216],\n","        [ 1.7671],\n","        [ 7.1521],\n","        [ 2.0034],\n","        [ 0.6895],\n","        [ 0.5175],\n","        [ 0.5217],\n","        [ 3.3780],\n","        [ 0.8716],\n","        [ 0.6603],\n","        [ 4.0713],\n","        [ 1.7671],\n","        [ 6.4021],\n","        [ 1.8129],\n","        [ 0.9962],\n","        [ 1.2828],\n","        [ 1.3649],\n","        [ 2.9310],\n","        [ 2.9733],\n","        [ 3.6442],\n","        [ 1.2900],\n","        [ 0.7305],\n","        [ 1.5565],\n","        [ 0.7016],\n","        [ 0.9101],\n","        [ 0.6895],\n","        [ 4.0713],\n","        [ 1.1075],\n","        [ 1.8420],\n","        [ 0.5217],\n","        [ 1.3823],\n","        [ 2.0955],\n","        [ 2.6128],\n","        [ 1.5376],\n","        [ 2.8810],\n","        [ 1.8286],\n","        [ 0.6432],\n","        [ 0.6577],\n","        [ 7.2866],\n","        [ 0.9101],\n","        [ 6.7705],\n","        [ 5.2705],\n","        [ 0.6110],\n","        [ 0.5343],\n","        [ 7.1521],\n","        [ 0.6192],\n","        [ 0.7458],\n","        [ 1.5084],\n","        [ 1.4074],\n","        [ 1.0849],\n","        [ 1.3111],\n","        [ 0.6192],\n","        [ 4.7612],\n","        [ 4.8241],\n","        [ 2.8810],\n","        [ 1.9773],\n","        [ 0.7917],\n","        [ 3.5541],\n","        [ 1.2828],\n","        [ 1.6927],\n","        [ 0.5356],\n","        [ 1.1151],\n","        [ 1.2249],\n","        [ 0.9804],\n","        [ 0.7462],\n","        [ 1.0532],\n","        [ 2.6700],\n","        [ 1.4941],\n","        [ 1.0169],\n","        [ 0.6801],\n","        [ 1.0770],\n","        [ 0.9357],\n","        [ 1.8286],\n","        [ 1.8393],\n","        [ 2.3401],\n","        [ 2.9310],\n","        [ 1.5376],\n","        [ 1.0532],\n","        [ 1.0601],\n","        [ 0.7305],\n","        [ 1.2140],\n","        [ 1.3823],\n","        [ 6.5842],\n","        [ 1.5260],\n","        [ 2.1710],\n","        [ 2.3250],\n","        [ 1.2566],\n","        [ 3.1327],\n","        [ 2.2528],\n","        [ 1.0424],\n","        [ 0.8615],\n","        [ 1.0533],\n","        [ 1.4448],\n","        [ 0.9804],\n","        [ 1.0147],\n","        [ 0.6834],\n","        [ 4.0071],\n","        [ 2.2995],\n","        [ 4.7596]], device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([512, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 256])\n","Previous layer shape is  torch.Size([512])\n","Layer ('_gnn._conv_layers.3.nn.0.weight', '_gnn._conv_layers.3.nn.0.weight') shape is torch.Size([336, 512]) and torch.Size([672, 1024])\n","shape of layer: model 0 torch.Size([336, 512])\n","shape of layer: model 1 torch.Size([672, 1024])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([336, 512])\n","T_var.shape = torch.Size([256, 512])\n","torch.Size([336, 512]) torch.Size([256, 512])\n","aligned_wt shape is torch.Size([336, 1024])\n","fc_layer1_weight_data shape is torch.Size([672, 1024])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.9190, 1.9116, 1.9360,  ..., 1.8777, 1.8754, 1.9312],\n","        [2.2555, 2.2546, 2.2778,  ..., 2.2870, 2.2764, 2.2735],\n","        [1.8242, 1.8149, 1.8485,  ..., 1.8407, 1.8205, 1.8285],\n","        ...,\n","        [1.5361, 1.5683, 1.5500,  ..., 1.5499, 1.5619, 1.5323],\n","        [2.0392, 2.0382, 2.0485,  ..., 2.0601, 2.0163, 2.0010],\n","        [2.0285, 2.0188, 2.0278,  ..., 2.0399, 2.0291, 2.0343]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","mu shape is (336,)\n","nu shape is (672,)\n","cpuM shape is (336, 672)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([672])\n","inverse marginals beta is  tensor([0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n","        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015], device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0030, device='cuda:0')\n","Here, trace is 1.9998656511306763 and matrix sum is 671.954833984375 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([336, 1024])\n","Shape of fc_layer0_weight_data is  torch.Size([336, 512])\n","Previous layer shape is  torch.Size([672, 1024])\n","Layer ('_gnn._conv_layers.3.nn.0.bias', '_gnn._conv_layers.3.nn.0.bias') shape is torch.Size([1, 336]) and torch.Size([1, 672])\n","shape of layer: model 0 torch.Size([1, 336])\n","shape of layer: model 1 torch.Size([1, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([1, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([672, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.9373],\n","        [0.8686],\n","        [0.6373],\n","        [0.8048],\n","        [1.7650],\n","        [1.2873],\n","        [0.6741],\n","        [0.4674],\n","        [0.4714],\n","        [0.7052],\n","        [1.2065],\n","        [1.8026],\n","        [0.6639],\n","        [1.0997],\n","        [1.2565],\n","        [2.6131],\n","        [0.4675],\n","        [1.7037],\n","        [2.3063],\n","        [0.6833],\n","        [0.8926],\n","        [1.0605],\n","        [1.3053],\n","        [0.7079],\n","        [1.7287],\n","        [1.3126],\n","        [1.4579],\n","        [1.7238],\n","        [0.4843],\n","        [0.6094],\n","        [1.6424],\n","        [0.5645],\n","        [0.8296],\n","        [0.4791],\n","        [1.6424],\n","        [1.0289],\n","        [0.5098],\n","        [1.0605],\n","        [0.9562],\n","        [2.2098],\n","        [1.1060],\n","        [0.6725],\n","        [1.8139],\n","        [2.7143],\n","        [1.8491],\n","        [0.9690],\n","        [0.4783],\n","        [1.3873],\n","        [1.5150],\n","        [0.9803],\n","        [1.8456],\n","        [2.9804],\n","        [2.6850],\n","        [1.7947],\n","        [0.4663],\n","        [0.4911],\n","        [0.7694],\n","        [0.4832],\n","        [0.9986],\n","        [0.8810],\n","        [1.0527],\n","        [0.4653],\n","        [1.8766],\n","        [2.4837],\n","        [0.7079],\n","        [1.3053],\n","        [0.5611],\n","        [0.6091],\n","        [1.6787],\n","        [0.4699],\n","        [2.7966],\n","        [0.5009],\n","        [1.7385],\n","        [2.7966],\n","        [0.9862],\n","        [1.4440],\n","        [0.6197],\n","        [0.6959],\n","        [0.4679],\n","        [0.4688],\n","        [0.7111],\n","        [0.4916],\n","        [0.8563],\n","        [3.1512],\n","        [0.9160],\n","        [1.3720],\n","        [1.5621],\n","        [0.5745],\n","        [0.4699],\n","        [1.6405],\n","        [0.4963],\n","        [0.5090],\n","        [2.7110],\n","        [0.9127],\n","        [0.9989],\n","        [0.5009],\n","        [1.4440],\n","        [0.8440],\n","        [0.4980],\n","        [0.9389],\n","        [0.6917],\n","        [0.6922],\n","        [1.3720],\n","        [1.6661],\n","        [0.8810],\n","        [1.6919],\n","        [0.5609],\n","        [0.6766],\n","        [1.8139],\n","        [2.2876],\n","        [1.6919],\n","        [1.2519],\n","        [1.3321],\n","        [0.5103],\n","        [2.0171],\n","        [0.6683],\n","        [1.2778],\n","        [0.5293],\n","        [1.8766],\n","        [0.7601],\n","        [0.4717],\n","        [0.5229],\n","        [0.6197],\n","        [0.4753],\n","        [2.9804],\n","        [0.6959],\n","        [2.6131],\n","        [0.6287],\n","        [0.5538],\n","        [1.1511],\n","        [2.7259],\n","        [0.6914],\n","        [0.8289],\n","        [0.7487],\n","        [0.4811],\n","        [0.6130],\n","        [0.4826],\n","        [0.4741],\n","        [0.6159],\n","        [2.1029],\n","        [1.4764],\n","        [1.6661],\n","        [1.0269],\n","        [0.6260],\n","        [2.7347],\n","        [0.6007],\n","        [0.5450],\n","        [1.3714],\n","        [1.0841],\n","        [0.8208],\n","        [0.6144],\n","        [0.6319],\n","        [0.4679],\n","        [2.3286],\n","        [1.5092],\n","        [0.8208],\n","        [2.0578],\n","        [1.4764],\n","        [0.4801],\n","        [0.7737],\n","        [1.1254],\n","        [0.9748],\n","        [1.5865],\n","        [0.6766],\n","        [2.2876],\n","        [2.2751],\n","        [3.1144],\n","        [0.7316],\n","        [1.3072],\n","        [0.9803],\n","        [1.0801],\n","        [1.3699],\n","        [0.4672],\n","        [0.6385],\n","        [0.6792],\n","        [1.1284],\n","        [2.0578],\n","        [1.8686],\n","        [3.2495],\n","        [1.0827],\n","        [1.7287],\n","        [2.0697],\n","        [3.1512],\n","        [0.8184],\n","        [1.2468],\n","        [0.6074],\n","        [3.0719],\n","        [0.8444],\n","        [0.5346],\n","        [0.5934],\n","        [0.5604],\n","        [0.7936],\n","        [2.7219],\n","        [1.3401],\n","        [0.5423],\n","        [0.6937],\n","        [1.4586],\n","        [0.4652],\n","        [1.3387],\n","        [0.4741],\n","        [0.4652],\n","        [0.8686],\n","        [2.4674],\n","        [1.4544],\n","        [1.6399],\n","        [1.0269],\n","        [0.5185],\n","        [1.7947],\n","        [2.0316],\n","        [1.3934],\n","        [1.3443],\n","        [3.5887],\n","        [0.6683],\n","        [0.9350],\n","        [2.0603],\n","        [0.4653],\n","        [1.2252],\n","        [0.7780],\n","        [1.9791],\n","        [0.4663],\n","        [0.5103],\n","        [0.4660],\n","        [1.6075],\n","        [0.6119],\n","        [0.9202],\n","        [1.1187],\n","        [0.8637],\n","        [1.2065],\n","        [1.0289],\n","        [0.4686],\n","        [2.9183],\n","        [1.6946],\n","        [0.9247],\n","        [0.5604],\n","        [0.6836],\n","        [0.5045],\n","        [0.8909],\n","        [2.3281],\n","        [1.3014],\n","        [1.9404],\n","        [0.5745],\n","        [0.5611],\n","        [0.6582],\n","        [1.5092],\n","        [0.9389],\n","        [0.4808],\n","        [1.3314],\n","        [1.7328],\n","        [1.1786],\n","        [0.9350],\n","        [1.0801],\n","        [0.9571],\n","        [3.1144],\n","        [3.0719],\n","        [0.5040],\n","        [2.1288],\n","        [0.6000],\n","        [0.5658],\n","        [2.8309],\n","        [0.5081],\n","        [0.5297],\n","        [0.7111],\n","        [0.8184],\n","        [2.4674],\n","        [1.0082],\n","        [0.6843],\n","        [3.2495],\n","        [0.7465],\n","        [0.8715],\n","        [0.4948],\n","        [0.6578],\n","        [2.5917],\n","        [0.9748],\n","        [0.6914],\n","        [0.8860],\n","        [0.4709],\n","        [0.5346],\n","        [2.7259],\n","        [0.4816],\n","        [0.6159],\n","        [0.6074],\n","        [0.5973],\n","        [2.5459],\n","        [1.2873],\n","        [1.3341],\n","        [0.4716],\n","        [0.4843],\n","        [0.9327],\n","        [2.1074],\n","        [1.0166],\n","        [0.5609],\n","        [0.5014],\n","        [0.5341],\n","        [2.0316],\n","        [0.9621],\n","        [1.2783],\n","        [2.3063],\n","        [0.6558],\n","        [0.5934],\n","        [1.2252],\n","        [0.7734],\n","        [0.6499],\n","        [0.5442],\n","        [1.8456],\n","        [0.9057],\n","        [2.7347],\n","        [2.0636],\n","        [2.5917],\n","        [0.9202],\n","        [1.2406],\n","        [0.5229],\n","        [2.2127],\n","        [1.2783],\n","        [0.9056],\n","        [0.4660],\n","        [0.6582],\n","        [1.3800],\n","        [0.6214],\n","        [0.4730],\n","        [0.5341],\n","        [1.3306],\n","        [0.7601],\n","        [0.4988],\n","        [2.0603],\n","        [1.0082],\n","        [0.9989],\n","        [0.8917],\n","        [0.6756],\n","        [1.9791],\n","        [0.5338],\n","        [0.5706],\n","        [2.1589],\n","        [1.3306],\n","        [2.2751],\n","        [2.7219],\n","        [0.5200],\n","        [2.5670],\n","        [0.6880],\n","        [0.4948],\n","        [2.3286],\n","        [0.8296],\n","        [3.0132],\n","        [0.8132],\n","        [6.8008],\n","        [0.4988],\n","        [0.9690],\n","        [1.1966],\n","        [0.8440],\n","        [0.6756],\n","        [0.8917],\n","        [0.8048],\n","        [0.4709],\n","        [1.7463],\n","        [0.4652],\n","        [1.6405],\n","        [0.6499],\n","        [0.4716],\n","        [0.8289],\n","        [0.8926],\n","        [0.4688],\n","        [0.6740],\n","        [0.6319],\n","        [1.2519],\n","        [2.2098],\n","        [0.6000],\n","        [1.7762],\n","        [1.1511],\n","        [2.2127],\n","        [0.4661],\n","        [1.6007],\n","        [0.7950],\n","        [1.2914],\n","        [2.3663],\n","        [1.0836],\n","        [2.0636],\n","        [3.5887],\n","        [0.7065],\n","        [0.7600],\n","        [0.5500],\n","        [0.8048],\n","        [1.1083],\n","        [2.8208],\n","        [3.0132],\n","        [6.8008],\n","        [0.6654],\n","        [0.7887],\n","        [1.9005],\n","        [0.6260],\n","        [0.6833],\n","        [2.8338],\n","        [0.9627],\n","        [0.8132],\n","        [0.4674],\n","        [0.4808],\n","        [0.6356],\n","        [1.0948],\n","        [0.5676],\n","        [0.4838],\n","        [1.1254],\n","        [0.5090],\n","        [0.7065],\n","        [1.7762],\n","        [2.7110],\n","        [1.0836],\n","        [0.5338],\n","        [0.4853],\n","        [0.5297],\n","        [1.2988],\n","        [0.8715],\n","        [0.4860],\n","        [2.4383],\n","        [0.5414],\n","        [0.8773],\n","        [2.1029],\n","        [0.6287],\n","        [0.4665],\n","        [1.3934],\n","        [1.3800],\n","        [0.5645],\n","        [0.7737],\n","        [0.4652],\n","        [0.9562],\n","        [0.6880],\n","        [0.7887],\n","        [0.4672],\n","        [0.8883],\n","        [0.4686],\n","        [0.6130],\n","        [2.8309],\n","        [2.0171],\n","        [0.9056],\n","        [0.5706],\n","        [1.0997],\n","        [0.4665],\n","        [1.2520],\n","        [2.6850],\n","        [0.4885],\n","        [0.6144],\n","        [0.5098],\n","        [1.3314],\n","        [0.8637],\n","        [1.8512],\n","        [0.6558],\n","        [0.4963],\n","        [0.7734],\n","        [2.0607],\n","        [0.8860],\n","        [0.4661],\n","        [2.7143],\n","        [1.6946],\n","        [1.6787],\n","        [1.3443],\n","        [1.0655],\n","        [1.6048],\n","        [1.9404],\n","        [0.6917],\n","        [1.3387],\n","        [0.6792],\n","        [0.4652],\n","        [0.9160],\n","        [1.2367],\n","        [0.4661],\n","        [1.3072],\n","        [0.8773],\n","        [2.1288],\n","        [1.6048],\n","        [1.2478],\n","        [1.1580],\n","        [1.0527],\n","        [0.4652],\n","        [1.6075],\n","        [0.6214],\n","        [1.0841],\n","        [0.5014],\n","        [0.7465],\n","        [1.3401],\n","        [1.4993],\n","        [0.6741],\n","        [1.9373],\n","        [0.6740],\n","        [1.2721],\n","        [0.5538],\n","        [0.9581],\n","        [1.1580],\n","        [1.8338],\n","        [1.2565],\n","        [0.7694],\n","        [0.8989],\n","        [0.4916],\n","        [0.6007],\n","        [1.3785],\n","        [1.7385],\n","        [1.6399],\n","        [0.9611],\n","        [0.4660],\n","        [1.2914],\n","        [0.9571],\n","        [0.4753],\n","        [0.4783],\n","        [0.7936],\n","        [2.5670],\n","        [0.6836],\n","        [0.4992],\n","        [0.9327],\n","        [0.6725],\n","        [1.2478],\n","        [2.0697],\n","        [2.4931],\n","        [0.7600],\n","        [0.7316],\n","        [2.8338],\n","        [0.6356],\n","        [0.6373],\n","        [2.1962],\n","        [0.9986],\n","        [1.3321],\n","        [0.5973],\n","        [1.1060],\n","        [1.5865],\n","        [0.5450],\n","        [2.3663],\n","        [1.5621],\n","        [0.4675],\n","        [0.7096],\n","        [0.8832],\n","        [1.3873],\n","        [2.1074],\n","        [1.0827],\n","        [0.6557],\n","        [1.7323],\n","        [1.8026],\n","        [2.0607],\n","        [1.8686],\n","        [0.5373],\n","        [1.0166],\n","        [1.3126],\n","        [1.0948],\n","        [0.4730],\n","        [2.3281],\n","        [0.8832],\n","        [1.6007],\n","        [0.7059],\n","        [1.5150],\n","        [0.9611],\n","        [1.2721],\n","        [0.7052],\n","        [1.7732],\n","        [2.1589],\n","        [0.9862],\n","        [0.6937],\n","        [0.5676],\n","        [0.4845],\n","        [0.6843],\n","        [0.9636],\n","        [0.4845],\n","        [1.1284],\n","        [0.4801],\n","        [1.4544],\n","        [0.5293],\n","        [1.7238],\n","        [2.4931],\n","        [0.6654],\n","        [0.8909],\n","        [0.5720],\n","        [0.8444],\n","        [0.7096],\n","        [2.2242],\n","        [0.7190],\n","        [0.4714],\n","        [0.5373],\n","        [0.7950],\n","        [0.4811],\n","        [0.5500],\n","        [0.7487],\n","        [1.4586],\n","        [0.4860],\n","        [1.2778],\n","        [0.4832],\n","        [1.0357],\n","        [1.2367],\n","        [1.7323],\n","        [0.5040],\n","        [0.7247],\n","        [0.8883],\n","        [0.4795],\n","        [2.8208],\n","        [0.5045],\n","        [0.6578],\n","        [1.7604],\n","        [1.3785],\n","        [1.7732],\n","        [0.5658],\n","        [0.6935],\n","        [1.7604],\n","        [1.3341],\n","        [0.5423],\n","        [0.4661],\n","        [0.7780],\n","        [0.5414],\n","        [0.4795],\n","        [0.4838],\n","        [1.1083],\n","        [0.5081],\n","        [0.4826],\n","        [0.4717],\n","        [2.1962],\n","        [0.5442],\n","        [0.4980],\n","        [0.4992],\n","        [0.7059],\n","        [2.9183],\n","        [0.8778],\n","        [0.6935],\n","        [0.6922],\n","        [1.1662],\n","        [1.4993],\n","        [0.5185],\n","        [0.4660],\n","        [0.5720],\n","        [0.8048],\n","        [1.7463],\n","        [1.1187],\n","        [1.0655],\n","        [0.4816],\n","        [0.6557],\n","        [1.8491],\n","        [0.4853],\n","        [1.9005],\n","        [0.9057],\n","        [0.6094],\n","        [0.9627],\n","        [0.4654],\n","        [1.3714],\n","        [1.8338],\n","        [1.3014],\n","        [1.7037],\n","        [0.5200],\n","        [0.8989],\n","        [1.2406],\n","        [0.9127],\n","        [0.8778],\n","        [2.2242],\n","        [1.7328],\n","        [1.2988],\n","        [0.4791],\n","        [0.4654],\n","        [1.2468],\n","        [0.9621],\n","        [1.2520],\n","        [2.5459],\n","        [1.1786],\n","        [0.7247],\n","        [0.6639],\n","        [1.3699],\n","        [2.4837],\n","        [1.8512],\n","        [1.7650],\n","        [2.4383],\n","        [0.6385],\n","        [0.9247],\n","        [0.9636],\n","        [0.7190],\n","        [1.1966],\n","        [1.0357],\n","        [0.4885],\n","        [0.9581],\n","        [0.4911],\n","        [1.1662],\n","        [0.6091],\n","        [1.4579],\n","        [0.8563],\n","        [0.6119]], device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  672\n","Ratio of trace to the matrix sum:  tensor(0.0030, device='cuda:0')\n","Here, trace is 1.9998656511306763 and matrix sum is 671.954833984375 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([672, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 336])\n","Previous layer shape is  torch.Size([672])\n","Layer ('_gnn._conv_layers.3.nn.2.weight', '_gnn._conv_layers.3.nn.2.weight') shape is torch.Size([256, 336]) and torch.Size([512, 672])\n","shape of layer: model 0 torch.Size([256, 336])\n","shape of layer: model 1 torch.Size([512, 672])\n","shape of previous transport map torch.Size([336, 672])\n","fc_layer0_weight_data.shape = torch.Size([256, 336])\n","T_var.shape = torch.Size([336, 672])\n","aligned_wt shape is torch.Size([256, 672])\n","fc_layer1_weight_data shape is torch.Size([512, 672])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[2.3158, 2.3001, 2.2884,  ..., 2.3178, 2.3433, 2.3242],\n","        [1.5538, 1.5229, 1.5251,  ..., 1.5139, 1.5427, 1.5717],\n","        [1.1655, 1.1710, 1.2205,  ..., 1.1962, 1.1755, 1.1809],\n","        ...,\n","        [1.1925, 1.1549, 1.1735,  ..., 1.1827, 1.1901, 1.1653],\n","        [1.6663, 1.6165, 1.5609,  ..., 1.6415, 1.6349, 1.6236],\n","        [2.3725, 2.3183, 2.3374,  ..., 2.3255, 2.3100, 2.3083]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","mu shape is (256,)\n","nu shape is (512,)\n","cpuM shape is (256, 512)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([512])\n","inverse marginals beta is  tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n","        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020],\n","       device='cuda:0')\n","tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n","        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999],\n","       device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 672])\n","Shape of fc_layer0_weight_data is  torch.Size([256, 336])\n","Previous layer shape is  torch.Size([512, 672])\n","Layer ('_gnn._conv_layers.3.nn.2.bias', '_gnn._conv_layers.3.nn.2.bias') shape is torch.Size([1, 256]) and torch.Size([1, 512])\n","shape of layer: model 0 torch.Size([1, 256])\n","shape of layer: model 1 torch.Size([1, 512])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([1, 256])\n","T_var.shape = torch.Size([256, 512])\n","aligned_wt shape is torch.Size([512, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 512])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[ 2.6620],\n","        [ 0.9510],\n","        [ 0.6668],\n","        [ 0.7589],\n","        [ 1.5951],\n","        [ 0.7713],\n","        [ 6.3653],\n","        [ 1.5024],\n","        [ 0.6741],\n","        [ 1.6851],\n","        [ 0.6095],\n","        [ 0.7627],\n","        [ 1.1414],\n","        [ 0.5150],\n","        [ 6.1978],\n","        [ 0.6638],\n","        [ 1.7005],\n","        [ 0.8123],\n","        [ 0.6832],\n","        [ 0.5146],\n","        [ 0.6095],\n","        [ 0.8937],\n","        [ 1.0714],\n","        [ 0.6773],\n","        [ 0.5320],\n","        [ 0.6932],\n","        [ 0.5533],\n","        [ 1.2018],\n","        [ 1.7494],\n","        [ 5.3606],\n","        [ 0.7198],\n","        [ 1.1339],\n","        [ 1.4189],\n","        [ 2.6620],\n","        [ 2.5614],\n","        [ 0.5093],\n","        [ 1.2256],\n","        [ 1.6827],\n","        [ 0.6109],\n","        [ 0.5115],\n","        [ 0.7342],\n","        [ 0.5068],\n","        [ 1.1476],\n","        [ 0.7837],\n","        [ 1.1090],\n","        [ 0.5843],\n","        [ 0.5636],\n","        [ 0.6072],\n","        [ 0.9707],\n","        [ 1.2176],\n","        [ 1.5023],\n","        [ 0.8107],\n","        [ 2.1969],\n","        [ 0.9834],\n","        [ 2.2480],\n","        [ 0.8234],\n","        [ 1.5498],\n","        [ 1.1414],\n","        [ 1.2762],\n","        [ 0.6160],\n","        [ 1.6040],\n","        [ 0.5263],\n","        [ 0.5150],\n","        [ 0.6638],\n","        [ 0.9866],\n","        [ 0.5879],\n","        [ 0.8571],\n","        [ 0.7201],\n","        [ 0.5605],\n","        [ 0.5129],\n","        [ 0.7589],\n","        [ 3.8205],\n","        [ 0.7201],\n","        [ 4.3002],\n","        [ 2.0670],\n","        [16.4312],\n","        [ 0.9823],\n","        [ 0.6972],\n","        [ 5.5784],\n","        [ 1.2113],\n","        [ 0.7118],\n","        [ 0.7931],\n","        [ 0.6668],\n","        [ 1.1357],\n","        [ 0.9348],\n","        [ 0.6708],\n","        [ 1.4507],\n","        [ 0.5097],\n","        [ 3.0744],\n","        [ 1.8434],\n","        [ 1.2042],\n","        [ 0.8530],\n","        [ 0.5164],\n","        [ 3.4746],\n","        [ 1.9242],\n","        [ 4.1124],\n","        [ 0.5308],\n","        [ 1.4764],\n","        [ 2.5032],\n","        [ 0.5132],\n","        [ 1.3036],\n","        [ 0.5064],\n","        [ 1.4764],\n","        [ 1.5302],\n","        [ 0.8997],\n","        [ 0.5110],\n","        [ 1.2176],\n","        [ 2.2480],\n","        [ 1.9226],\n","        [ 1.7034],\n","        [ 0.5069],\n","        [ 0.5587],\n","        [ 4.4111],\n","        [ 1.0160],\n","        [ 0.9536],\n","        [ 1.1476],\n","        [ 1.6294],\n","        [ 1.0527],\n","        [ 2.4122],\n","        [ 0.8145],\n","        [ 0.5843],\n","        [ 1.2762],\n","        [ 0.9848],\n","        [ 2.2380],\n","        [ 0.5088],\n","        [ 0.6809],\n","        [ 0.5533],\n","        [ 1.0905],\n","        [ 1.6851],\n","        [ 0.5291],\n","        [ 5.2846],\n","        [ 0.7937],\n","        [ 1.8867],\n","        [ 0.9707],\n","        [ 0.5291],\n","        [ 0.5279],\n","        [ 0.9848],\n","        [ 3.2104],\n","        [ 1.2905],\n","        [ 0.5068],\n","        [ 0.5783],\n","        [ 0.5974],\n","        [ 1.7349],\n","        [ 0.9008],\n","        [ 0.7931],\n","        [ 0.5597],\n","        [ 1.2973],\n","        [ 1.1299],\n","        [ 2.2380],\n","        [ 0.7719],\n","        [ 0.5832],\n","        [ 6.1978],\n","        [ 1.9468],\n","        [ 0.5882],\n","        [ 2.9358],\n","        [ 1.8084],\n","        [ 4.1377],\n","        [ 0.6890],\n","        [ 1.0160],\n","        [ 3.5594],\n","        [ 0.5620],\n","        [ 1.9226],\n","        [ 0.9348],\n","        [ 3.0744],\n","        [ 0.7118],\n","        [ 1.0039],\n","        [ 3.4632],\n","        [ 1.8084],\n","        [ 4.2507],\n","        [ 1.5256],\n","        [ 1.5024],\n","        [ 1.7341],\n","        [ 0.6520],\n","        [ 1.6484],\n","        [ 1.6736],\n","        [ 3.7720],\n","        [ 0.5069],\n","        [ 1.2368],\n","        [ 1.2018],\n","        [ 0.8571],\n","        [ 1.8522],\n","        [ 0.6186],\n","        [ 0.5342],\n","        [ 0.6292],\n","        [ 0.8364],\n","        [ 0.7781],\n","        [ 0.6603],\n","        [ 0.6160],\n","        [ 1.5707],\n","        [ 2.7325],\n","        [ 1.2361],\n","        [ 0.8487],\n","        [ 0.5086],\n","        [ 0.9008],\n","        [ 1.1299],\n","        [ 2.1941],\n","        [ 2.3551],\n","        [ 0.7220],\n","        [ 0.6292],\n","        [ 0.5336],\n","        [ 0.8192],\n","        [ 4.2659],\n","        [ 0.7713],\n","        [ 3.2104],\n","        [ 0.5072],\n","        [ 6.2541],\n","        [ 0.6932],\n","        [ 0.6072],\n","        [ 1.7034],\n","        [ 0.8954],\n","        [ 1.4323],\n","        [ 0.5999],\n","        [ 1.6736],\n","        [ 6.3653],\n","        [ 0.5342],\n","        [ 3.4559],\n","        [ 2.0948],\n","        [ 1.9753],\n","        [ 0.7094],\n","        [ 0.7220],\n","        [ 8.2801],\n","        [ 1.2361],\n","        [ 0.5832],\n","        [ 3.1894],\n","        [ 0.5783],\n","        [ 0.5176],\n","        [ 0.6732],\n","        [ 0.7507],\n","        [ 1.9123],\n","        [ 1.6484],\n","        [ 2.2124],\n","        [ 1.5498],\n","        [ 0.6645],\n","        [ 5.4058],\n","        [ 0.8107],\n","        [ 2.9825],\n","        [ 0.5069],\n","        [ 1.4028],\n","        [ 1.0039],\n","        [ 3.5425],\n","        [ 2.5661],\n","        [ 1.7349],\n","        [ 1.3021],\n","        [ 0.5956],\n","        [ 0.7343],\n","        [ 0.7640],\n","        [ 1.2562],\n","        [ 1.4028],\n","        [ 5.3606],\n","        [ 0.5164],\n","        [ 2.5509],\n","        [ 1.2562],\n","        [ 0.5131],\n","        [ 1.6137],\n","        [ 0.6520],\n","        [ 2.5032],\n","        [ 0.6232],\n","        [ 0.8905],\n","        [ 0.6603],\n","        [ 0.6809],\n","        [ 0.8487],\n","        [ 0.5064],\n","        [ 0.9603],\n","        [ 5.7595],\n","        [ 0.8692],\n","        [ 0.9715],\n","        [ 2.8017],\n","        [ 0.5420],\n","        [ 0.7640],\n","        [ 0.7094],\n","        [ 1.2256],\n","        [ 0.7294],\n","        [ 0.8145],\n","        [ 1.5302],\n","        [ 3.4632],\n","        [ 2.0940],\n","        [ 2.3551],\n","        [ 1.9242],\n","        [ 0.5131],\n","        [ 2.9358],\n","        [ 1.2495],\n","        [ 5.7595],\n","        [ 1.2113],\n","        [ 0.5580],\n","        [ 0.8692],\n","        [ 3.7720],\n","        [ 3.2108],\n","        [ 3.8205],\n","        [ 0.6853],\n","        [ 0.6232],\n","        [ 2.1941],\n","        [ 3.0492],\n","        [ 0.5519],\n","        [ 2.0940],\n","        [ 1.8696],\n","        [ 1.0669],\n","        [ 2.5614],\n","        [ 0.5097],\n","        [ 1.1357],\n","        [ 2.0948],\n","        [ 0.9866],\n","        [ 1.5023],\n","        [ 1.6040],\n","        [ 0.5492],\n","        [ 2.1707],\n","        [ 1.5376],\n","        [ 0.5437],\n","        [16.4312],\n","        [ 4.4111],\n","        [ 0.8905],\n","        [ 1.1035],\n","        [ 0.6732],\n","        [ 1.0714],\n","        [ 1.6294],\n","        [ 2.1969],\n","        [ 0.6853],\n","        [ 2.2124],\n","        [ 1.0376],\n","        [ 1.8434],\n","        [ 2.5661],\n","        [ 0.7342],\n","        [ 0.5129],\n","        [ 0.6741],\n","        [ 1.1035],\n","        [ 0.7507],\n","        [ 0.5279],\n","        [ 5.5784],\n","        [ 0.5877],\n","        [ 0.7950],\n","        [ 4.3213],\n","        [ 1.2368],\n","        [ 1.8522],\n","        [ 0.5877],\n","        [ 0.5999],\n","        [ 1.6137],\n","        [ 0.9996],\n","        [ 1.0961],\n","        [ 0.5069],\n","        [ 1.5376],\n","        [ 0.8954],\n","        [ 2.2079],\n","        [ 0.7837],\n","        [ 0.7937],\n","        [ 5.2436],\n","        [ 0.5437],\n","        [ 1.1339],\n","        [ 0.5071],\n","        [ 1.9191],\n","        [ 1.6827],\n","        [ 1.2414],\n","        [ 0.5519],\n","        [ 2.5509],\n","        [ 0.5068],\n","        [ 0.9742],\n","        [ 1.2973],\n","        [ 0.9715],\n","        [ 1.5256],\n","        [ 0.8889],\n","        [ 2.2236],\n","        [ 1.7494],\n","        [ 5.4058],\n","        [ 0.5088],\n","        [ 0.5176],\n","        [ 0.5320],\n","        [ 0.6109],\n","        [ 0.6773],\n","        [ 0.5974],\n","        [ 0.5336],\n","        [ 0.8364],\n","        [ 0.8064],\n","        [ 2.9825],\n","        [ 5.2436],\n","        [ 0.8997],\n","        [ 4.8883],\n","        [ 1.4189],\n","        [ 1.9123],\n","        [ 0.8937],\n","        [ 0.7781],\n","        [ 0.5263],\n","        [ 2.5076],\n","        [ 1.3829],\n","        [ 0.5470],\n","        [ 2.7325],\n","        [ 0.8311],\n","        [ 5.2846],\n","        [ 0.5620],\n","        [ 0.5580],\n","        [ 3.4298],\n","        [ 4.1377],\n","        [ 4.3213],\n","        [ 4.2659],\n","        [ 0.9603],\n","        [ 1.9468],\n","        [ 0.8123],\n","        [ 1.0669],\n","        [ 0.7719],\n","        [ 0.6361],\n","        [ 0.5306],\n","        [ 0.5308],\n","        [ 4.1124],\n","        [ 1.7341],\n","        [ 0.5066],\n","        [ 1.2495],\n","        [ 1.8696],\n","        [ 0.9742],\n","        [ 0.5470],\n","        [ 0.6832],\n","        [ 0.5063],\n","        [ 0.9823],\n","        [ 1.8374],\n","        [ 0.5063],\n","        [ 6.2541],\n","        [ 0.7294],\n","        [ 0.6178],\n","        [ 1.9191],\n","        [ 3.1894],\n","        [ 8.2801],\n","        [ 0.5064],\n","        [ 0.5420],\n","        [ 2.1707],\n","        [ 0.6178],\n","        [ 0.5666],\n","        [ 0.5918],\n","        [ 0.5636],\n","        [ 0.6708],\n","        [ 0.5597],\n","        [ 2.3683],\n","        [ 0.6645],\n","        [ 0.5132],\n","        [ 0.5492],\n","        [ 1.3021],\n","        [ 0.5072],\n","        [ 0.5110],\n","        [ 0.9834],\n","        [ 1.2905],\n","        [ 0.5066],\n","        [ 3.5594],\n","        [ 0.6186],\n","        [ 0.8064],\n","        [ 0.5306],\n","        [ 0.7343],\n","        [ 0.5064],\n","        [ 0.6972],\n","        [ 1.7117],\n","        [ 4.8883],\n","        [ 2.5076],\n","        [ 1.7117],\n","        [ 0.7198],\n","        [ 3.2108],\n","        [ 1.1090],\n","        [ 1.0075],\n","        [ 0.8234],\n","        [ 0.5879],\n","        [ 1.0376],\n","        [ 1.2414],\n","        [ 1.5707],\n","        [ 0.8192],\n","        [ 4.3002],\n","        [ 0.5918],\n","        [ 0.5666],\n","        [ 1.3829],\n","        [ 2.2079],\n","        [ 0.8889],\n","        [ 2.3683],\n","        [ 3.5425],\n","        [ 0.5146],\n","        [ 1.0075],\n","        [ 0.8530],\n","        [ 0.5956],\n","        [ 0.5605],\n","        [ 1.8374],\n","        [ 3.4746],\n","        [ 1.0527],\n","        [ 1.3036],\n","        [ 3.4559],\n","        [ 0.6361],\n","        [ 0.5587],\n","        [ 0.5115],\n","        [ 0.5882],\n","        [ 1.8867],\n","        [ 0.5093],\n","        [ 0.6191],\n","        [ 4.2507],\n","        [ 0.7418],\n","        [ 0.7950],\n","        [ 1.9753],\n","        [ 2.4122],\n","        [ 1.4507],\n","        [ 0.8311],\n","        [ 0.7418],\n","        [ 0.9996],\n","        [ 0.5068],\n","        [ 0.7627],\n","        [ 0.6191],\n","        [ 1.7005],\n","        [ 1.2042],\n","        [ 2.8017],\n","        [ 0.9449],\n","        [ 0.6890],\n","        [ 1.0905],\n","        [ 0.9510],\n","        [ 1.0961],\n","        [ 0.5086],\n","        [ 2.0670],\n","        [ 1.4323],\n","        [ 2.2236],\n","        [ 0.9536],\n","        [ 3.4298],\n","        [ 1.5951],\n","        [ 3.0492],\n","        [ 0.5071],\n","        [ 0.9449]], device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  512\n","Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n","Here, trace is 0.0 and matrix sum is 511.97381591796875 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([512, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 256])\n","Previous layer shape is  torch.Size([512])\n","Layer ('_gnn._post_processing.0.weight', '_gnn._post_processing.0.weight') shape is torch.Size([336, 1041]) and torch.Size([336, 2065])\n","shape of layer: model 0 torch.Size([336, 1041])\n","shape of layer: model 1 torch.Size([336, 2065])\n","shape of previous transport map torch.Size([256, 512])\n","fc_layer0_weight_data.shape = torch.Size([336, 1041])\n","T_var.shape = torch.Size([256, 512])\n","T_var.shape = torch.Size([1024, 2048])\n","aligned_wt shape is torch.Size([336, 2065])\n","fc_layer1_weight_data shape is torch.Size([336, 2065])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[3.2391, 3.2454, 3.2412,  ..., 3.2345, 3.2562, 3.2307],\n","        [2.0523, 2.0652, 2.0535,  ..., 2.0495, 2.0408, 2.0329],\n","        [2.9124, 2.9153, 2.9046,  ..., 2.9132, 2.8835, 2.9259],\n","        ...,\n","        [2.4164, 2.4271, 2.4113,  ..., 2.4092, 2.4198, 2.4096],\n","        [2.0675, 2.0900, 2.0941,  ..., 2.0557, 2.0685, 2.0814],\n","        [2.8829, 2.8720, 2.8823,  ..., 2.8880, 2.8574, 2.8871]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  336\n","mu shape is (336,)\n","nu shape is (336,)\n","cpuM shape is (336, 336)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([336])\n","inverse marginals beta is  tensor([0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","        0.0030, 0.0030, 0.0030], device='cuda:0')\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0060, device='cuda:0')\n","Here, trace is 1.999932885169983 and matrix sum is 335.9887390136719 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([336, 2065])\n","Shape of fc_layer0_weight_data is  torch.Size([336, 1041])\n","Previous layer shape is  torch.Size([336, 2065])\n","Layer ('_gnn._post_processing.0.bias', '_gnn._post_processing.0.bias') shape is torch.Size([1, 336]) and torch.Size([1, 336])\n","shape of layer: model 0 torch.Size([1, 336])\n","shape of layer: model 1 torch.Size([1, 336])\n","shape of previous transport map torch.Size([336, 336])\n","fc_layer0_weight_data.shape = torch.Size([1, 336])\n","T_var.shape = torch.Size([336, 336])\n","aligned_wt shape is torch.Size([336, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 336])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[0.5465],\n","        [0.3432],\n","        [0.3853],\n","        [1.0053],\n","        [0.6324],\n","        [0.4439],\n","        [0.2379],\n","        [0.3491],\n","        [0.9540],\n","        [0.7598],\n","        [1.2602],\n","        [0.4035],\n","        [0.9475],\n","        [0.2495],\n","        [0.2360],\n","        [0.4291],\n","        [0.2355],\n","        [0.9062],\n","        [1.2299],\n","        [0.6084],\n","        [0.3160],\n","        [0.4451],\n","        [0.2489],\n","        [0.5280],\n","        [0.3608],\n","        [0.2747],\n","        [0.2659],\n","        [0.2721],\n","        [0.4101],\n","        [0.4996],\n","        [0.3259],\n","        [0.4612],\n","        [0.5388],\n","        [0.5081],\n","        [0.2359],\n","        [1.1079],\n","        [0.2925],\n","        [0.3327],\n","        [0.9170],\n","        [1.5291],\n","        [1.0411],\n","        [0.2819],\n","        [0.4052],\n","        [0.2381],\n","        [0.8267],\n","        [0.4112],\n","        [0.7146],\n","        [0.2446],\n","        [0.2777],\n","        [0.5735],\n","        [0.4959],\n","        [0.2296],\n","        [0.4761],\n","        [0.3288],\n","        [1.4013],\n","        [0.9845],\n","        [0.7754],\n","        [0.3143],\n","        [0.3189],\n","        [0.2313],\n","        [0.3827],\n","        [0.2925],\n","        [0.4678],\n","        [0.4855],\n","        [0.2735],\n","        [0.4589],\n","        [0.8796],\n","        [0.2384],\n","        [0.2869],\n","        [0.3539],\n","        [0.4820],\n","        [0.4077],\n","        [0.4720],\n","        [0.3389],\n","        [0.6349],\n","        [0.3649],\n","        [0.2516],\n","        [0.2400],\n","        [0.3547],\n","        [0.5820],\n","        [0.3403],\n","        [0.2473],\n","        [0.2317],\n","        [0.2680],\n","        [0.2763],\n","        [0.4110],\n","        [0.4398],\n","        [1.0462],\n","        [0.3634],\n","        [0.2834],\n","        [1.4370],\n","        [0.5678],\n","        [0.3935],\n","        [0.9557],\n","        [0.3800],\n","        [1.1986],\n","        [0.2564],\n","        [0.2474],\n","        [0.2741],\n","        [0.6072],\n","        [0.2615],\n","        [0.4177],\n","        [0.2624],\n","        [0.2412],\n","        [0.7215],\n","        [0.4071],\n","        [0.9008],\n","        [0.3674],\n","        [0.2317],\n","        [0.3737],\n","        [0.3783],\n","        [0.2780],\n","        [0.6990],\n","        [0.2350],\n","        [0.3239],\n","        [0.2818],\n","        [0.3752],\n","        [0.3561],\n","        [0.7885],\n","        [1.3009],\n","        [0.3837],\n","        [0.5369],\n","        [0.4469],\n","        [0.2594],\n","        [0.3202],\n","        [1.7932],\n","        [0.2296],\n","        [1.1560],\n","        [0.3407],\n","        [0.7686],\n","        [0.5571],\n","        [0.2406],\n","        [0.4977],\n","        [0.3307],\n","        [0.2957],\n","        [0.5674],\n","        [0.4769],\n","        [0.3851],\n","        [1.2640],\n","        [0.2296],\n","        [0.3711],\n","        [0.2831],\n","        [0.5130],\n","        [0.4424],\n","        [0.2912],\n","        [0.2315],\n","        [0.2929],\n","        [0.2313],\n","        [0.3681],\n","        [1.5045],\n","        [0.4871],\n","        [1.0370],\n","        [0.4047],\n","        [0.5921],\n","        [0.2682],\n","        [0.2810],\n","        [0.8032],\n","        [0.3083],\n","        [0.3920],\n","        [0.6736],\n","        [0.4941],\n","        [0.4283],\n","        [0.2347],\n","        [0.2748],\n","        [0.9280],\n","        [0.2516],\n","        [0.5400],\n","        [0.7192],\n","        [0.3541],\n","        [0.7791],\n","        [0.7685],\n","        [0.3555],\n","        [0.6279],\n","        [0.3444],\n","        [0.2362],\n","        [0.3081],\n","        [0.5218],\n","        [0.2583],\n","        [0.9492],\n","        [0.8039],\n","        [0.4162],\n","        [0.2468],\n","        [0.5461],\n","        [0.5247],\n","        [0.3712],\n","        [0.9263],\n","        [2.1812],\n","        [1.4275],\n","        [0.2339],\n","        [0.8244],\n","        [1.0551],\n","        [0.7516],\n","        [0.2660],\n","        [0.4097],\n","        [0.6298],\n","        [0.5957],\n","        [0.2798],\n","        [0.5822],\n","        [0.3417],\n","        [1.3308],\n","        [0.3373],\n","        [0.2917],\n","        [0.4237],\n","        [0.5505],\n","        [0.6725],\n","        [0.2343],\n","        [0.4057],\n","        [0.2373],\n","        [0.3421],\n","        [0.6080],\n","        [0.7640],\n","        [0.5565],\n","        [0.4622],\n","        [1.5015],\n","        [0.5423],\n","        [0.2751],\n","        [0.4809],\n","        [0.4219],\n","        [0.3510],\n","        [0.3090],\n","        [0.3609],\n","        [0.4325],\n","        [0.5104],\n","        [0.2312],\n","        [0.4158],\n","        [0.5644],\n","        [1.7723],\n","        [0.9929],\n","        [1.1050],\n","        [0.5244],\n","        [0.2300],\n","        [0.4026],\n","        [0.3250],\n","        [0.4384],\n","        [0.2310],\n","        [0.6176],\n","        [0.4047],\n","        [1.3325],\n","        [0.3699],\n","        [0.9357],\n","        [0.8061],\n","        [0.8265],\n","        [0.3040],\n","        [0.7578],\n","        [1.4554],\n","        [0.8610],\n","        [0.3557],\n","        [0.5226],\n","        [0.3559],\n","        [0.5623],\n","        [0.2622],\n","        [0.5098],\n","        [0.7024],\n","        [0.4129],\n","        [0.5611],\n","        [1.2173],\n","        [0.3404],\n","        [0.9872],\n","        [0.3453],\n","        [0.2388],\n","        [0.2498],\n","        [0.2311],\n","        [0.2441],\n","        [1.5402],\n","        [0.2734],\n","        [1.8286],\n","        [0.2783],\n","        [0.4605],\n","        [1.2916],\n","        [0.6851],\n","        [0.2905],\n","        [0.2999],\n","        [0.3078],\n","        [0.6613],\n","        [0.7126],\n","        [0.5416],\n","        [0.6721],\n","        [1.2173],\n","        [0.2299],\n","        [0.5816],\n","        [0.2823],\n","        [0.7560],\n","        [0.6622],\n","        [0.5593],\n","        [0.9639],\n","        [0.6068],\n","        [0.6098],\n","        [0.8386],\n","        [0.2332],\n","        [0.9971],\n","        [0.5261],\n","        [0.7295],\n","        [0.4871],\n","        [0.2394],\n","        [0.5747],\n","        [0.3286],\n","        [0.4532],\n","        [1.8859],\n","        [0.5390],\n","        [0.2571],\n","        [1.0639],\n","        [0.2402],\n","        [0.6000],\n","        [0.5779],\n","        [0.5015],\n","        [0.4770],\n","        [0.5711],\n","        [0.3674],\n","        [0.2354],\n","        [0.8283],\n","        [0.3870],\n","        [0.3762],\n","        [1.0799],\n","        [0.5016],\n","        [0.3424],\n","        [0.5397],\n","        [0.7297],\n","        [0.2296],\n","        [0.7886],\n","        [0.2824],\n","        [0.3272],\n","        [0.2600],\n","        [0.2853],\n","        [0.3642],\n","        [0.3925],\n","        [0.5637],\n","        [0.8798],\n","        [0.3618],\n","        [0.2962],\n","        [0.2886],\n","        [1.3471],\n","        [0.2594],\n","        [0.7313],\n","        [0.2542],\n","        [0.9030],\n","        [0.2404]], device='cuda:0')\n","returns a uniform measure of cardinality:  336\n","returns a uniform measure of cardinality:  336\n","Ratio of trace to the matrix sum:  tensor(0.0060, device='cuda:0')\n","Here, trace is 1.999932885169983 and matrix sum is 335.9887390136719 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([336, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 336])\n","Previous layer shape is  torch.Size([336])\n","Layer ('_gnn._post_processing.2.weight', '_gnn._post_processing.2.weight') shape is torch.Size([256, 336]) and torch.Size([256, 336])\n","shape of layer: model 0 torch.Size([256, 336])\n","shape of layer: model 1 torch.Size([256, 336])\n","shape of previous transport map torch.Size([336, 336])\n","fc_layer0_weight_data.shape = torch.Size([256, 336])\n","T_var.shape = torch.Size([336, 336])\n","aligned_wt shape is torch.Size([256, 336])\n","fc_layer1_weight_data shape is torch.Size([256, 336])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.3265, 1.3515, 1.3050,  ..., 1.2969, 1.2672, 1.3137],\n","        [1.5782, 1.5736, 1.5645,  ..., 1.5643, 1.5408, 1.5488],\n","        [1.2978, 1.3965, 1.3266,  ..., 1.3770, 1.3527, 1.3798],\n","        ...,\n","        [1.8274, 1.8352, 1.8339,  ..., 1.8380, 1.7529, 1.8529],\n","        [1.9698, 2.0362, 2.0479,  ..., 2.0220, 1.9965, 1.9959],\n","        [1.6139, 1.6052, 1.6668,  ..., 1.7049, 1.6214, 1.5965]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  256\n","mu shape is (256,)\n","nu shape is (256,)\n","cpuM shape is (256, 256)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([256])\n","inverse marginals beta is  tensor([0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039, 0.0039,\n","        0.0039, 0.0039, 0.0039, 0.0039], device='cuda:0')\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 336])\n","Shape of fc_layer0_weight_data is  torch.Size([256, 336])\n","Previous layer shape is  torch.Size([256, 336])\n","Layer ('_gnn._post_processing.2.bias', '_gnn._post_processing.2.bias') shape is torch.Size([1, 256]) and torch.Size([1, 256])\n","shape of layer: model 0 torch.Size([1, 256])\n","shape of layer: model 1 torch.Size([1, 256])\n","shape of previous transport map torch.Size([256, 256])\n","fc_layer0_weight_data.shape = torch.Size([1, 256])\n","T_var.shape = torch.Size([256, 256])\n","aligned_wt shape is torch.Size([256, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 256])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[0.8518],\n","        [1.0153],\n","        [1.8806],\n","        [0.5473],\n","        [0.5351],\n","        [0.9526],\n","        [0.7839],\n","        [1.3311],\n","        [0.7394],\n","        [0.7988],\n","        [0.9206],\n","        [0.5752],\n","        [0.7647],\n","        [0.5277],\n","        [0.6125],\n","        [1.0314],\n","        [0.5265],\n","        [1.4566],\n","        [1.0385],\n","        [0.7044],\n","        [0.5764],\n","        [0.7758],\n","        [0.5532],\n","        [0.6679],\n","        [0.5995],\n","        [0.8177],\n","        [0.8918],\n","        [0.5797],\n","        [1.0133],\n","        [0.7619],\n","        [0.6148],\n","        [0.8067],\n","        [0.7342],\n","        [0.8025],\n","        [0.5271],\n","        [0.5303],\n","        [1.3028],\n","        [0.5411],\n","        [1.3051],\n","        [0.5671],\n","        [1.0854],\n","        [0.5274],\n","        [1.4369],\n","        [1.1295],\n","        [0.6759],\n","        [0.5460],\n","        [0.6083],\n","        [0.6180],\n","        [0.7968],\n","        [0.8844],\n","        [1.1774],\n","        [0.7341],\n","        [0.5358],\n","        [1.1245],\n","        [1.5720],\n","        [0.8839],\n","        [0.5419],\n","        [1.0842],\n","        [0.6724],\n","        [0.6096],\n","        [0.5292],\n","        [0.5355],\n","        [0.9865],\n","        [1.0050],\n","        [1.0814],\n","        [1.0274],\n","        [0.6714],\n","        [0.8701],\n","        [0.5758],\n","        [0.7264],\n","        [0.5300],\n","        [1.6202],\n","        [0.5432],\n","        [0.7814],\n","        [1.1475],\n","        [1.1783],\n","        [1.0271],\n","        [0.6877],\n","        [0.5266],\n","        [0.9254],\n","        [0.6153],\n","        [0.8571],\n","        [0.6724],\n","        [0.6876],\n","        [0.5314],\n","        [0.5393],\n","        [0.5692],\n","        [1.2914],\n","        [1.5264],\n","        [0.7276],\n","        [0.8166],\n","        [0.6115],\n","        [0.8583],\n","        [0.7018],\n","        [0.8946],\n","        [0.7421],\n","        [0.5283],\n","        [0.5369],\n","        [0.6206],\n","        [0.7991],\n","        [1.0333],\n","        [1.4966],\n","        [0.7259],\n","        [1.4288],\n","        [0.8313],\n","        [1.2746],\n","        [0.5429],\n","        [1.3279],\n","        [0.5265],\n","        [1.2463],\n","        [0.7865],\n","        [0.5288],\n","        [1.3889],\n","        [0.6027],\n","        [1.4203],\n","        [1.1990],\n","        [0.7576],\n","        [2.1939],\n","        [0.6726],\n","        [0.9089],\n","        [1.7619],\n","        [1.7204],\n","        [1.2847],\n","        [1.3271],\n","        [0.6957],\n","        [0.6572],\n","        [0.9401],\n","        [0.6576],\n","        [0.6198],\n","        [0.5336],\n","        [0.7495],\n","        [0.5288],\n","        [1.1569],\n","        [0.6507],\n","        [0.5264],\n","        [0.5278],\n","        [1.3837],\n","        [0.5399],\n","        [0.5521],\n","        [0.5313],\n","        [1.5502],\n","        [0.7370],\n","        [0.5321],\n","        [0.5282],\n","        [0.7399],\n","        [1.1017],\n","        [0.5638],\n","        [0.6116],\n","        [0.5755],\n","        [1.8200],\n","        [0.6049],\n","        [0.8572],\n","        [0.5782],\n","        [0.5321],\n","        [0.8684],\n","        [0.5484],\n","        [0.8870],\n","        [1.7931],\n","        [0.7965],\n","        [0.8802],\n","        [0.5264],\n","        [0.6205],\n","        [0.8168],\n","        [0.9686],\n","        [1.3500],\n","        [0.5409],\n","        [0.5268],\n","        [0.5480],\n","        [0.5289],\n","        [0.7994],\n","        [0.5288],\n","        [0.7666],\n","        [0.6600],\n","        [0.8798],\n","        [0.8766],\n","        [1.6872],\n","        [0.5300],\n","        [0.7377],\n","        [0.9412],\n","        [0.5972],\n","        [1.1926],\n","        [0.9325],\n","        [0.6950],\n","        [0.9074],\n","        [0.9462],\n","        [0.7979],\n","        [0.5361],\n","        [0.9445],\n","        [0.8970],\n","        [0.5502],\n","        [0.6049],\n","        [0.9124],\n","        [0.5306],\n","        [0.8802],\n","        [0.8004],\n","        [0.6893],\n","        [0.7814],\n","        [0.5265],\n","        [0.7108],\n","        [0.5407],\n","        [0.5269],\n","        [0.7172],\n","        [0.6499],\n","        [0.5279],\n","        [1.3835],\n","        [1.3549],\n","        [0.5347],\n","        [0.6670],\n","        [0.9676],\n","        [0.5979],\n","        [0.9903],\n","        [0.5831],\n","        [0.6404],\n","        [0.5924],\n","        [0.9229],\n","        [1.3716],\n","        [0.5354],\n","        [1.6241],\n","        [0.5677],\n","        [0.6544],\n","        [1.1158],\n","        [0.5714],\n","        [0.6382],\n","        [1.5313],\n","        [0.5745],\n","        [0.9106],\n","        [0.5356],\n","        [0.5512],\n","        [0.5390],\n","        [1.4023],\n","        [1.5558],\n","        [0.5782],\n","        [1.3601],\n","        [0.5795],\n","        [0.5983],\n","        [0.8686],\n","        [1.8047],\n","        [0.8255],\n","        [1.0070],\n","        [1.1922],\n","        [0.6873],\n","        [0.9522],\n","        [1.2739],\n","        [0.5856],\n","        [1.0267],\n","        [1.2324],\n","        [0.8863],\n","        [0.5311],\n","        [1.1366],\n","        [0.5831],\n","        [0.5467],\n","        [1.1449],\n","        [0.9872],\n","        [0.5331],\n","        [0.8472],\n","        [0.9533]], device='cuda:0')\n","returns a uniform measure of cardinality:  256\n","returns a uniform measure of cardinality:  256\n","Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n","Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([256, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 256])\n","Previous layer shape is  torch.Size([256])\n","Layer ('_gnn._readout.0.weight', '_gnn._readout.0.weight') shape is torch.Size([128, 768]) and torch.Size([128, 768])\n","shape of layer: model 0 torch.Size([128, 768])\n","shape of layer: model 1 torch.Size([128, 768])\n","shape of previous transport map torch.Size([256, 256])\n","fc_layer0_weight_data.shape = torch.Size([128, 768])\n","T_var.shape = torch.Size([256, 256])\n","torch.Size([128, 768]) torch.Size([256, 256])\n","aligned_wt shape is torch.Size([128, 768])\n","fc_layer1_weight_data shape is torch.Size([128, 768])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.9309, 1.9482, 1.9025,  ..., 1.9382, 1.9788, 1.9549],\n","        [1.1757, 1.1979, 1.1697,  ..., 1.1493, 1.1694, 1.1640],\n","        [2.1537, 2.1528, 2.1462,  ..., 2.1309, 2.1599, 2.1796],\n","        ...,\n","        [1.8461, 1.8493, 1.8361,  ..., 1.8178, 1.8062, 1.8474],\n","        [1.3697, 1.3640, 1.3526,  ..., 1.3788, 1.3644, 1.3180],\n","        [1.9487, 1.9478, 1.9126,  ..., 1.9256, 1.9162, 1.9529]],\n","       device='cuda:0')\n","returns a uniform measure of cardinality:  128\n","returns a uniform measure of cardinality:  128\n","mu shape is (128,)\n","nu shape is (128,)\n","cpuM shape is (128, 128)\n","the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([128])\n","inverse marginals beta is  tensor([0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n","        0.0078, 0.0078], device='cuda:0')\n","tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n","Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([128, 768])\n","Shape of fc_layer0_weight_data is  torch.Size([128, 768])\n","Previous layer shape is  torch.Size([128, 768])\n","Layer ('_gnn._readout.0.bias', '_gnn._readout.0.bias') shape is torch.Size([1, 128]) and torch.Size([1, 128])\n","shape of layer: model 0 torch.Size([1, 128])\n","shape of layer: model 1 torch.Size([1, 128])\n","shape of previous transport map torch.Size([128, 128])\n","fc_layer0_weight_data.shape = torch.Size([1, 128])\n","T_var.shape = torch.Size([128, 128])\n","aligned_wt shape is torch.Size([128, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 128])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[0.2616],\n","        [0.9502],\n","        [1.3420],\n","        [0.7724],\n","        [0.3514],\n","        [0.8229],\n","        [0.8336],\n","        [0.3493],\n","        [0.4431],\n","        [0.4253],\n","        [0.5755],\n","        [0.2525],\n","        [0.5444],\n","        [0.4667],\n","        [0.5072],\n","        [0.4273],\n","        [0.4912],\n","        [0.2548],\n","        [0.3688],\n","        [1.3504],\n","        [0.3793],\n","        [0.7757],\n","        [0.2929],\n","        [0.2588],\n","        [0.6623],\n","        [0.3819],\n","        [0.2503],\n","        [0.3161],\n","        [0.3409],\n","        [0.2531],\n","        [1.0556],\n","        [0.5204],\n","        [0.2508],\n","        [0.3174],\n","        [0.4966],\n","        [0.5029],\n","        [0.2843],\n","        [0.4931],\n","        [0.5899],\n","        [0.2723],\n","        [0.2846],\n","        [0.9342],\n","        [0.2502],\n","        [0.5070],\n","        [0.3893],\n","        [0.4445],\n","        [0.7765],\n","        [0.2504],\n","        [0.3319],\n","        [0.2724],\n","        [0.2508],\n","        [0.2692],\n","        [0.4444],\n","        [0.2533],\n","        [0.3059],\n","        [0.4020],\n","        [0.4007],\n","        [0.2925],\n","        [0.3834],\n","        [0.3069],\n","        [0.9146],\n","        [0.6924],\n","        [0.5303],\n","        [0.2743],\n","        [0.7946],\n","        [0.6563],\n","        [0.4238],\n","        [0.4579],\n","        [0.2502],\n","        [0.3031],\n","        [0.3327],\n","        [1.0235],\n","        [0.5845],\n","        [0.6771],\n","        [0.3518],\n","        [0.2949],\n","        [0.4448],\n","        [0.2508],\n","        [0.2553],\n","        [0.2526],\n","        [0.3037],\n","        [0.5202],\n","        [0.2662],\n","        [0.6669],\n","        [0.4014],\n","        [0.3532],\n","        [0.3195],\n","        [0.4188],\n","        [2.4371],\n","        [0.2969],\n","        [0.3506],\n","        [0.6561],\n","        [0.4653],\n","        [0.6092],\n","        [0.4050],\n","        [0.7044],\n","        [0.3377],\n","        [0.3024],\n","        [0.7545],\n","        [0.5855],\n","        [0.4503],\n","        [0.3642],\n","        [0.6219],\n","        [0.8500],\n","        [0.2963],\n","        [0.9893],\n","        [0.6278],\n","        [0.5485],\n","        [0.3937],\n","        [0.8613],\n","        [0.3723],\n","        [0.3046],\n","        [0.4140],\n","        [0.4845],\n","        [0.2657],\n","        [0.8484],\n","        [0.3151],\n","        [0.2818],\n","        [0.3641],\n","        [0.4473],\n","        [0.2569],\n","        [0.3038],\n","        [0.6799],\n","        [0.2718],\n","        [0.2989],\n","        [1.1292],\n","        [0.2619],\n","        [1.0599]], device='cuda:0')\n","returns a uniform measure of cardinality:  128\n","returns a uniform measure of cardinality:  128\n","Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n","Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([128, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 128])\n","Previous layer shape is  torch.Size([128])\n","Layer ('_tasks.0._affine.weight', '_tasks.0._affine.weight') shape is torch.Size([3, 128]) and torch.Size([3, 128])\n","shape of layer: model 0 torch.Size([3, 128])\n","shape of layer: model 1 torch.Size([3, 128])\n","shape of previous transport map torch.Size([128, 128])\n","fc_layer0_weight_data.shape = torch.Size([3, 128])\n","T_var.shape = torch.Size([128, 128])\n","aligned_wt shape is torch.Size([3, 128])\n","fc_layer1_weight_data shape is torch.Size([3, 128])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[1.0242, 1.0790, 1.0910],\n","        [1.1281, 1.0927, 0.9953],\n","        [1.0795, 1.0983, 1.0880]], device='cuda:0')\n","returns a uniform measure of cardinality:  3\n","returns a uniform measure of cardinality:  3\n","mu shape is (3,)\n","nu shape is (3,)\n","cpuM shape is (3, 3)\n","the transport map is  tensor([[0.3333, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.3333],\n","        [0.0000, 0.3333, 0.0000]], device='cuda:0')\n","shape of inverse marginals beta is  torch.Size([3])\n","inverse marginals beta is  tensor([0.3333, 0.3333, 0.3333], device='cuda:0')\n","tensor([1.0000, 1.0000, 1.0000], device='cuda:0')\n","Ratio of trace to the matrix sum:  tensor(0.3333, device='cuda:0')\n","Here, trace is 0.9999997019767761 and matrix sum is 2.9999990463256836 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([3, 128])\n","Shape of fc_layer0_weight_data is  torch.Size([3, 128])\n","Previous layer shape is  torch.Size([3, 128])\n","Layer ('_tasks.0._affine.bias', '_tasks.0._affine.bias') shape is torch.Size([1, 3]) and torch.Size([1, 3])\n","shape of layer: model 0 torch.Size([1, 3])\n","shape of layer: model 1 torch.Size([1, 3])\n","shape of previous transport map torch.Size([3, 3])\n","fc_layer0_weight_data.shape = torch.Size([1, 3])\n","T_var.shape = torch.Size([3, 3])\n","aligned_wt shape is torch.Size([3, 1])\n","fc_layer1_weight_data shape is torch.Size([1, 3])\n","Processing the coordinates to form ground_metric\n","dont leave off the squaring of the ground metric\n","ground metric is  tensor([[0.0567],\n","        [0.0511],\n","        [0.0550]], device='cuda:0')\n","returns a uniform measure of cardinality:  3\n","returns a uniform measure of cardinality:  3\n","Ratio of trace to the matrix sum:  tensor(0.3333, device='cuda:0')\n","Here, trace is 0.9999997019767761 and matrix sum is 2.9999990463256836 \n","this is past correction for weight mode\n","Shape of aligned wt is  torch.Size([3, 1])\n","Shape of fc_layer0_weight_data is  torch.Size([1, 3])\n"]}],"source":["_, model_large_mapped_state_dict, _ = get_wassersteinized_layers_modularized(args, [model_small.cuda(), model_large.cuda()])\n","model_large_mapped_state_dict = {\n","    k: v if not 'bias' in k else v[:, 0] \n","    for k, v in model_large_mapped_state_dict.items()\n","}"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["{'_gnn._conv_layers.0.nn.0.weight': tensor([[ 0.0332, -0.0824, -0.0883,  ..., -0.0690,  0.1500, -0.0187],\n","         [-0.0218, -0.0412, -0.0847,  ..., -0.0582,  0.0275,  0.0800],\n","         [-0.0923, -0.2520, -0.0319,  ..., -0.0799,  0.1501, -0.1278],\n","         ...,\n","         [ 0.0184, -0.0025,  0.0227,  ..., -0.1167,  0.1622,  0.0630],\n","         [ 0.0160,  0.0439,  0.0154,  ..., -0.1215, -0.0129, -0.0198],\n","         [-0.0344,  0.0107, -0.0051,  ...,  0.0452, -0.1165, -0.1032]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.0.nn.0.bias': tensor([ 0.0972, -0.0143, -0.0563,  0.1735, -0.1152, -0.0431, -0.0147,  0.1131,\n","         -0.0279, -0.2034, -0.0861,  0.0259, -0.0841,  0.1089, -0.2077, -0.0246,\n","         -0.0415, -0.1971,  0.0082, -0.0222,  0.0365,  0.0777, -0.1152, -0.2722,\n","          0.0303, -0.2197,  0.0287,  0.1358,  0.1201,  0.1457, -0.1649,  0.0323,\n","          0.0815,  0.0145, -0.0563,  0.0736,  0.0364, -0.0438, -0.2197,  0.1086,\n","         -0.0463, -0.1232, -0.2537, -0.1211,  0.1016,  0.0489,  0.0082, -0.0208,\n","         -0.1031,  0.0163, -0.1347, -0.0214,  0.0777, -0.2505, -0.0013, -0.0463,\n","          0.1318, -0.0305,  0.0365, -0.1633, -0.2395, -0.0222,  0.1629, -0.1649,\n","         -0.1153, -0.1570,  0.1016, -0.2132, -0.2395, -0.0865,  0.0308, -0.0488,\n","          0.0364,  0.0059, -0.2034, -0.0246,  0.1131, -0.0372,  0.0421,  0.0421,\n","         -0.1343, -0.1079, -0.0830, -0.0741, -0.1079, -0.2086,  0.2567, -0.1657,\n","         -0.0451,  0.2278,  0.1175, -0.0454, -0.0103, -0.0352,  0.1175, -0.0140,\n","         -0.0454,  0.1318, -0.2044,  0.0059, -0.1035, -0.1570,  0.0145, -0.0163,\n","          0.1246, -0.2132,  0.0815, -0.2002,  0.0618,  0.0303, -0.0305, -0.0977,\n","         -0.0488, -0.0865,  0.3033,  0.1352,  0.0088,  0.0239,  0.1177,  0.1329,\n","         -0.1971,  0.0239, -0.2499,  0.2278, -0.1540, -0.1211, -0.0035, -0.2537,\n","         -0.1459,  0.1177,  0.1181,  0.0732,  0.0736, -0.2722, -0.0841, -0.0279,\n","          0.0088,  0.1050,  0.0489, -0.2505, -0.0013, -0.0018, -0.0315,  0.1352,\n","         -0.0143,  0.0618, -0.1209,  0.1358, -0.1550, -0.1035,  0.0369, -0.0438,\n","          0.1181, -0.0956,  0.1086,  0.1246, -0.0103, -0.0752, -0.0315, -0.0163,\n","         -0.0038, -0.2394, -0.0372, -0.0352, -0.0415, -0.1550,  0.0812, -0.0833,\n","         -0.0956, -0.2297, -0.0830,  0.0279, -0.0632, -0.2002,  0.0791, -0.0147,\n","          0.1264, -0.1153,  0.0732,  0.0695, -0.0867, -0.1633,  0.1457, -0.3009,\n","         -0.1657, -0.0867, -0.0038, -0.1101, -0.1245, -0.1629,  0.1264, -0.0861,\n","         -0.1232,  0.0259,  0.0323, -0.1694,  0.0791, -0.0140, -0.1459,  0.2567,\n","         -0.2394, -0.1855,  0.0163,  0.1629,  0.0308, -0.1343,  0.0782,  0.0369,\n","          0.3033, -0.1855,  0.0782, -0.0208, -0.1031, -0.1101, -0.2499, -0.2679,\n","         -0.0833, -0.1209, -0.1347, -0.4402,  0.0279,  0.0812,  0.0237, -0.4402,\n","         -0.0640, -0.2086, -0.1540, -0.2297, -0.0977, -0.0768,  0.1089,  0.0287,\n","          0.1201, -0.2679, -0.0640, -0.3009, -0.0214, -0.0741, -0.2077, -0.0632,\n","         -0.2044,  0.1329,  0.1050, -0.0768, -0.1245, -0.1694, -0.0431, -0.0451,\n","          0.0695,  0.0237, -0.1629, -0.0752,  0.0972, -0.0018,  0.1735, -0.0035],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.0.nn.2.weight': tensor([[ 0.0996, -0.0790, -0.0031,  ..., -0.2141,  0.0057, -0.0771],\n","         [-0.0791, -0.0594,  0.0476,  ...,  0.0642,  0.0042, -0.0698],\n","         [-0.0537,  0.0294, -0.0232,  ..., -0.0314, -0.0253, -0.0212],\n","         ...,\n","         [-0.2267,  0.0516,  0.0501,  ...,  0.0783,  0.2606, -0.1956],\n","         [-0.0653,  0.0234, -0.0673,  ..., -0.1088,  0.0968, -0.3951],\n","         [ 0.0933,  0.0711,  0.0341,  ...,  0.0852, -0.1020, -0.0710]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.0.nn.2.bias': tensor([-0.1006, -0.1102, -0.0114,  0.0222,  0.1085,  0.0064, -0.0435,  0.3151,\n","         -0.0431, -0.1671, -0.3110, -0.0482, -0.1426, -0.2086, -0.1358,  0.0222,\n","         -0.2961, -0.1759, -0.1042, -0.0507, -0.0737, -0.0659, -0.1455, -0.1845,\n","         -0.0352,  0.0426, -0.1545, -0.1058, -0.1111,  0.2331, -0.1749, -0.1185,\n","          0.0426, -0.2961, -0.0531,  0.3241, -0.1850, -0.2314, -0.0337, -0.0967,\n","         -0.1802, -0.0729,  0.0673,  0.1746, -0.2451, -0.0243, -0.2453,  0.1811,\n","         -0.1649, -0.2948,  0.1137, -0.0230, -0.1019, -0.0473,  0.0239, -0.0969,\n","          0.1999,  0.0294, -0.0965, -0.1052, -0.2150, -0.0278, -0.2535, -0.0378,\n","         -0.0473, -0.0431, -0.0118,  0.1667, -0.0982,  0.0439, -0.0463, -0.1289,\n","         -0.2535,  0.0456,  0.0759, -0.0894, -0.0774, -0.1100,  0.0632, -0.1137,\n","         -0.2092, -0.0836,  0.0099,  0.2250,  0.2331, -0.0719, -0.1102, -0.1084,\n","         -0.1845, -0.1097, -0.1405, -0.0300,  0.0436,  0.0063, -0.0856,  0.2338,\n","         -0.2254,  0.0107,  0.0439,  0.0199,  0.0099, -0.0014,  0.0318, -0.1037,\n","         -0.1796,  0.2638, -0.0230, -0.0273, -0.1471, -0.1019,  0.0877,  0.0294,\n","         -0.1802, -0.1426, -0.0261, -0.0641, -0.0738, -0.2254,  0.0673, -0.1317,\n","         -0.0651, -0.1405, -0.2020, -0.0377, -0.0145, -0.1411,  0.0632,  0.2221,\n","         -0.1035,  0.0258, -0.0965, -0.0295,  0.1391,  0.1225, -0.0133,  0.0216,\n","         -0.2627, -0.1759, -0.0364, -0.0293,  0.0426, -0.1467,  0.0981, -0.1052,\n","          0.1839,  0.0462, -0.2948,  0.0648, -0.0062, -0.1766,  0.1639, -0.0337,\n","         -0.0774,  0.0027, -0.1671, -0.1143,  0.0555, -0.0604,  0.1667, -0.0671,\n","         -0.0603, -0.0286, -0.0751, -0.1717, -0.0850, -0.2086,  0.1746, -0.0293,\n","         -0.1358, -0.0101,  0.0877, -0.2627, -0.0140, -0.0296, -0.1084, -0.0027,\n","          0.0214,  0.0214, -0.1358, -0.1131, -0.0014,  0.1453,  0.0216, -0.0490,\n","          0.3049,  0.1168, -0.0985,  0.1887, -0.1759, -0.2150, -0.2852, -0.1288,\n","          0.1639,  0.0436, -0.3110,  0.0934, -0.0738, -0.0232, -0.0317,  0.0298,\n","         -0.0588, -0.0114, -0.2022, -0.1109,  0.1391,  0.0149, -0.2260,  0.0374,\n","          0.1892,  0.0608,  0.0370, -0.1649, -0.1022,  0.1428, -0.0376, -0.1137,\n","          0.0759, -0.1022, -0.1040, -0.0244,  0.1225, -0.0145, -0.1288, -0.0101,\n","         -0.2100, -0.1850,  0.0461,  0.1732, -0.0214,  0.0934, -0.2100, -0.0472,\n","          0.0960, -0.1411,  0.0318,  0.0374, -0.1586, -0.0214,  0.0461, -0.0364,\n","          0.0107,  0.0370,  0.0303, -0.1052,  0.2015, -0.0724, -0.0309, -0.0377,\n","         -0.0696, -0.0598, -0.0719,  0.1999, -0.0376, -0.2092, -0.1102,  0.0448,\n","         -0.0729, -0.1455,  0.1454, -0.0575,  0.2472,  0.1454, -0.0286, -0.1110,\n","         -0.0836,  0.0062,  0.2638, -0.0659, -0.0490, -0.0188,  0.0149, -0.0970,\n","          0.1137, -0.0810, -0.1052,  0.2802, -0.1586, -0.2022, -0.1358, -0.1987,\n","          0.1892, -0.2314,  0.0963, -0.0417, -0.0435,  0.0062, -0.1467, -0.0737,\n","         -0.1100,  0.0632, -0.0967,  0.1490, -0.0309,  0.0960,  0.1921, -0.0724,\n","         -0.1607, -0.0688,  0.0147, -0.1046, -0.2508,  0.2472, -0.0598,  0.0788,\n","         -0.0207, -0.0857, -0.0496,  0.1887, -0.0603, -0.1471, -0.0970, -0.0355,\n","         -0.1759, -0.1111, -0.0641, -0.0985,  0.0221, -0.0856,  0.0950, -0.0850,\n","         -0.0604,  0.0513, -0.0062, -0.0378, -0.0982,  0.0456, -0.0969, -0.0027,\n","          0.0074, -0.0575, -0.2260, -0.1400, -0.0696, -0.0394,  0.0409, -0.1213,\n","          0.0448,  0.1513, -0.0371, -0.1987, -0.0243,  0.0426, -0.0371, -0.0278,\n","         -0.2376,  0.1921,  0.0834,  0.0298, -0.1182,  0.0950, -0.2451, -0.0812,\n","         -0.0986,  0.0258,  0.2624, -0.0810, -0.0588, -0.1040,  0.0981,  0.3544,\n","          0.2624,  0.0199, -0.0671, -0.0300, -0.0888,  0.3664,  0.2015,  0.3664,\n","          0.0378, -0.1056,  0.1250, -0.1046,  0.0074, -0.1097, -0.0634, -0.2376,\n","          0.0513, -0.0295, -0.0273,  0.0409,  0.0887, -0.1717, -0.2453, -0.0355,\n","         -0.1056, -0.1182, -0.1102,  0.0378, -0.0857, -0.0751,  0.0887, -0.0394,\n","         -0.1586,  0.0788,  0.0612,  0.1428, -0.0140, -0.0482,  0.2221,  0.2476,\n","         -0.0531,  0.1732,  0.0612, -0.1042, -0.1185, -0.2852,  0.0648, -0.0688,\n","         -0.0371, -0.1213, -0.5525,  0.3544, -0.0118,  0.2802,  0.0054, -0.1539,\n","          0.1811,  0.0063,  0.2338,  0.1085,  0.0054, -0.0651, -0.0463, -0.0371,\n","          0.0221, -0.1058,  0.2837,  0.1453, -0.0232,  0.0064,  0.0929,  0.1508,\n","         -0.1131, -0.0342, -0.1766,  0.1168, -0.2020,  0.0239, -0.1586,  0.2250,\n","         -0.1317, -0.0864, -0.1037, -0.0317, -0.0296, -0.1035, -0.0888, -0.1796,\n","          0.0049, -0.1539,  0.0963, -0.0496,  0.1513, -0.1143, -0.0342, -0.0634,\n","          0.1250,  0.0130,  0.2837, -0.2508,  0.2476,  0.0049,  0.1346, -0.1109,\n","         -0.0127,  0.0834,  0.0608,  0.0027, -0.0261, -0.1289,  0.1490,  0.0725,\n","         -0.0295, -0.1400, -0.0472, -0.0133,  0.0725, -0.1749, -0.0199, -0.1110,\n","         -0.0417, -0.0244, -0.0864,  0.0345, -0.0507,  0.3241,  0.3151, -0.1006,\n","          0.0555,  0.0303, -0.1545,  0.0130,  0.0462,  0.1346, -0.0207,  0.0929,\n","         -0.0188, -0.0199, -0.0986,  0.3049, -0.0812, -0.0352, -0.0894,  0.0345,\n","         -0.1607, -0.5525,  0.1508,  0.1839,  0.0147,  0.0632, -0.0295, -0.0127],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.1.nn.0.weight': tensor([[ 0.0275,  0.0242, -0.0989,  ..., -0.1643, -0.0759, -0.1005],\n","         [-0.0453, -0.0110, -0.0842,  ...,  0.0032, -0.0740,  0.1294],\n","         [-0.0044,  0.0278,  0.1482,  ..., -0.0822, -0.0493,  0.0059],\n","         ...,\n","         [ 0.1444,  0.0148,  0.0179,  ..., -0.0408,  0.1420,  0.0358],\n","         [ 0.3260, -0.0449, -0.0877,  ...,  0.0177, -0.0196,  0.0077],\n","         [ 0.0955,  0.0022, -0.0410,  ..., -0.1385,  0.0426,  0.0501]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.1.nn.0.bias': tensor([ 4.5739e-01,  7.8298e-02,  3.1823e-01, -7.7204e-02, -2.0703e-02,\n","          5.6136e-01,  5.8020e-01,  1.7882e-01, -1.1738e-01,  4.2636e-02,\n","         -8.0942e-01, -2.1253e-04,  2.5875e-01, -2.2441e-01,  3.1985e-01,\n","          1.4108e-01,  7.7418e-01,  1.0325e-01, -1.9710e-01, -3.4779e-01,\n","         -3.6264e-02,  6.7994e-02,  4.3559e-01,  3.1578e-03,  5.4061e-01,\n","         -1.7891e-01, -2.3047e-01,  4.4975e-01,  1.9340e-01, -6.1514e-02,\n","          5.3660e-02,  4.5758e-01,  6.7771e-01,  3.5060e-02,  4.2833e-01,\n","          1.5897e-02, -1.3094e-01, -2.2404e+00,  1.1293e-01,  6.7771e-01,\n","          3.0944e-01, -2.1768e-03,  8.9652e-02,  1.1512e-01,  1.5061e-01,\n","         -9.1459e-02,  2.5829e-01,  3.9386e-01,  1.4508e-01,  2.6965e-01,\n","          2.6085e-01,  8.7284e-02,  6.3670e-01,  4.7609e-01,  1.3428e-01,\n","          6.6962e-01,  1.9550e-01,  1.5897e-02,  3.8436e-01,  4.6229e-01,\n","         -3.2078e-01,  1.6819e-01, -2.3079e-02, -2.1265e-01,  3.5457e-01,\n","         -9.4823e-02, -7.6429e-02,  7.0945e-02,  6.7433e-02,  5.1994e-01,\n","          1.4997e-01,  2.8649e-01,  4.8865e-02,  6.9369e-01, -2.3047e-01,\n","          1.1293e-01,  6.7433e-02, -1.0093e-01, -9.4085e-01,  3.8436e-01,\n","          7.6421e-02, -3.3863e-01,  5.6136e-01, -4.7719e-01,  5.0600e-03,\n","          7.2189e-01,  1.8009e-01,  1.9571e-01, -7.9533e-03,  2.6044e-01,\n","          4.6521e-01,  1.2562e-01,  1.2062e-01,  4.2713e-02,  3.1823e-01,\n","          9.1921e-01,  7.8841e-01,  3.6135e-01,  4.5813e-01, -1.2887e-01,\n","         -3.2199e-01,  7.7796e-02,  4.2853e-01, -1.0174e-01, -1.0646e-01,\n","          7.8841e-01,  2.1722e+00, -2.9850e-01,  3.7469e-01, -6.5674e-02,\n","         -1.4692e-01,  1.5690e-01,  4.0110e-02,  4.2833e-01,  6.0482e-01,\n","          2.3918e-01,  2.5084e-02,  3.7245e-01, -4.1041e-02,  1.2007e-01,\n","          1.4997e-01,  1.8311e-01,  5.2252e-01, -9.3114e-02,  4.6707e-01,\n","         -1.7053e-01,  3.0944e-01,  1.0427e-01,  6.3404e-02,  3.6625e-02,\n","          6.6962e-01, -2.1460e-01,  1.2562e-01,  4.5758e-01, -3.6358e-02,\n","          4.5187e-01, -6.3734e-02,  3.1007e-01,  4.6310e-01,  1.0555e-01,\n","         -1.5727e-01,  1.3446e-01,  5.3339e-02,  3.5875e-02,  9.1463e-01,\n","         -2.0703e-02, -4.5713e-01,  3.3660e-01, -1.4899e-01,  1.9571e-01,\n","          2.0266e-01,  9.0008e-02,  1.6749e-01, -2.0303e-03, -1.5420e-01,\n","          1.7476e+00,  4.1939e-01, -9.1459e-02, -6.2142e-02, -5.4394e-02,\n","         -1.8307e-01,  1.0011e-01,  1.7882e-01,  3.2986e-01,  2.8607e-01,\n","          3.2986e-01,  9.1921e-01,  6.2667e-02, -2.8992e-01,  3.1525e-01,\n","          8.3446e-01,  1.2062e-01,  4.9080e-01, -7.6429e-02, -2.2924e-01,\n","         -6.3078e-02, -2.8992e-01,  4.0423e-01, -3.5961e-02,  1.7476e+00,\n","          1.1711e-01,  3.3292e-01,  3.8316e-02,  7.5874e-01, -3.2078e-01,\n","          1.2853e-01,  6.4171e-02,  4.3829e-01, -2.7838e-02,  7.9195e-01,\n","          3.7370e-01,  8.9652e-02,  1.2273e-01,  7.7658e-01,  5.2379e-03,\n","          3.1985e-01, -3.4899e-01,  2.9402e-02,  3.8708e-01,  5.3226e-01,\n","         -2.1499e-01,  7.2778e-01, -2.1416e-01,  5.0612e-01,  3.6625e-02,\n","         -3.8457e-01,  1.8713e-01,  5.2617e-01,  2.6515e-01,  6.3404e-02,\n","         -2.4572e-01, -2.7021e-01,  6.1443e-01,  8.3446e-01,  6.0482e-01,\n","         -5.0052e-02, -2.0303e-03,  2.9517e-01,  7.1325e-02, -8.7797e-02,\n","         -1.3061e-01,  2.3658e-01,  6.4171e-02,  2.2058e-01,  2.1003e-01,\n","          2.8649e-01,  5.5954e-01, -3.4779e-01,  1.6717e-01, -8.5276e-02,\n","          2.7764e-01,  3.1525e-01, -4.0543e-02,  5.1684e-01,  5.1774e-02,\n","          9.2413e-01,  5.7016e-01,  2.2058e-01, -7.6559e-01,  3.9386e-01,\n","          4.5187e-01, -2.3444e-01,  3.3644e-01,  2.4514e-01,  3.3757e-01,\n","         -8.0942e-01, -2.1768e-03, -7.7204e-02, -2.6703e-01,  1.0970e-01,\n","          6.9369e-01,  2.1965e-01,  1.9608e-01,  4.8296e-01, -1.7239e-02,\n","         -3.9916e-01, -1.4899e-01, -5.0477e-01,  4.3803e-01,  7.0935e-02,\n","          1.9875e-01, -2.1563e-01,  5.8020e-01,  4.6511e-01,  5.5734e-01,\n","          1.6248e-01,  3.8695e-01,  2.1965e-01,  6.9879e-01,  3.1578e-03,\n","         -4.5981e-01, -9.3114e-02,  5.0600e-03, -7.7202e-02,  5.7016e-01,\n","          3.6733e-01, -4.8769e-02, -1.7053e-01,  2.5020e-01,  2.9517e-01,\n","         -3.0233e-01,  1.8713e-01,  1.6024e-01,  5.0612e-01, -2.2335e-01,\n","          1.3668e-02,  5.2252e-01, -7.1529e-02,  2.2609e-01, -6.1966e-01,\n","          2.3495e-01, -2.6703e-01,  1.6749e-01, -8.0306e-01,  6.6165e-02,\n","          4.9028e-01, -1.3061e-01, -2.2108e-02,  1.8706e-01, -2.6622e-02,\n","          3.3292e-01,  4.5739e-01,  7.2778e-01,  6.0083e-01,  4.0398e-01,\n","          1.2007e-01, -5.1985e-02,  6.2478e-01, -6.3078e-02,  2.7554e-01,\n","          1.4508e-01, -2.5586e-01,  5.6701e-01, -4.8769e-02, -4.5388e-02,\n","         -7.6559e-01, -2.4051e-01,  1.5580e-01,  2.9420e-01, -4.0543e-02,\n","          4.6707e-01,  3.7453e-01,  2.8816e-01,  4.9028e-01,  1.3428e-01,\n","          9.0067e-02, -3.9582e-02,  4.4316e-01,  8.8236e-01, -1.1738e-01,\n","         -5.0477e-01, -3.3249e-01,  2.1722e+00,  4.7609e-01, -2.7838e-02,\n","          2.6898e-01,  4.0423e-01, -4.5388e-02,  2.5084e-02, -2.1416e-01,\n","         -1.0405e-01,  2.7554e-01,  1.6248e-01, -6.5674e-02, -1.8748e-01,\n","         -6.1966e-01,  3.7453e-01, -4.7719e-01, -1.0607e-02,  1.8009e-01,\n","          2.0266e-01,  6.1036e-01,  3.5825e-01, -1.9288e-01, -5.1985e-02,\n","          3.8316e-02,  5.1074e-01,  3.3660e-01, -2.2360e-01, -2.1265e-01,\n","         -3.6290e-01,  9.7146e-01,  4.6521e-01,  7.7418e-01,  9.1463e-01,\n","          1.9608e-01,  3.5760e-01, -3.0223e-01, -8.4524e-01,  1.2273e-01,\n","         -3.3863e-01,  3.5971e-01, -1.1282e+00,  6.3670e-01, -3.3413e-01,\n","         -2.6784e-02,  4.0087e-02,  5.5734e-01, -2.2924e-01,  1.6024e-01,\n","          3.5760e-01, -2.6026e-02, -2.3444e-01, -2.2658e-01, -1.0646e-01,\n","          2.6898e-01, -1.3506e-01, -6.0356e-02,  1.9250e-01, -2.7021e-01,\n","         -3.3249e-01,  1.0970e-01,  2.6605e-01, -9.4823e-02,  4.4517e-01,\n","          6.7994e-02,  2.5734e-01, -2.1563e-01,  6.9855e-01,  3.6733e-01,\n","          5.4061e-01, -2.3076e-03,  6.5528e-01,  2.6515e-01, -1.8509e-01,\n","          2.3918e-01,  2.2609e-01,  2.3685e-01,  4.8296e-01,  8.8236e-01,\n","          9.5704e-02,  1.0067e-01, -2.7375e-01,  8.3584e-02,  1.4645e-01,\n","          5.5794e-01,  3.4570e-01,  1.3446e-01, -7.7202e-02,  2.6388e-01,\n","          1.9875e-01, -2.6784e-02,  5.6701e-01, -1.8307e-01,  6.0860e-01,\n","          3.5717e-01,  4.6569e-01,  6.5528e-01,  3.6135e-01, -8.0306e-01,\n","          4.6229e-01,  2.6085e-01,  1.0325e-01,  4.8865e-02,  1.0637e-01,\n","         -2.5516e-01,  2.8607e-01,  3.5971e-01, -2.2360e-01, -3.5578e-01,\n","          2.8274e-01,  2.6965e-01,  2.8274e-01,  4.3559e-01,  4.2835e-01,\n","          2.0438e-01,  5.2617e-01,  3.5717e-01, -2.2658e-01, -2.5586e-01,\n","          4.0110e-02,  4.2835e-01, -2.3079e-02, -2.5516e-01,  4.4316e-01,\n","          6.9855e-01, -1.3094e-01, -5.5604e-01,  6.3897e-01,  1.7681e-02,\n","          6.2667e-02,  4.3094e-01,  6.9879e-01,  2.6388e-01,  1.5580e-01,\n","         -3.6290e-01,  3.7469e-01, -6.7905e-02,  4.6511e-01, -4.5981e-01,\n","          3.3059e-02,  1.9250e-01,  4.0398e-01,  2.0208e-01,  6.4708e-01,\n","          2.6044e-01,  4.6195e-01,  7.8917e-02,  1.0637e-01,  5.0930e-01,\n","         -3.6358e-02,  3.0942e-01,  9.0008e-02, -2.1460e-01, -1.2887e-01,\n","          6.4708e-01,  3.8708e-01,  5.5954e-01, -1.7891e-01, -6.0356e-02,\n","          4.2636e-02,  4.2132e-02, -2.1320e-01, -2.1499e-01,  8.3584e-02,\n","         -2.4975e-02, -2.3694e-01,  5.1074e-01,  2.7861e-01,  7.0935e-02,\n","          3.5875e-02,  3.4570e-01,  6.0860e-01,  7.7658e-01,  9.7146e-01,\n","          3.5825e-01,  5.1684e-01,  6.3897e-01,  5.0930e-01, -5.0052e-02,\n","          5.1774e-02,  2.7764e-01,  3.5060e-02,  2.7861e-01, -8.5276e-02,\n","          2.3495e-01,  3.7245e-01, -2.9850e-01,  5.2379e-03,  5.3006e-01,\n","          7.7796e-02,  1.5061e-01,  5.3006e-01, -1.0174e-01, -2.4572e-01,\n","          4.5947e-01, -2.2441e-01, -2.1320e-01,  4.9080e-01,  7.1325e-02,\n","         -2.3076e-03, -1.4692e-01, -3.5578e-01,  1.8311e-01,  1.4108e-01,\n","          2.5020e-01, -4.1041e-02,  5.5794e-01, -2.2404e+00, -7.9533e-03,\n","         -5.4394e-02, -2.3694e-01,  1.4645e-01,  1.2853e-01, -1.0405e-01,\n","          4.6195e-01, -1.0607e-02,  2.9420e-01,  1.9550e-01,  7.8298e-02,\n","          5.2890e-01,  1.0011e-01,  4.4975e-01, -1.0093e-01,  4.3094e-01,\n","          4.5813e-01, -1.0875e-01, -5.5604e-01,  4.5251e-01,  4.6569e-01,\n","          8.7284e-02,  4.2713e-02, -2.6026e-02,  6.0083e-01,  5.3226e-01,\n","         -1.0875e-01,  2.9402e-02, -3.3413e-01,  2.0438e-01, -1.8748e-01,\n","         -1.3506e-01, -8.4524e-01,  3.5457e-01,  1.6717e-01,  4.3829e-01,\n","          4.2853e-01,  4.3803e-01,  3.8695e-01,  1.1711e-01,  1.5690e-01,\n","          3.3059e-02,  6.1443e-01,  1.0067e-01,  7.8917e-02,  2.1003e-01,\n","         -2.1714e-01, -2.1253e-04, -1.9710e-01,  7.9195e-01, -1.5420e-01,\n","          2.6605e-01,  6.6544e-01,  9.0067e-02, -3.5961e-02, -2.7375e-01,\n","          5.8445e-01,  9.5704e-02,  1.0427e-01,  2.5734e-01, -6.1514e-02,\n","         -6.7905e-02,  1.8706e-01,  5.3083e-01, -2.1714e-01, -6.2142e-02,\n","          3.3757e-01,  2.5829e-01,  6.6165e-02, -7.1529e-02,  4.6310e-01,\n","          3.3644e-01, -8.4622e-02,  3.1007e-01,  2.3685e-01,  6.2478e-01,\n","          2.5875e-01,  7.5874e-01,  4.2132e-02,  3.0942e-01,  5.3083e-01,\n","          2.8816e-01, -3.0233e-01, -2.2335e-01,  1.9340e-01, -1.1282e+00,\n","         -2.2108e-02, -2.4975e-02, -3.9916e-01,  1.0555e-01,  6.1036e-01,\n","          4.4517e-01, -3.0223e-01, -3.6264e-02,  4.5947e-01,  9.2413e-01,\n","         -6.3734e-02, -3.8457e-01,  5.8445e-01,  5.2890e-01,  2.0208e-01,\n","          1.7681e-02,  1.1512e-01,  7.0945e-02,  5.3339e-02, -1.7239e-02,\n","         -3.4899e-01, -3.2199e-01, -8.4622e-02, -2.4051e-01,  4.1939e-01,\n","         -2.6622e-02, -9.4085e-01,  2.3658e-01, -3.9582e-02,  3.1046e-01,\n","          6.6544e-01, -8.7797e-02,  2.4514e-01,  1.3668e-02,  1.6819e-01,\n","          3.1046e-01,  4.5251e-01,  7.6421e-02, -1.8509e-01,  5.1994e-01,\n","         -4.5713e-01,  4.0087e-02,  7.2189e-01, -1.9288e-01,  3.7370e-01,\n","         -1.5727e-01,  5.3660e-02], device='cuda:0'),\n"," '_gnn._conv_layers.1.nn.2.weight': tensor([[-0.1153, -0.0131,  0.0228,  ..., -0.0350, -0.0076,  0.0293],\n","         [ 0.0407, -0.1190,  0.0993,  ..., -0.0131, -0.0693, -0.1095],\n","         [-0.1124,  0.0478,  0.0903,  ...,  0.1377, -0.1953,  0.0017],\n","         ...,\n","         [ 0.0366, -0.0444, -0.0305,  ..., -0.1629, -0.1840, -0.1008],\n","         [ 0.0145,  0.0339, -0.1518,  ..., -0.1083, -0.0081, -0.0719],\n","         [ 0.0698, -0.2337,  0.1265,  ...,  0.0520, -0.0055, -0.0029]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.1.nn.2.bias': tensor([-0.0410,  0.1678,  0.1278,  0.0318, -0.0150, -0.1727,  0.2068,  0.0328,\n","          0.3789, -0.6101,  0.2152,  0.5428,  0.1478,  0.0522,  0.0217,  0.5428,\n","          0.1183,  0.5793,  0.2774,  0.1857,  0.4322,  0.2048,  0.0133, -0.1139,\n","          0.0256,  0.0859,  0.3835, -0.0480, -0.0320, -0.0463,  0.4767,  0.1983,\n","         -0.0126,  0.1085,  0.2077,  0.0433,  0.5035, -0.0092, -0.1184,  0.5908,\n","          0.0493, -0.0077,  0.7220,  0.2588,  0.5548, -0.1904,  0.4218,  0.4438,\n","          0.0699,  0.5908,  0.0702,  0.2774, -0.0137,  0.4483,  0.4896,  0.0051,\n","          0.0220, -0.0251, -0.0147, -0.0777, -0.1448,  0.1546,  0.1314, -0.1559,\n","          0.3116,  0.1389,  0.1420,  0.5263,  0.4130,  0.2690,  0.4767,  0.4991,\n","          0.1025,  0.0062, -0.0062,  0.1484,  0.1783,  0.0965,  0.7510,  0.4896,\n","          0.0319,  0.3507,  0.0256,  0.5982, -0.0176,  0.1794,  0.4991,  0.3100,\n","          0.3848,  0.4655, -0.0944,  0.4397,  0.2908, -0.0134,  0.0522,  0.4322,\n","         -0.0388,  0.1651,  0.2622, -0.0177, -0.0150, -0.0961, -0.0159, -0.0409,\n","          0.0697,  0.0604,  0.3368,  0.4999, -0.0066, -0.0258, -0.0823,  0.0307,\n","          0.0926,  0.1298,  0.4397,  0.1879,  0.0217, -0.2659,  0.5483,  0.2820,\n","          0.0130,  0.2749, -0.0566,  0.0909,  0.1018,  0.6705,  0.0318,  0.2104,\n","         -0.2146,  0.0009,  0.3789,  0.1909,  0.7220,  0.0890,  0.1364,  0.2621,\n","         -0.2659,  0.0214, -0.0101,  0.3138,  0.3334,  0.3099, -0.0566,  0.0319,\n","         -0.0417,  0.0261, -0.0497,  0.2820, -0.0165,  0.0214,  0.2050, -0.2862,\n","          0.1894, -0.0555,  0.1546,  0.4655,  0.4061, -0.0367,  0.2550, -0.0407,\n","          0.1908,  0.1406,  0.7510,  0.2534,  0.1651,  0.3116,  0.3138,  0.0856,\n","          0.1014,  0.3368,  0.0328, -0.1184,  0.1420,  0.5259,  0.4130, -0.0992,\n","          0.2513, -0.1728,  0.5548,  0.4999,  0.0175, -0.1727,  0.1258,  0.0051,\n","          0.3704, -0.0605,  0.0261,  0.2808, -0.0101, -0.0040,  0.0223,  0.0493,\n","          0.3585,  0.1314,  0.1894,  0.5859,  0.2548,  0.2048,  0.2127, -0.0410,\n","          0.1420, -0.0068, -0.0555, -0.0068,  0.2550,  0.5437,  0.2375,  0.1698,\n","          0.1908,  0.2908, -0.0731, -0.2613,  0.0259,  0.0129, -0.0160,  0.0129,\n","          0.1183,  0.1070,  0.1071,  0.2815, -0.0625,  0.3974, -0.0055, -0.0062,\n","          0.2808,  0.3492,  0.0062,  0.2104,  0.0490,  0.4117,  0.1071, -0.0040,\n","         -0.0388,  0.2629,  0.1905,  0.0389, -0.0315,  0.0929, -0.0639,  0.2690,\n","          0.2534, -0.0347,  0.3835,  0.1905, -0.0310,  0.3585,  0.0335,  0.5035,\n","         -0.0077, -0.1111,  0.2068,  0.0398,  0.6118,  0.0490, -0.2862,  0.2152,\n","          0.0189,  0.1278,  0.0832, -0.0480, -0.2613,  0.1484,  0.0909, -0.0071,\n","         -0.1025, -0.0480,  0.4113, -0.0494,  0.1908, -0.0777,  0.3850, -0.0159,\n","          0.5483,  0.0613,  0.0220, -0.0315,  0.1205,  0.1138,  0.2050,  0.2077,\n","          0.3920,  0.5437,  0.0335,  0.0389,  0.2407,  0.1389,  0.0352, -0.0480,\n","          0.4295,  0.4218,  0.5259,  0.2580,  0.1258,  0.3150,  0.1393,  0.4980,\n","          0.4350, -0.1025, -0.0055,  0.0259,  0.4113, -0.0639,  0.0579, -0.0761,\n","         -0.0258,  0.1297,  0.0832, -0.0119,  0.0604,  0.3099,  0.3215,  0.1071,\n","          0.1297,  0.0667,  0.2028,  0.0013,  0.4350, -0.0944, -0.0320,  0.1046,\n","         -0.1728, -0.0409,  0.6374,  0.3848,  0.1314, -0.0160,  0.0189,  0.4291,\n","         -0.0059,  0.0335,  0.0702, -0.0605,  0.1678, -0.1559, -0.0463, -0.0126,\n","         -0.0119,  0.3048,  0.1698, -0.0625,  0.1298,  0.2127,  0.1205,  0.5753,\n","          0.3507,  0.7441,  0.1460,  0.2881,  0.2513,  0.1364,  0.1018, -0.0177,\n","          0.2437, -0.1111,  0.1478,  0.2375,  0.1857, -0.0092,  0.2956,  0.2028,\n","          0.1478,  0.0130,  0.1393,  0.3705,  0.0890,  0.0697,  0.1014,  0.0216,\n","          0.0398,  0.4438,  0.2622,  0.1794, -0.0407,  0.0667, -0.0177,  0.2407,\n","         -0.3185,  0.2580,  0.1314,  0.2548, -0.0494, -0.1216, -0.0165,  0.1879,\n","          0.0048,  0.6374,  0.0598,  0.2437,  0.3882,  0.6595, -0.0071,  0.1909,\n","          0.1071,  0.3100,  0.0307,  0.4291, -0.0251,  0.0598,  0.1085,  0.6705,\n","          0.0164, -0.0176,  0.0699,  0.2306,  0.3974,  0.2881, -0.0134,  0.3705,\n","         -0.0761,  0.0165, -0.2061,  0.5753, -0.0059,  0.0613,  0.5263,  0.0859,\n","          0.0164,  0.3538, -0.0992,  0.0926, -0.0347, -0.0310, -0.0961,  0.2481,\n","         -0.0731,  0.6118, -0.1216, -0.2146, -0.6101,  0.7441,  0.3538,  0.4441,\n","         -0.0823, -0.3185,  0.3850, -0.0497,  0.2481, -0.2061, -0.1904,  0.0013,\n","          0.1460,  0.0657,  0.2815,  0.0048,  0.1983, -0.0809,  0.3334, -0.0315,\n","          0.3704, -0.0147, -0.0367,  0.1406,  0.0216,  0.1070,  0.2306,  0.0175,\n","          0.1025, -0.0177,  0.2629, -0.0066,  0.3215, -0.1139,  0.1061,  0.0546,\n","          0.5982,  0.2749,  0.0277,  0.1783,  0.4295,  0.5793,  0.5859,  0.1908,\n","          0.4117,  0.0546,  0.0223,  0.0009,  0.3048, -0.0809,  0.0433,  0.1138,\n","          0.0215,  0.0133,  0.3492,  0.0965,  0.3920,  0.3150, -0.0315,  0.4441,\n","          0.1061,  0.0165,  0.2956,  0.0929,  0.3882,  0.1420,  0.0856, -0.0137,\n","          0.0277,  0.1478,  0.4061,  0.0657,  0.0215,  0.2588,  0.0335,  0.0352,\n","          0.0579,  0.6595, -0.0417,  0.1046, -0.1448,  0.2621,  0.4483,  0.4980],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.2.nn.0.weight': tensor([[-0.0688,  0.0312,  0.2070,  ..., -0.0284, -0.0172,  0.0580],\n","         [-0.0120,  0.0034, -0.1807,  ...,  0.0185,  0.0075, -0.0371],\n","         [ 0.0225, -0.0545,  0.2449,  ..., -0.0395, -0.0050,  0.0914],\n","         ...,\n","         [-0.1162,  0.0025,  0.0366,  ..., -0.0214, -0.0451,  0.0235],\n","         [ 0.0281,  0.0743,  0.1701,  ..., -0.0217, -0.0041, -0.0135],\n","         [-0.0097,  0.0051,  0.0536,  ...,  0.0188,  0.0006, -0.0509]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.2.nn.0.bias': tensor([-1.8776e-01, -4.6549e-02, -7.6052e-02, -3.1684e-02,  2.7652e-02,\n","          1.2059e-01, -7.4282e-02, -6.9213e-02, -3.3939e-02,  8.5300e-04,\n","         -3.1825e-02,  1.4923e-02, -7.4548e-02, -1.4306e-02,  2.2822e-03,\n","         -9.7151e-02,  4.8397e-02,  2.7652e-02,  4.2167e-03, -5.9004e-02,\n","         -6.9794e-02,  2.1115e-02, -3.5417e-02, -2.8088e-02, -3.5539e-02,\n","         -8.8973e-03,  1.2873e-01,  4.2167e-03, -5.9378e-02, -6.8519e-02,\n","         -4.7248e-02,  2.6482e-02, -2.0303e-02,  4.4357e-02,  8.1007e-03,\n","         -2.6945e-02,  8.2428e-02, -8.4065e-03, -3.2901e-02, -6.0919e-02,\n","         -5.6470e-03, -1.0355e-01,  4.0882e-03, -6.8519e-02,  1.4828e-02,\n","         -5.9867e-03,  7.3228e-03,  4.0951e-03, -3.1407e-02, -6.7692e-02,\n","         -5.9761e-02, -4.6787e-02, -7.8284e-03, -9.2042e-02,  1.4713e-01,\n","          6.5667e-02, -4.5775e-02, -1.4720e-01, -1.1963e-01,  6.5667e-02,\n","         -1.0074e-01, -1.3205e-02, -1.0943e-01,  5.0136e-02, -6.4177e-02,\n","         -3.7056e-02, -6.2470e-02,  3.7213e-02, -5.5907e-02,  1.3509e-02,\n","         -2.8088e-02,  1.2681e-01,  1.7954e-01, -3.1057e-02, -3.4971e-02,\n","          6.4484e-02,  3.5930e-02, -6.8362e-02, -1.7236e-02,  1.2189e-01,\n","         -4.7864e-02, -1.1336e-01,  1.2819e-02, -1.1578e-01,  1.1749e-01,\n","         -5.0111e-02, -3.6860e-04, -5.8784e-02, -1.0558e-02, -3.6071e-03,\n","         -8.1590e-02, -3.2273e-02, -4.0356e-02, -9.8244e-02, -9.2008e-03,\n","          2.6482e-02, -7.0576e-02, -5.8784e-02, -6.5481e-02,  4.8844e-03,\n","         -9.9152e-02,  6.5661e-03, -2.9988e-02, -7.5022e-02, -2.8249e-02,\n","         -5.7176e-02, -5.3847e-02,  1.1714e-02, -2.2125e-02, -8.4065e-03,\n","          5.9616e-03,  1.3273e-02, -3.4789e-03, -3.4331e-02, -1.1658e-02,\n","         -1.4520e-04, -1.3278e-02,  9.4204e-02, -2.6281e-02, -4.7248e-02,\n","          6.7865e-02,  4.0884e-03,  1.6668e-01,  4.9058e-02, -4.3062e-02,\n","          1.7954e-01, -1.1774e-02, -5.0705e-02, -9.2053e-02, -2.9494e-02,\n","         -9.2672e-03,  4.2129e-02, -2.9882e-02, -5.6547e-02, -1.4877e-01,\n","         -4.8049e-02, -8.5958e-02, -5.2811e-02,  8.5477e-03, -4.6110e-02,\n","         -6.3802e-02, -6.8983e-02,  6.2354e-02, -1.4720e-01, -1.5876e-01,\n","         -6.2047e-02, -1.3205e-02, -4.4937e-02,  2.4903e-02,  2.1115e-02,\n","         -4.4422e-03, -2.9342e-02, -1.1067e-02, -1.2399e-02, -9.2042e-02,\n","          4.6662e-03, -2.1315e-02, -7.1941e-02,  4.0221e-02, -2.9882e-02,\n","         -1.0996e-02, -8.5119e-02, -4.5805e-03,  9.4072e-03, -5.5561e-02,\n","          8.1007e-03,  1.7334e-02, -5.5561e-02, -6.4759e-02,  1.5747e-02,\n","         -3.2901e-02, -1.9470e-02, -4.6110e-02, -7.8628e-02, -3.6356e-02,\n","         -3.3081e-03,  6.4011e-02,  4.1053e-02,  1.4828e-02, -2.9810e-02,\n","         -5.6329e-02, -1.6176e-03, -2.6177e-02, -6.4177e-02, -7.5022e-02,\n","         -1.9602e-02, -7.1941e-02,  7.9449e-02, -5.0382e-02, -6.3109e-02,\n","          1.2189e-01,  1.5506e-02, -8.4768e-02, -3.7056e-02, -3.8143e-02,\n","         -9.9610e-03, -2.9988e-02,  3.7213e-02, -1.3427e-01, -4.5805e-03,\n","          2.9180e-02, -7.2171e-02, -1.3721e-02, -7.5048e-02,  2.2607e-01,\n","          3.3248e-02, -5.7307e-02, -4.5562e-02,  2.1479e-02, -2.8101e-02,\n","          8.5300e-04, -1.2126e-01,  1.3273e-02, -1.7238e-02,  2.0533e-02,\n","         -1.0335e-01,  5.5785e-03, -9.8244e-02,  7.9449e-02,  7.5923e-02,\n","          4.8844e-03, -7.2783e-02, -4.4740e-02, -3.3898e-02,  7.5923e-02,\n","         -3.1454e-02, -2.2125e-02, -4.6382e-02, -6.2470e-02, -5.6701e-02,\n","          2.1479e-02, -4.4461e-02, -4.9319e-02, -4.4123e-02, -7.5048e-02,\n","         -6.8115e-02,  1.0341e-01,  4.0221e-02,  1.1247e-02,  9.4204e-02,\n","         -1.1424e-02, -4.9319e-02,  6.2609e-02, -6.9500e-03, -6.8115e-02,\n","         -6.6529e-02,  1.1717e-01, -3.1454e-02, -1.0074e-01, -1.3994e-02,\n","         -3.3081e-03, -1.5634e-03, -3.5417e-02, -1.4306e-02, -1.0355e-01,\n","         -1.4652e-01, -6.9794e-02, -4.6004e-02,  3.8929e-03, -4.4817e-02,\n","          4.5411e-03, -2.5138e-01,  7.3228e-03, -8.5119e-02,  4.6832e-02,\n","          2.4803e-02, -3.5900e-02, -6.2047e-02, -7.6117e-02, -1.2237e-01,\n","         -5.9378e-02, -9.2053e-02, -3.0879e-02,  2.9180e-02, -5.6198e-02,\n","         -9.2672e-03, -3.8400e-02, -3.0879e-02,  4.2408e-03, -3.3898e-02,\n","         -1.7295e-02, -3.6356e-02, -4.4422e-03,  4.4357e-02,  4.2408e-03,\n","         -2.6177e-02,  1.8786e-02,  4.2218e-02, -2.8101e-02, -8.8973e-03,\n","          4.5166e-02, -1.2237e-01, -2.6945e-02, -5.3751e-02, -3.5900e-02,\n","          1.5895e-02, -4.0356e-02,  1.8513e-02, -1.0991e-01,  4.4703e-02,\n","          1.7566e-01,  2.5877e-02,  3.3417e-02,  4.4584e-03, -1.9185e-02,\n","         -8.7610e-02, -2.0217e-01,  4.0050e-02, -5.0642e-02, -3.6223e-02,\n","          8.4331e-02, -3.6223e-02, -1.5614e-01, -5.5907e-02, -5.5252e-03,\n","          2.1810e-02, -9.8015e-02, -3.2273e-02,  6.7865e-02,  1.3509e-02,\n","          1.6820e-02, -4.3842e-02, -6.0827e-02, -5.8439e-02,  3.3248e-02,\n","         -6.8983e-02, -6.0919e-02,  2.7765e-01,  1.0341e-01, -1.9470e-02,\n","          1.6618e-02,  6.5661e-03,  3.9579e-02,  4.2268e-02, -9.2161e-02,\n","          2.2822e-03,  1.1247e-02,  6.3357e-03, -5.6198e-02, -4.7864e-02,\n","         -7.4282e-02,  4.4703e-02, -2.2562e-03, -6.8362e-02,  4.9878e-02,\n","          4.5411e-03, -7.2171e-02, -1.3994e-02, -3.1684e-02, -1.4520e-04,\n","         -1.0943e-01, -9.3483e-03, -3.8143e-02, -1.7236e-02,  4.3396e-02,\n","         -4.4389e-02,  1.3699e-02, -5.6470e-03, -1.2652e-02, -4.0538e-02,\n","         -4.4461e-02,  5.7380e-03,  4.1967e-02, -1.0166e-02, -9.7159e-03,\n","          6.2354e-02,  1.1717e-01, -5.8439e-02, -9.6151e-04, -7.8802e-02,\n","         -1.0991e-01,  1.0944e-02, -6.4759e-02, -4.3062e-02, -1.7024e-03,\n","         -9.2161e-02, -7.2926e-02, -4.9993e-03,  6.4484e-02,  6.4004e-02,\n","         -2.9810e-02, -9.2231e-02, -8.8239e-03,  4.6832e-02,  2.1447e-02,\n","         -3.4971e-02, -8.5958e-02, -1.7238e-02, -7.0392e-02, -7.3269e-03,\n","          1.2059e-01, -1.7110e-01,  1.3699e-02,  1.5895e-02, -3.5539e-02,\n","         -1.5698e-02, -5.6044e-02, -7.3269e-03, -8.2891e-02,  8.2428e-02,\n","          6.2524e-02,  4.4584e-03, -5.0382e-02,  4.0310e-03, -1.4877e-01,\n","          2.4903e-02,  2.0533e-02,  1.2523e-02,  4.6172e-02, -2.1621e-03,\n","          3.3417e-02,  5.2841e-03,  2.6552e-02, -4.4937e-02,  1.2856e-01,\n","          3.8929e-03,  2.2406e-03, -7.6052e-02, -8.7610e-02, -2.5413e-02,\n","         -9.9610e-03,  2.2607e-01, -4.4418e-02, -4.6382e-02,  1.6618e-02,\n","         -1.5630e-02, -3.6860e-04, -2.6775e-02, -6.0827e-02, -1.1658e-02,\n","         -2.6775e-02, -9.6151e-04, -7.6117e-02, -4.5562e-02,  3.3454e-02,\n","         -6.9500e-03, -6.3747e-02, -1.7295e-02,  4.2218e-02, -5.0111e-02,\n","         -8.4768e-02,  1.8786e-02,  2.4879e-02,  1.4923e-02, -1.0620e-01,\n","         -1.3427e-01,  4.0310e-03, -9.0924e-02, -6.5485e-02, -6.9213e-02,\n","         -1.4652e-01, -1.5614e-01, -2.1591e-02,  1.8513e-02, -4.6787e-02,\n","         -2.1315e-02, -4.6549e-02, -5.0642e-02, -9.2008e-03,  8.4331e-02,\n","         -2.5413e-02, -3.1171e-02,  2.7765e-01, -6.3747e-02,  2.4803e-02,\n","         -5.7176e-02,  4.1053e-02,  4.2268e-02, -1.9405e-02,  1.0944e-02,\n","          2.1447e-02, -4.6004e-02,  2.4879e-02,  4.0446e-02, -5.7307e-02,\n","          1.2681e-01, -6.4738e-02, -7.0576e-02, -1.0700e-01, -3.8400e-02,\n","         -1.1774e-02, -9.8015e-02,  2.2406e-03, -7.2742e-04, -8.8239e-03,\n","          5.5785e-03, -9.7159e-03, -1.1578e-01,  4.8421e-02,  4.0446e-02,\n","         -1.8776e-01, -9.2231e-02, -5.9867e-03, -1.5698e-02,  1.2873e-01,\n","         -2.8927e-02,  6.2524e-02,  3.5930e-02, -9.8482e-02,  4.9878e-02,\n","         -8.1590e-02, -7.2742e-04, -5.6329e-02, -3.4789e-03, -6.4738e-02,\n","          2.2837e-02,  5.7380e-03,  5.2841e-03,  1.7334e-02,  2.6548e-02,\n","          2.3312e-02,  2.2697e-04, -1.9405e-02,  4.2129e-02,  4.9058e-02,\n","          2.6548e-02,  3.3454e-02, -2.4554e-02, -1.4045e-03, -1.2652e-02,\n","          4.8397e-02,  2.1810e-02, -3.4331e-02, -1.1022e-01,  1.7566e-01,\n","         -5.9004e-02, -9.3483e-03, -2.9342e-02,  1.1749e-01,  1.8904e-02,\n","          1.6820e-02, -2.4540e-02, -3.1057e-02,  4.1967e-02, -3.3939e-02,\n","         -9.9152e-02, -4.4817e-02, -1.0166e-02,  1.2523e-02, -5.6701e-02,\n","         -7.4548e-02, -6.3109e-02,  1.6668e-01, -1.2126e-01, -3.1171e-02,\n","         -2.5138e-01, -2.9494e-02, -5.2811e-02,  4.3396e-02, -1.0335e-01,\n","         -6.6529e-02,  1.5747e-02, -2.8927e-02, -1.3721e-02, -5.3751e-02,\n","         -7.0392e-02, -6.0383e-02, -2.2887e-02, -1.0064e-01, -3.5326e-02,\n","          1.1690e-01, -4.8049e-02,  4.0882e-03, -1.2699e-01, -1.0064e-01,\n","         -1.3278e-02, -1.7024e-03, -7.8802e-02,  1.2819e-02, -1.5630e-02,\n","         -2.2849e-02,  2.6552e-02,  1.6888e-01, -1.1424e-02, -5.0705e-02,\n","         -2.1621e-03, -3.1825e-02, -1.1273e-01,  8.5477e-03, -1.1963e-01,\n","         -2.0217e-01,  3.9579e-02,  1.1714e-02, -1.2399e-02, -2.2887e-02,\n","          2.2697e-04, -4.9993e-03, -3.2389e-02, -2.2849e-02, -2.4540e-02,\n","         -5.6547e-02,  2.5877e-02,  1.6888e-01, -5.3847e-02, -6.0383e-02,\n","         -5.5252e-03, -1.1336e-01, -6.3802e-02, -6.5485e-02,  6.2609e-02,\n","         -1.2699e-01, -1.1273e-01,  9.8430e-03, -6.7692e-02, -4.4123e-02,\n","          4.6662e-03,  9.4072e-03, -4.3842e-02,  5.0136e-02, -9.0924e-02,\n","         -3.5326e-02,  2.2837e-02, -1.9602e-02, -4.4740e-02, -1.9185e-02,\n","         -4.4418e-02, -1.5876e-01,  8.1360e-03,  1.4697e-03, -8.2891e-02,\n","         -3.1407e-02, -1.1067e-02,  1.4713e-01,  4.0050e-02, -2.8249e-02,\n","         -7.2783e-02,  6.4011e-02, -1.6176e-03,  8.1360e-03, -7.8284e-03,\n","          4.5166e-02, -1.5778e-02, -2.6281e-02,  5.9616e-03, -3.2389e-02,\n","          6.3357e-03, -6.5481e-02,  4.6172e-02,  1.4697e-03, -1.5634e-03,\n","         -9.7151e-02, -1.4045e-03, -2.1591e-02, -4.4389e-02,  6.4004e-02,\n","         -1.5778e-02, -1.7110e-01, -5.6044e-02,  1.2856e-01, -1.0558e-02,\n","          2.3312e-02, -2.4554e-02, -4.5775e-02,  4.8421e-02, -1.0996e-02,\n","         -1.0700e-01, -2.2562e-03, -7.2926e-02, -5.9761e-02, -2.0303e-02,\n","          1.1690e-01,  4.0951e-03,  4.0884e-03, -1.1022e-01,  1.5506e-02,\n","         -7.8628e-02, -3.6071e-03, -9.8482e-02, -4.0538e-02, -1.0620e-01,\n","          9.8430e-03,  1.8904e-02], device='cuda:0'),\n"," '_gnn._conv_layers.2.nn.2.weight': tensor([[ 0.0153,  0.0502,  0.0250,  ..., -0.0031, -0.0627, -0.0303],\n","         [-0.0233, -0.0663,  0.0520,  ..., -0.0482, -0.1078,  0.1040],\n","         [ 0.0503,  0.0019, -0.0167,  ..., -0.0853, -0.0393,  0.0544],\n","         ...,\n","         [ 0.0251, -0.0961,  0.0390,  ...,  0.0075, -0.1877, -0.0776],\n","         [-0.1076,  0.1519, -0.0266,  ..., -0.1467, -0.0861, -0.0111],\n","         [-0.0547,  0.0539,  0.0945,  ..., -0.0317, -0.0792,  0.1539]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.2.nn.2.bias': tensor([-0.0140,  0.1906,  0.1129,  0.0134,  0.1330,  0.0499, -0.2106, -0.0235,\n","         -0.0569, -0.0275,  0.0367,  0.0435,  0.0183,  0.2458, -0.0011, -0.0783,\n","         -0.0821, -0.1403, -0.0180, -0.1024, -0.0418,  0.3437, -0.1569,  0.0250,\n","         -0.1309, -0.0140,  0.1117, -0.0650, -0.0803, -0.0558,  0.0671,  0.0563,\n","          0.0244, -0.0547,  0.0308, -0.0162, -0.0162,  0.1129,  0.0848,  0.1636,\n","         -0.1005, -0.2024, -0.0320,  0.1977, -0.0844, -0.0342, -0.0009,  0.1037,\n","          0.1090, -0.1403, -0.1202, -0.0664, -0.0558,  0.2805, -0.0895,  0.3716,\n","          0.2968, -0.1756, -0.0310,  0.1977, -0.1756, -0.0342,  0.0270,  0.0008,\n","         -0.0110, -0.1202,  0.0561, -0.2280,  0.0224, -0.0863, -0.0503,  0.0287,\n","         -0.0762, -0.2432,  0.1228, -0.0241,  0.0249,  0.0375, -0.1771,  0.0192,\n","          0.0165,  0.0454,  0.1330, -0.0911,  0.0012, -0.0326, -0.0473, -0.0612,\n","         -0.0734,  0.2847, -0.0433, -0.0645, -0.1091, -0.0180, -0.3331,  0.2303,\n","         -0.0125, -0.0895,  0.0876,  0.0371,  0.0559, -0.0428, -0.0020,  0.0422,\n","         -0.0911, -0.0488,  0.0181, -0.0762, -0.0428,  0.0200, -0.2280, -0.0706,\n","          0.1146, -0.2033,  0.0247, -0.1075,  0.5715,  0.1090,  0.0351, -0.0460,\n","          0.0660,  0.0162,  0.0192,  0.0597, -0.0611, -0.0302,  0.0890, -0.3331,\n","         -0.0326, -0.0196, -0.0080, -0.0559, -0.0062, -0.0273, -0.0090, -0.0412,\n","         -0.0795, -0.0484,  0.1636,  0.0876, -0.0548, -0.0523, -0.0899,  0.0135,\n","         -0.1205, -0.0645,  0.1611, -0.0523, -0.1609, -0.0506, -0.0555, -0.0484,\n","          0.1146,  0.0135, -0.1075, -0.2033,  0.0827, -0.0559,  0.0330, -0.0858,\n","          0.0848, -0.1079, -0.0549,  0.0573, -0.0276,  0.0604, -0.0323, -0.0235,\n","         -0.0406, -0.1173, -0.0575, -0.0387, -0.2432, -0.0797, -0.0787, -0.0066,\n","          0.0141, -0.0844, -0.0069, -0.1014,  0.0012,  0.1906,  0.0559,  0.1854,\n","          0.2077,  0.0390,  0.2443,  0.1519,  0.0407,  0.2703, -0.0797, -0.0213,\n","          0.2703,  0.6254, -0.0062, -0.0421,  0.1611,  0.2105, -0.0590,  0.0642,\n","         -0.0541, -0.0590, -0.0825, -0.0253,  0.2128, -0.0856, -0.0825, -0.0243,\n","         -0.0734, -0.0143, -0.0406,  0.1182,  0.0249, -0.0276,  0.0422,  0.0494,\n","          0.2591,  0.0694, -0.0541,  0.0183,  0.1854, -0.1624,  0.2847,  0.0244,\n","         -0.0069, -0.1079,  0.0561,  0.0641,  0.0697, -0.1571, -0.0473,  0.1228,\n","          0.0947, -0.0460,  0.0183,  0.0435,  0.0311,  0.0247,  0.0351, -0.1508,\n","         -0.0821,  0.0563, -0.0243,  0.0195,  0.0491, -0.0594, -0.0521, -0.0706,\n","         -0.1508, -0.0863,  0.0372,  0.0167,  0.1519,  0.0940, -0.1017,  0.0390,\n","          0.0308,  0.0494, -0.0913,  0.0270,  0.1706, -0.1280,  0.2458,  0.0134,\n","          0.0604,  0.0909,  0.0951,  0.1131, -0.0241,  0.0200,  0.0367, -0.0066,\n","         -0.1491,  0.0162,  0.0733,  0.0912,  0.0597,  0.0890, -0.0213,  0.0287,\n","         -0.1372,  0.0543, -0.0125, -0.0503,  0.5715,  0.0947,  0.0357, -0.0856,\n","         -0.0549, -0.1635, -0.2024, -0.0686, -0.0424,  0.0909, -0.1091,  0.6254,\n","         -0.0302, -0.0418,  0.2128, -0.0858,  0.0940,  0.0357, -0.0912,  0.1380,\n","          0.0454,  0.0416, -0.0412,  0.0912,  0.0418,  0.0422,  0.2443, -0.0273,\n","         -0.0196, -0.0310, -0.1832, -0.1372, -0.0899, -0.0360, -0.0501,  0.0694,\n","         -0.0871, -0.0110, -0.0374, -0.0185, -0.1303, -0.0611,  0.1294,  0.1658,\n","          0.3197, -0.0453, -0.1832, -0.0374, -0.0323,  0.0224,  0.0507,  0.1182,\n","          0.0604,  0.1706,  0.0223, -0.1226, -0.0760, -0.0069, -0.1381, -0.3389,\n","         -0.1635,  0.0573, -0.0760, -0.0912,  0.0008, -0.0895, -0.1624,  0.0046,\n","         -0.0009,  0.1131, -0.0575, -0.0020, -0.1571, -0.0984,  0.1658,  0.0129,\n","         -0.0555, -0.0797,  0.0733,  0.0642, -0.0275, -0.1014, -0.1037, -0.0080,\n","         -0.1205,  0.0361,  0.1294,  0.0416,  0.1380, -0.1226,  0.0641,  0.0951,\n","         -0.0803, -0.0488,  0.0330, -0.0011,  0.3716,  0.0375, -0.3389, -0.0090,\n","         -0.0032, -0.0326, -0.0895, -0.0547, -0.0415,  0.0311, -0.0642,  0.0827,\n","          0.0660,  0.0499, -0.0859, -0.0686, -0.0453,  0.0141, -0.0521, -0.1280,\n","         -0.0569,  0.3437, -0.1303, -0.0069, -0.0387,  0.2886,  0.2591, -0.0143,\n","          0.0671,  0.0917,  0.1037, -0.1037,  0.0048, -0.0787, -0.0762,  0.3137,\n","         -0.0871, -0.0217, -0.0032, -0.0049, -0.1491, -0.0326,  0.0167, -0.1800,\n","         -0.0762,  0.2805, -0.0783,  0.0361,  0.0504,  0.0543,  0.1260, -0.1309,\n","         -0.1609,  0.0507, -0.0244, -0.0664,  0.0195, -0.0346, -0.0217, -0.1800,\n","          0.0418, -0.0797, -0.0049,  0.0552, -0.0913,  0.1117, -0.0655, -0.1268,\n","         -0.0790, -0.0185,  0.0165,  0.3197, -0.0346,  0.2968,  0.2303,  0.0129,\n","          0.0046,  0.3137,  0.0136, -0.0253, -0.0642, -0.0594,  0.0407, -0.0548,\n","          0.0136,  0.2077,  0.2105, -0.1268, -0.0859,  0.0250, -0.1569,  0.0504,\n","          0.0697,  0.0048,  0.0422, -0.0506,  0.0353,  0.0223, -0.0421, -0.1173,\n","          0.0604,  0.0372,  0.0181, -0.0433, -0.0360, -0.0790, -0.0795, -0.1024,\n","          0.1260, -0.0655, -0.0421, -0.0424, -0.0244, -0.0501,  0.0552,  0.2886,\n","         -0.0650,  0.0917, -0.1017,  0.0491, -0.1381, -0.0984, -0.0415, -0.0320,\n","         -0.0421, -0.0612,  0.0353,  0.0371,  0.0183, -0.1771, -0.1005, -0.2106],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.3.nn.0.weight': tensor([[-0.0543, -0.0521,  0.0852,  ...,  0.1814, -0.0414, -0.0222],\n","         [-0.0130, -0.0255,  0.0637,  ..., -0.0134, -0.0522,  0.0572],\n","         [ 0.0188, -0.0402,  0.0057,  ..., -0.0231,  0.0154,  0.0460],\n","         ...,\n","         [-0.0289, -0.0180,  0.0949,  ...,  0.0231, -0.0321,  0.0119],\n","         [-0.0043,  0.0164, -0.0322,  ...,  0.0037, -0.0194,  0.0054],\n","         [ 0.0605, -0.0080,  0.0245,  ..., -0.0306,  0.0354,  0.0192]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.3.nn.0.bias': tensor([-7.2510e-02, -2.8259e-02,  1.6837e-02,  2.5372e-02,  6.5716e-02,\n","         -4.6265e-02,  1.8855e-02,  1.7956e-03,  2.9770e-03, -2.0411e-02,\n","         -4.2908e-02, -6.7144e-02, -1.8234e-02,  3.8476e-02, -4.4990e-02,\n","         -9.9155e-02, -1.7622e-03, -6.3187e-02, -8.7104e-02,  1.9346e-02,\n","          2.9421e-02, -3.6729e-02, -4.7008e-02, -2.0545e-02, -6.4189e-02,\n","         -4.7313e-02, -5.3264e-02, -6.3993e-02,  5.2248e-03,  1.5221e-02,\n","         -6.0727e-02,  1.2372e-02, -2.6459e-02,  4.4529e-03, -6.0727e-02,\n","         -3.5364e-02, -8.0095e-03, -3.6729e-02, -3.2190e-02, -8.3299e-02,\n","         -3.8672e-02,  1.8770e-02, -6.7596e-02,  1.0319e-01, -6.9001e-02,\n","         -3.2754e-02,  4.3315e-03, -5.0383e-02, -5.5583e-02, -3.3248e-02,\n","         -6.8861e-02, -1.1352e-01, -1.0197e-01, -6.6830e-02,  1.2806e-03,\n","         -6.0402e-03, -2.3605e-02, -5.0089e-03, -3.4051e-02, -2.8827e-02,\n","          3.6465e-02, -4.1160e-04, -7.0097e-02, -9.4078e-02, -2.0545e-02,\n","         -4.7008e-02, -1.2064e-02, -1.5131e-02,  6.2256e-02, -2.5238e-03,\n","         -1.0634e-01, -7.1320e-03, -6.4584e-02, -1.0634e-01, -3.3507e-02,\n","         -5.2696e-02, -1.5758e-02,  1.9999e-02,  1.9720e-03,  2.2659e-03,\n","         -2.0710e-02, -6.0908e-03,  2.7768e-02, -1.2019e-01, -3.0405e-02,\n","         -4.9755e-02, -5.7490e-02, -1.2970e-02, -2.5238e-03, -6.0648e-02,\n","          6.7100e-03, -7.9311e-03,  1.0306e-01, -3.0253e-02, -3.4061e-02,\n","         -7.1320e-03, -5.2696e-02, -2.7132e-02, -6.8188e-03, -3.1426e-02,\n","         -1.9711e-02, -1.9739e-02, -4.9755e-02, -6.1678e-02, -2.8827e-02,\n","         -6.2716e-02, -1.2049e-02, -1.8918e-02, -6.7596e-02, -8.6365e-02,\n","         -6.2716e-02, -4.4797e-02, -4.8115e-02, -8.0540e-03, -7.5678e-02,\n","         -1.8475e-02, -4.5871e-02, -9.7053e-03, -7.0097e-02, -2.3153e-02,\n","          3.0566e-03,  9.2487e-03, -1.5758e-02, -3.7258e-03, -1.1352e-01,\n","          1.9999e-02, -9.9155e-02, -1.6278e-02,  1.1627e-02, -4.0582e-02,\n","         -1.0357e-01,  1.9767e-02,  2.6499e-02, -2.2591e-02,  4.7745e-03,\n","         -1.5361e-02,  4.9948e-03,  3.5711e-03, -1.5533e-02,  7.9148e-02,\n","         -5.4015e-02, -6.1678e-02, -3.5278e-02, -1.6122e-02, -1.0392e-01,\n","          1.4696e-02,  1.0989e-02,  4.9804e-02, -3.7738e-02, -2.6049e-02,\n","          1.5520e-02, -1.6463e-02,  1.9720e-03, -8.7982e-02, -5.5348e-02,\n","         -2.6049e-02, -7.7289e-02, -5.4015e-02,  4.6133e-03,  2.3887e-02,\n","         -3.9496e-02, -3.3007e-02, -5.8475e-02, -1.8918e-02, -8.6365e-02,\n","         -8.5873e-02, -1.1876e-01, -2.1743e-02, -4.7089e-02, -3.3248e-02,\n","         -3.7566e-02, -4.9667e-02,  1.7171e-03, -1.6836e-02,  1.9125e-02,\n","         -3.9623e-02, -7.7289e-02, -6.9778e-02, -1.2402e-01,  3.7750e-02,\n","         -6.4189e-02, -7.7760e-02, -1.2019e-01, -2.5936e-02,  4.4659e-02,\n","          1.5101e-02, -1.1710e-01, -2.7150e-02,  1.0195e-02,  1.4247e-02,\n","         -1.2020e-02,  2.4838e-02, -1.0342e-01, -4.8443e-02,  1.0786e-02,\n","         -1.9815e-02, -5.3293e-02,  3.6678e-05, -4.8387e-02,  3.5711e-03,\n","          9.7769e-06, -2.8259e-02, -9.3440e-02, -5.3122e-02, -6.0627e-02,\n","         -3.5278e-02, -8.8007e-03, -6.6830e-02, -7.6250e-02, -5.0630e-02,\n","         -4.8618e-02, -1.3723e-01, -1.8475e-02, -3.1251e-02, -7.7391e-02,\n","         -4.1160e-04, -4.3686e-02, -2.4021e-02, -7.4168e-02,  1.2806e-03,\n","         -8.0540e-03, -1.0223e-03,  5.9394e-02,  1.5369e-02,  3.0664e-02,\n","         -3.9210e-02,  2.8109e-02, -4.2908e-02, -3.5364e-02,  2.2228e-03,\n","          1.1117e-01, -6.2823e-02,  3.0863e-02, -1.2020e-02,  1.9358e-02,\n","         -7.4956e-03, -2.9275e-02, -8.7963e-02, -4.6848e-02,  7.2707e-02,\n","         -1.2970e-02, -1.2064e-02, -1.7926e-02, -5.5348e-02, -3.1426e-02,\n","          4.7229e-03, -4.8086e-02, -6.4355e-02, -4.1737e-02, -3.1251e-02,\n","         -3.7566e-02,  3.2301e-02, -1.1876e-01, -1.1710e-01, -7.4428e-03,\n","         -8.0099e-02,  1.4654e-02,  1.2461e-02, -1.0768e-01,  7.9222e-03,\n","          9.8070e-03, -2.0710e-02, -2.5936e-02, -9.3440e-02, -3.4467e-02,\n","         -1.9324e-02, -1.2402e-01, -2.2486e-02, -2.8392e-02,  6.5384e-03,\n","          1.7975e-02, -9.8317e-02, -3.3007e-02,  1.9767e-02,  2.9124e-02,\n","          2.8420e-03,  1.0195e-02, -1.0357e-01, -4.7715e-03, -1.5533e-02,\n","          1.5101e-02, -1.4418e-02, -9.6522e-02, -4.6265e-02, -4.8195e-02,\n","         -2.9489e-03,  5.2248e-03,  3.1220e-02, -7.9251e-02,  3.4905e-02,\n","         -1.2049e-02, -7.1745e-03, -1.0084e-02, -7.6250e-02, -3.2452e-02,\n","          4.5967e-02, -8.7104e-02, -1.7797e-02,  1.4247e-02, -4.3686e-02,\n","         -2.3799e-02, -1.7471e-02, -1.0854e-02, -6.8861e-02, -2.9939e-02,\n","         -1.0392e-01, -7.7518e-02, -9.8317e-02,  3.0664e-02, -4.4327e-02,\n","          9.2487e-03,  8.3484e-02,  4.5967e-02, -2.9937e-02, -1.0242e-03,\n","         -1.7926e-02, -5.0082e-02,  1.5931e-02, -3.2673e-03, -1.0084e-02,\n","         -4.8052e-02, -2.3153e-02,  6.9807e-03, -7.7391e-02, -3.4467e-02,\n","         -3.4061e-02,  2.9384e-02, -1.8861e-02, -7.4168e-02, -1.0059e-02,\n","          1.2780e-02, -8.1288e-02, -4.8052e-02, -8.5873e-02, -1.0342e-01,\n","          8.9978e-03, -9.7350e-02, -1.9519e-02,  6.5384e-03, -8.7982e-02,\n","         -2.6459e-02, -1.1480e-01,  2.5766e-02,  2.6177e-01,  6.9807e-03,\n","         -3.2754e-02,  4.2564e-02, -2.7132e-02, -1.8861e-02,  2.9384e-02,\n","          2.5372e-02,  2.8420e-03, -6.4895e-02,  1.4196e-04, -6.0648e-02,\n","         -1.7471e-02, -2.9489e-03,  2.6499e-02,  2.9421e-02,  2.2659e-03,\n","          1.8852e-02, -1.6463e-02, -4.4797e-02, -8.3299e-02,  1.4654e-02,\n","         -6.6092e-02, -4.0582e-02,  8.3484e-02, -1.0804e-03, -5.9046e-02,\n","          2.4905e-02, -4.6437e-02,  8.9539e-02, -3.7718e-02, -7.7518e-02,\n","         -1.3723e-01,  2.0548e-02, -2.3148e-02, -1.1282e-02, -2.5295e-02,\n","         -3.8767e-02,  1.0736e-01, -1.1480e-01,  2.6177e-01, -1.8315e-02,\n","         -2.4532e-02, -7.1046e-02, -1.6122e-02,  1.9346e-02,  1.0787e-01,\n","          3.2551e-02,  2.5766e-02,  1.7956e-03,  4.7229e-03, -1.6673e-02,\n","          3.8267e-02,  1.2579e-02, -5.0890e-03, -3.9496e-02, -7.9311e-03,\n","          2.0548e-02, -6.6092e-02,  1.0306e-01, -3.7718e-02, -1.0059e-02,\n","          5.3626e-03,  9.8070e-03, -4.6742e-02, -2.8392e-02, -5.3862e-03,\n","         -9.2294e-02, -1.0650e-02, -2.8656e-02,  7.9148e-02, -1.6278e-02,\n","          1.3778e-03, -5.0630e-02, -5.0082e-02,  1.2372e-02,  2.3887e-02,\n","          9.7769e-06, -3.2190e-02, -1.9519e-02, -2.4532e-02,  1.7171e-03,\n","          2.9228e-02,  2.2228e-03, -1.5361e-02, -1.0768e-01, -7.5678e-02,\n","         -2.9937e-02,  1.2780e-02,  3.8476e-02,  1.3778e-03, -4.4804e-02,\n","         -1.0197e-01, -5.7106e-03,  1.5520e-02, -8.0095e-03, -4.8086e-02,\n","          2.8109e-02, -6.9085e-02, -1.7797e-02,  6.7100e-03, -2.3799e-02,\n","         -7.7406e-02,  2.9124e-02, -1.0804e-03,  1.0319e-01, -6.2823e-02,\n","          6.2256e-02, -4.8618e-02, -3.6944e-02, -5.9213e-02,  7.2707e-02,\n","         -1.9711e-02, -4.8387e-02,  1.9125e-02,  1.4196e-04, -3.0405e-02,\n","         -4.4168e-02,  1.1528e-03, -4.7089e-02, -2.8656e-02, -8.0099e-02,\n","         -5.9213e-02, -4.4628e-02, -4.0872e-02,  3.6465e-02,  3.6678e-05,\n","          5.9394e-02,  1.5931e-02, -3.7738e-02, -7.1745e-03, -2.2486e-02,\n","         -4.8443e-02,  5.5019e-02,  1.8855e-02, -7.2510e-02,  1.8852e-02,\n","         -4.5637e-02,  1.1627e-02,  3.2347e-02, -4.0872e-02,  6.8463e-02,\n","         -4.4990e-02, -2.3605e-02, -2.9634e-02, -6.0908e-03,  1.4696e-02,\n","          5.0095e-02, -6.4584e-02, -6.0627e-02, -3.2408e-02, -1.0223e-03,\n","         -4.6437e-02,  3.2301e-02, -3.7258e-03,  4.3315e-03,  2.4838e-02,\n","         -9.7350e-02,  1.9358e-02, -6.9486e-03,  3.1220e-02,  1.8770e-02,\n","         -4.4628e-02, -7.7760e-02, -9.4447e-02, -2.3148e-02, -2.1743e-02,\n","          1.0787e-01, -1.6673e-02,  1.6837e-02, -8.2761e-02, -3.4051e-02,\n","         -4.8115e-02, -1.4418e-02, -3.8672e-02, -5.8475e-02,  1.0989e-02,\n","          8.9539e-02, -5.7490e-02, -1.7622e-03,  2.0706e-02, -2.8925e-02,\n","         -5.0383e-02, -7.9251e-02,  3.7750e-02, -1.7791e-02,  6.4407e-02,\n","         -6.7144e-02, -7.7406e-02, -6.9778e-02,  1.0405e-02,  3.4905e-02,\n","         -4.7313e-02,  3.8267e-02, -3.2673e-03, -8.7963e-02, -2.8925e-02,\n","         -5.9046e-02, -2.0443e-02, -5.5583e-02, -3.2408e-02, -4.5637e-02,\n","         -2.0411e-02, -6.5970e-02, -8.1288e-02, -3.3507e-02, -1.9815e-02,\n","          1.2579e-02,  5.2655e-03, -1.9324e-02,  3.2588e-02,  5.2655e-03,\n","         -3.9623e-02,  4.6133e-03, -5.3122e-02, -9.7053e-03, -6.3993e-02,\n","         -9.4447e-02, -1.8315e-02, -2.9275e-02, -1.2805e-02, -2.7150e-02,\n","          2.0706e-02,  8.3941e-02, -2.1112e-02,  2.9770e-03,  1.0405e-02,\n","          2.4905e-02,  4.7745e-03, -1.1282e-02, -2.2591e-02, -5.3293e-02,\n","         -5.3862e-03, -4.5871e-02, -5.0089e-03, -3.5658e-02, -4.4168e-02,\n","          6.4407e-02, -7.4428e-03,  2.1472e-02,  2.9228e-02,  4.5177e-03,\n","          1.0736e-01, -7.4956e-03,  1.7975e-02, -6.5458e-02,  5.0095e-02,\n","         -6.5970e-02,  1.2461e-02, -1.9804e-02, -6.5458e-02, -4.8195e-02,\n","          1.0786e-02,  1.1528e-03, -2.4021e-02, -1.0650e-02,  4.5177e-03,\n","         -5.0890e-03, -3.8767e-02,  7.9222e-03,  4.9948e-03,  3.0566e-03,\n","         -8.2761e-02, -1.0854e-02, -6.8188e-03, -6.9486e-03, -2.0443e-02,\n","          1.1117e-01, -2.8680e-02, -1.9804e-02, -1.9739e-02, -4.1215e-02,\n","          5.5019e-02, -8.8007e-03, -1.0242e-03, -1.2805e-02, -2.5295e-02,\n","         -6.4895e-02, -3.9210e-02, -3.6944e-02, -4.7715e-03, -1.7791e-02,\n","         -6.9001e-02,  5.3626e-03, -7.1046e-02, -2.9939e-02,  1.5221e-02,\n","          3.2551e-02, -4.4781e-04,  4.9804e-02,  6.8463e-02, -4.6848e-02,\n","         -6.3187e-02,  8.9978e-03, -2.9634e-02, -4.4327e-02, -3.0253e-02,\n","         -2.8680e-02,  8.3941e-02, -6.4355e-02, -4.6742e-02,  4.4529e-03,\n","         -4.4781e-04,  4.4659e-02, -3.2452e-02, -4.4804e-02, -9.6522e-02,\n","         -4.1737e-02,  2.1472e-02, -1.8234e-02, -4.9667e-02, -9.4078e-02,\n","         -6.9085e-02,  6.5716e-02, -9.2294e-02, -1.6836e-02,  3.0863e-02,\n","          3.2588e-02, -2.1112e-02,  4.2564e-02, -3.5658e-02, -5.7106e-03,\n","          3.2347e-02, -6.0402e-03, -4.1215e-02, -1.5131e-02, -5.3264e-02,\n","          2.7768e-02,  1.5369e-02], device='cuda:0'),\n"," '_gnn._conv_layers.3.nn.2.weight': tensor([[ 0.0084, -0.0199,  0.0320,  ...,  0.0513, -0.0006,  0.0830],\n","         [-0.0034, -0.0170,  0.0333,  ..., -0.0330, -0.0338,  0.0290],\n","         [ 0.0849,  0.0412,  0.0089,  ...,  0.0108,  0.0341, -0.0437],\n","         ...,\n","         [-0.2369,  0.0658,  0.0111,  ..., -0.0130,  0.0961,  0.0269],\n","         [-0.0028,  0.0232, -0.0040,  ...,  0.0843, -0.0495,  0.0602],\n","         [-0.0618, -0.0458, -0.0255,  ..., -0.0831, -0.0044, -0.0295]],\n","        device='cuda:0'),\n"," '_gnn._conv_layers.3.nn.2.bias': tensor([-1.1494e-01, -3.5021e-02, -1.8626e-02, -2.4429e-02,  6.7417e-02,\n","         -2.5161e-02,  2.8098e-01,  6.3081e-02,  2.0244e-02,  7.1598e-02,\n","         -1.4450e-02, -2.4657e-02, -4.4653e-02, -3.6642e-03, -2.7243e-01,\n","         -1.8425e-02, -7.1184e-02, -2.7520e-02, -1.9724e-02, -3.5681e-03,\n","         -1.4450e-02,  3.3116e-02,  4.2301e-02, -1.9334e-02,  7.8245e-03,\n","         -2.0372e-02, -9.3269e-03,  4.8738e-02, -7.3446e-02, -2.3529e-01,\n","          2.3188e-02, -4.4279e-02,  5.9146e-02, -1.1494e-01,  1.1153e-01,\n","          3.0960e-03,  4.9895e-02,  7.1485e-02,  1.5688e-02, -2.7203e-03,\n","         -2.2948e-02,  1.7670e-03,  4.6086e-02,  2.7014e-02, -4.3049e-02,\n","         -1.2342e-02,  1.1530e-02, -1.4270e-02,  3.7175e-02, -4.8380e-02,\n","         -6.1950e-02, -2.7427e-02, -9.3915e-02,  3.7830e-02,  9.7363e-02,\n","         -2.8142e-02, -6.4176e-02, -4.4653e-02, -5.1215e-02,  1.6090e-02,\n","         -6.6704e-02,  6.9632e-03, -3.6642e-03, -1.8425e-02,  3.7993e-02,\n","          1.3793e-02, -3.0007e-02,  2.3203e-02,  1.1216e-02,  4.2640e-03,\n","         -2.4429e-02, -1.6679e-01,  2.3203e-02,  1.8929e-01,  8.9133e-02,\n","         -7.2525e-01,  3.7775e-02, -2.0634e-02,  2.4608e-01, -4.8074e-02,\n","         -2.1556e-02, -2.6422e-02, -1.8626e-02, -4.4371e-02,  3.5301e-02,\n","         -1.8898e-02, -5.9519e-02, -2.1458e-03, -1.3345e-01, -7.7773e-02,\n","         -4.7727e-02,  3.0912e-02, -3.9845e-03,  1.5248e-01, -8.1481e-02,\n","          1.8093e-01,  7.6387e-03,  6.1858e-02, -1.0778e-01, -3.2056e-03,\n","          5.3658e-02, -2.4789e-04,  6.1858e-02,  6.4385e-02, -3.2311e-02,\n","          3.7082e-03, -4.8380e-02,  9.7363e-02, -8.1406e-02,  7.2444e-02,\n","          1.8782e-03,  1.1028e-02,  1.9422e-01,  3.9499e-02,  3.6287e-02,\n","          4.6086e-02, -6.7885e-02, -4.0232e-02,  1.0480e-01,  2.8769e-02,\n","         -1.2342e-02, -5.1215e-02, -3.6771e-02,  9.6910e-02,  2.9238e-03,\n","         -1.9572e-02, -9.3269e-03,  4.3255e-02,  7.1598e-02,  7.3854e-03,\n","         -2.3191e-01, -2.6458e-02, -7.9760e-02,  3.7175e-02,  7.3854e-03,\n","         -6.0855e-03, -3.6771e-02, -1.3955e-01,  5.3031e-02, -6.4786e-04,\n","         -1.1809e-02,  1.4593e-02, -7.2777e-02, -3.2370e-02, -2.6422e-02,\n","          1.1134e-02, -5.2230e-02, -4.4083e-02,  9.6910e-02, -2.5198e-02,\n","          1.3374e-02, -2.7243e-01, -8.2516e-02, -1.2684e-02,  1.2837e-01,\n","         -7.6165e-02,  1.8205e-01, -2.0099e-02,  3.9499e-02,  1.5627e-01,\n","          1.1364e-02, -8.1406e-02,  3.5301e-02, -1.3345e-01, -2.1556e-02,\n","         -3.7752e-02, -1.5085e-01, -7.6165e-02,  1.8708e-01,  6.4172e-02,\n","          6.3081e-02, -7.2736e-02,  1.8732e-02, -6.8766e-02,  7.1065e-02,\n","         -1.6463e-01,  1.8782e-03, -4.9310e-02,  4.8738e-02, -3.0007e-02,\n","          7.9306e-02, -1.5158e-02,  8.1280e-03,  1.7086e-02, -2.8865e-02,\n","         -2.5556e-02, -1.8183e-02,  1.6090e-02, -6.5151e-02, -1.1811e-01,\n","         -4.9276e-02, -2.9549e-02, -1.6734e-03, -3.2370e-02, -4.4083e-02,\n","          9.4917e-02, -1.0109e-01, -2.2194e-02,  1.7086e-02, -6.9199e-03,\n","          2.9034e-02,  1.8776e-01, -2.5161e-02, -1.3955e-01, -9.6794e-04,\n","          2.7605e-01, -2.0372e-02, -1.4270e-02,  7.2444e-02, -3.2081e-02,\n","         -5.8654e-02, -1.3676e-02,  7.1065e-02,  2.8098e-01,  8.1280e-03,\n","          1.5165e-01,  9.0399e-02, -8.3821e-02,  2.2536e-02, -2.2194e-02,\n","         -3.6469e-01, -4.9276e-02,  1.3374e-02,  1.3973e-01, -1.1809e-02,\n","         -4.2528e-03, -1.9055e-02,  2.5067e-02,  8.2063e-02, -6.8766e-02,\n","         -9.4620e-02, -6.4176e-02, -1.8473e-02,  2.3842e-01, -2.7427e-02,\n","          1.3046e-01,  1.8782e-03,  5.8387e-02, -3.7752e-02, -1.5439e-01,\n","         -1.1062e-01, -7.2777e-02, -5.2458e-02,  1.4449e-02, -2.2950e-02,\n","          2.5862e-02, -5.0251e-02,  5.8387e-02, -2.3529e-01, -3.9845e-03,\n","          1.1106e-01, -5.0251e-02,  4.3273e-03, -6.7154e-02,  1.8732e-02,\n","         -1.0778e-01,  1.6636e-02, -3.1821e-02, -1.8183e-02, -1.9572e-02,\n","         -2.9549e-02,  1.3699e-03, -3.5503e-02, -2.5299e-01,  3.1796e-02,\n","         -3.6084e-02,  1.2235e-01,  9.1452e-03,  2.5862e-02,  2.2536e-02,\n","          4.9895e-02,  2.3778e-02,  2.8769e-02,  6.4385e-02, -1.5085e-01,\n","          9.0363e-02, -1.0109e-01, -8.1481e-02,  4.3273e-03,  1.2837e-01,\n","          5.1055e-02, -2.5299e-01, -4.8074e-02,  1.0952e-02,  3.1796e-02,\n","         -1.6463e-01,  1.4069e-01, -1.6679e-01,  2.0985e-02,  1.6636e-02,\n","          9.4917e-02, -1.3232e-01,  1.0297e-02,  9.0363e-02, -7.8975e-02,\n","         -4.0947e-02,  1.1153e-01, -2.1458e-03, -4.4371e-02,  9.0399e-02,\n","          3.7993e-02, -6.1950e-02, -6.6704e-02,  9.9907e-03, -9.2725e-02,\n","          6.4734e-02,  9.3536e-03, -7.2525e-01,  1.9422e-01, -3.1821e-02,\n","          4.3903e-02, -1.9055e-02,  4.2301e-02, -6.7885e-02, -9.3915e-02,\n","          2.0985e-02, -9.4620e-02, -3.9468e-02, -7.7773e-02, -1.1062e-01,\n","         -2.2948e-02,  4.2640e-03,  2.0244e-02,  4.3903e-02,  2.5067e-02,\n","         -6.0855e-03,  2.4608e-01,  1.3775e-02, -2.6531e-02, -1.8910e-01,\n","         -4.9310e-02,  7.9306e-02,  1.3775e-02, -1.3676e-02, -6.7154e-02,\n","         -3.7533e-02,  4.3533e-02,  1.8782e-03,  6.4734e-02, -3.2081e-02,\n","         -9.4417e-02,  2.7014e-02, -2.6458e-02, -2.3009e-01,  9.3536e-03,\n","         -4.4279e-02, -9.1171e-04, -8.1247e-02,  7.1485e-02,  5.0662e-02,\n","          1.0297e-02,  1.1106e-01,  1.7670e-03, -3.6227e-02, -5.2230e-02,\n","         -3.6084e-02,  6.4172e-02, -3.1733e-02,  9.6256e-02, -7.3446e-02,\n","          2.3842e-01,  2.9238e-03, -4.2528e-03,  7.8245e-03,  1.5688e-02,\n","         -1.9334e-02,  1.4593e-02, -6.9199e-03, -2.8865e-02,  2.8311e-02,\n","          1.3046e-01, -2.3009e-01, -3.2311e-02, -2.1431e-01,  5.9146e-02,\n","          8.2063e-02,  3.3116e-02, -2.5556e-02,  6.9632e-03, -1.0798e-01,\n","          5.7442e-02,  9.7379e-03, -1.1811e-01,  2.9700e-02, -2.3191e-01,\n","          1.1364e-02,  1.0952e-02, -1.4935e-01,  1.8205e-01, -1.8910e-01,\n","          1.8776e-01, -3.5503e-02, -8.2516e-02, -2.7520e-02, -4.0947e-02,\n","         -2.5198e-02, -1.6471e-02,  7.6207e-03,  7.6387e-03,  1.8093e-01,\n","         -7.2736e-02, -4.9242e-04,  5.1055e-02, -7.8975e-02, -3.6227e-02,\n","          9.7379e-03, -1.9724e-02, -1.8273e-04,  3.7775e-02, -7.7497e-02,\n","         -1.8273e-04,  2.7605e-01,  2.3778e-02, -1.5096e-02, -8.1247e-02,\n","          1.3973e-01, -3.6469e-01,  1.3699e-03,  9.1452e-03, -9.2725e-02,\n","         -1.5096e-02,  1.1832e-02,  1.4129e-02,  1.1530e-02, -1.8898e-02,\n","          1.1134e-02, -1.0168e-01, -1.8473e-02, -3.2056e-03,  9.9907e-03,\n","         -5.2458e-02, -9.6794e-04,  3.7082e-03,  3.7830e-02,  5.3031e-02,\n","         -4.9242e-04,  1.5627e-01, -1.5158e-02,  2.8311e-02,  7.6207e-03,\n","         -2.2950e-02, -2.4789e-04, -2.0634e-02,  7.2830e-02, -2.1431e-01,\n","         -1.0798e-01,  7.2830e-02,  2.3188e-02,  1.4069e-01, -4.3049e-02,\n","         -3.7937e-02, -2.8142e-02,  1.3793e-02, -3.9468e-02,  5.0662e-02,\n","         -6.5151e-02,  2.9034e-02,  1.8929e-01,  1.4129e-02,  1.1832e-02,\n","          5.7442e-02, -9.4417e-02, -3.1733e-02, -1.0168e-01, -1.5439e-01,\n","         -3.5681e-03, -3.7937e-02,  3.0912e-02,  1.4449e-02,  1.1216e-02,\n","         -7.7497e-02,  1.5248e-01, -4.0232e-02,  5.3658e-02,  1.5165e-01,\n","         -1.6471e-02,  1.1028e-02, -2.7203e-03, -1.2684e-02, -7.9760e-02,\n","          3.0960e-03,  1.6329e-02,  1.8708e-01, -2.3404e-02, -2.6531e-02,\n","         -8.3821e-02,  1.0480e-01, -5.9519e-02,  2.9700e-02, -2.3404e-02,\n","         -3.7533e-02, -6.4786e-04, -2.4657e-02,  1.6329e-02, -7.1184e-02,\n","         -4.7727e-02,  1.2235e-01, -3.4704e-02, -2.0099e-02,  4.3255e-02,\n","         -3.5021e-02,  4.3533e-02, -1.6734e-03,  8.9133e-02, -5.8654e-02,\n","          9.6256e-02,  3.6287e-02, -1.4935e-01,  6.7417e-02, -1.3232e-01,\n","         -9.1171e-04, -3.4704e-02], device='cuda:0'),\n"," '_gnn._post_processing.0.weight': tensor([[ 1.1473e-01,  5.5062e-02,  4.2475e-02,  ..., -1.0342e-01,\n","           1.6252e-02, -8.6980e-02],\n","         [ 1.6011e-01, -9.4836e-02, -3.4356e-02,  ..., -1.0359e-02,\n","          -1.5671e-02,  4.9284e-03],\n","         [ 1.6736e-02,  2.0399e-02, -2.3234e-02,  ...,  7.1299e-03,\n","          -3.1316e-02, -2.5870e-02],\n","         ...,\n","         [ 5.3315e-02,  1.6121e-02, -3.5011e-02,  ..., -6.7972e-02,\n","          -1.4133e-02, -5.6744e-02],\n","         [ 1.7713e-02, -5.6888e-02, -1.9359e-02,  ..., -1.0902e-01,\n","          -6.1708e-05,  1.9594e-02],\n","         [ 8.9716e-02,  7.9100e-02, -1.6284e-01,  ..., -5.6112e-02,\n","          -1.9451e-02,  9.4602e-02]], device='cuda:0'),\n"," '_gnn._post_processing.0.bias': tensor([ 0.0264, -0.0145, -0.0175, -0.0540,  0.0315,  0.0201, -0.0040, -0.0150,\n","          0.0499, -0.0401, -0.0682,  0.0175,  0.0495,  0.0047,  0.0024,  0.0192,\n","          0.0023, -0.0485,  0.0653,  0.0301, -0.0125,  0.0202,  0.0046,  0.0253,\n","         -0.0158,  0.0076, -0.0079,  0.0073,  0.0179,  0.0236, -0.0132, -0.0224,\n","         -0.0272, -0.0254, -0.0036, -0.0598, -0.0105,  0.0125, -0.0491,  0.0818,\n","          0.0548,  0.0083,  0.0176, -0.0041,  0.0427,  0.0180, -0.0375,  0.0040,\n","          0.0079, -0.0293, -0.0246, -0.0008, -0.0234,  0.0122,  0.0748, -0.0529,\n","          0.0398,  0.0111,  0.0114,  0.0009,  0.0161,  0.0093,  0.0216,  0.0227,\n","          0.0075,  0.0211,  0.0457,  0.0029, -0.0100,  0.0141, -0.0237, -0.0190,\n","         -0.0231,  0.0130, -0.0329,  0.0149,  0.0050,  0.0032,  0.0141,  0.0286,\n","         -0.0143,  0.0044,  0.0011,  0.0069, -0.0090,  0.0180,  0.0198,  0.0551,\n","          0.0147,  0.0084,  0.0768, -0.0290,  0.0168,  0.0500, -0.0172,  0.0636,\n","         -0.0069, -0.0057,  0.0076,  0.0300,  0.0062, -0.0197,  0.0063, -0.0047,\n","          0.0367,  0.0177,  0.0469,  0.0150, -0.0024, -0.0167,  0.0158,  0.0079,\n","         -0.0366,  0.0021, -0.0131, -0.0095, -0.0168, -0.0155,  0.0405,  0.0692,\n","          0.0161,  0.0259, -0.0215, -0.0072, -0.0128, -0.0976, -0.0009,  0.0612,\n","         -0.0144,  0.0394, -0.0283, -0.0046, -0.0247, -0.0136,  0.0095, -0.0289,\n","         -0.0234,  0.0162, -0.0684, -0.0003, -0.0165,  0.0084, -0.0257, -0.0213,\n","         -0.0104,  0.0010, -0.0106, -0.0022, -0.0163,  0.0805, -0.0241,  0.0545,\n","         -0.0188, -0.0304,  0.0069,  0.0082,  0.0414,  0.0106,  0.0167, -0.0352,\n","          0.0232,  0.0191,  0.0021,  0.0076,  0.0484,  0.0050, -0.0273, -0.0378,\n","         -0.0153, -0.0412, -0.0406,  0.0142, -0.0325,  0.0134, -0.0037, -0.0118,\n","          0.0249,  0.0058,  0.0496, -0.0427,  0.0183,  0.0043,  0.0264,  0.0251,\n","          0.0153, -0.0496,  0.1177,  0.0762,  0.0018,  0.0426, -0.0568,  0.0384,\n","          0.0067,  0.0179, -0.0326,  0.0294, -0.0094,  0.0286, -0.0144,  0.0709,\n","         -0.0141,  0.0092,  0.0188, -0.0279,  0.0339, -0.0032, -0.0189,  0.0027,\n","         -0.0145,  0.0301,  0.0391, -0.0283,  0.0213,  0.0803, -0.0274, -0.0089,\n","         -0.0237, -0.0199,  0.0139, -0.0119,  0.0146, -0.0206, -0.0255,  0.0009,\n","         -0.0195,  0.0275,  0.0952, -0.0533,  0.0583, -0.0263,  0.0001,  0.0174,\n","         -0.0132,  0.0197, -0.0020, -0.0319, -0.0188,  0.0710,  0.0152,  0.0489,\n","          0.0415,  0.0427,  0.0103,  0.0388,  0.0778, -0.0459,  0.0142,  0.0250,\n","         -0.0155,  0.0274, -0.0075,  0.0242, -0.0368, -0.0194,  0.0273,  0.0646,\n","          0.0131,  0.0518, -0.0147,  0.0030, -0.0060, -0.0021,  0.0039,  0.0825,\n","          0.0075,  0.0983,  0.0080, -0.0224,  0.0687, -0.0358,  0.0091,  0.0099,\n","         -0.0118, -0.0345,  0.0362,  0.0261, -0.0351,  0.0646,  0.0001, -0.0298,\n","         -0.0096,  0.0387,  0.0333,  0.0272, -0.0517, -0.0313,  0.0302,  0.0434,\n","          0.0016, -0.0536,  0.0252,  0.0371,  0.0228, -0.0043, -0.0294, -0.0135,\n","         -0.0219,  0.1015,  0.0260, -0.0069,  0.0560, -0.0045, -0.0309, -0.0296,\n","          0.0237,  0.0222, -0.0292,  0.0150, -0.0035,  0.0428, -0.0176,  0.0156,\n","         -0.0582,  0.0237,  0.0132,  0.0260,  0.0372, -0.0009, -0.0418,  0.0084,\n","         -0.0133,  0.0060,  0.0086,  0.0148,  0.0167,  0.0275,  0.0457,  0.0146,\n","          0.0096, -0.0102,  0.0718,  0.0060,  0.0373, -0.0066,  0.0470,  0.0033],\n","        device='cuda:0'),\n"," '_gnn._post_processing.2.weight': tensor([[-0.0011, -0.0789, -0.0679,  ..., -0.0321,  0.0259, -0.0304],\n","         [-0.0804, -0.1258,  0.0185,  ..., -0.0329,  0.0271,  0.0602],\n","         [ 0.0106,  0.0128, -0.0089,  ..., -0.0475,  0.0253,  0.0606],\n","         ...,\n","         [-0.0390, -0.0024,  0.0351,  ...,  0.0516, -0.0524, -0.0062],\n","         [-0.0618,  0.0216, -0.0480,  ..., -0.0985,  0.1127,  0.1367],\n","         [ 0.0667, -0.0115, -0.0190,  ..., -0.2274,  0.0399,  0.0773]],\n","        device='cuda:0'),\n"," '_gnn._post_processing.2.bias': tensor([ 4.0986e-02,  5.3393e-02,  1.1197e-01, -1.0232e-02, -6.8762e-03,\n","          4.8756e-02, -3.7174e-02,  7.5543e-02, -3.3325e-02,  3.6687e-02,\n","          4.6340e-02, -1.5368e-02,  3.3800e-02, -3.1971e-03,  1.8708e-02,\n","          5.4568e-02, -2.3356e-04,  8.4018e-02,  5.5082e-02,  2.8387e-02,\n","          1.3812e-02,  3.4753e-02,  9.7708e-03,  2.4833e-02,  1.7067e-02,\n","          3.8244e-02,  4.4125e-02, -1.6051e-02,  5.3251e-02, -3.5299e-02,\n","         -2.0722e-02,  3.7336e-02, -3.2862e-02,  3.6991e-02, -2.6360e-03,\n","         -4.8794e-03,  7.3613e-02, -8.6990e-03,  7.3775e-02,  1.2326e-02,\n","          5.8458e-02, -2.9182e-03,  8.2698e-02,  6.1589e-02,  2.5629e-02,\n","         -9.9468e-03,  1.8184e-02,  1.9372e-02,  3.6522e-02, -4.5284e-02,\n","          6.4958e-02,  3.1116e-02, -7.1284e-03, -6.2974e-02,  9.1708e-02,\n","         -4.5247e-02,  7.1785e-03, -6.0109e-02, -2.7016e-02, -2.0092e-02,\n","          2.5308e-03, -7.0300e-03,  5.1281e-02,  5.2639e-02,  5.8175e-02,\n","          5.4275e-02,  2.5186e-02,  4.2435e-02, -1.5453e-02,  3.0418e-02,\n","         -4.7474e-03,  9.4903e-02,  7.5123e-03, -3.6963e-02,  6.2863e-02,\n","          6.5022e-02,  5.4254e-02, -2.8531e-02,  3.3983e-05,  4.6703e-02,\n","          1.9044e-02,  4.1413e-02, -2.7016e-02,  2.6781e-02,  3.7017e-03,\n","         -8.2020e-03,  1.2669e-02,  7.2838e-02,  8.8678e-02, -3.2269e-02,\n","         -3.9887e-02,  1.8588e-02,  4.1502e-02,  2.8148e-02,  4.4344e-02,\n","          3.1830e-02,  1.9791e-03, -7.4814e-03, -2.1416e-02, -3.8447e-02,\n","          5.4709e-02,  8.6695e-02,  3.0375e-02,  8.2154e-02,  3.9344e-02,\n","          7.1684e-02,  7.4365e-03,  7.5327e-02, -1.5579e-03,  6.9738e-02,\n","          3.5660e-02,  2.2870e-03,  7.9464e-02,  1.7481e-02,  8.1579e-02,\n","          6.6465e-02,  3.3184e-02,  1.3225e-01, -2.7039e-02,  4.5442e-02,\n","          1.0422e-01,  1.0150e-01,  7.2374e-02,  7.5270e-02,  2.7561e-02,\n","          2.3729e-02,  4.7813e-02,  2.3766e-02,  1.9588e-02, -6.3471e-03,\n","          3.2477e-02,  2.2886e-03,  6.3524e-02,  2.3045e-02, -9.7559e-04,\n","         -3.3297e-03,  7.9110e-02,  6.6379e-03,  9.5475e-03,  3.6328e-03,\n","          9.0260e-02,  3.1370e-02, -5.7227e-03, -3.6260e-03,  3.1631e-02,\n","          5.9618e-02,  1.1759e-02,  1.8598e-02,  1.3674e-02,  1.0802e-01,\n","          1.7758e-02,  4.1417e-02, -1.5820e-02, -5.7269e-03,  4.2297e-02,\n","         -1.0483e-02, -4.5488e-02,  1.0626e-01, -3.8235e-02,  4.3223e-02,\n","         -1.1849e-03, -2.1411e-02,  3.8166e-02,  4.9952e-02,  7.6826e-02,\n","         -8.6467e-03,  5.4091e-04, -1.0391e-02, -4.1065e-03, -3.8472e-02,\n","         -4.0216e-03,  3.3963e-02,  2.4017e-02,  4.3195e-02,  4.2941e-02,\n","          9.9316e-02, -4.7332e-03,  3.1435e-02,  4.7900e-02,  1.6760e-02,\n","          6.6018e-02,  4.7243e-02, -2.9234e-02, -4.7067e-02,  4.8278e-02,\n","          3.6614e-02,  5.4803e-03, -4.9886e-02,  4.4530e-02,  9.1480e-03,\n","         -1.9501e-02, -4.7451e-02, -5.0487e-03,  4.3224e-02,  3.6817e-02,\n","          2.6948e-02,  3.5228e-02, -9.1446e-05,  2.8988e-02,  6.8598e-03,\n","          6.7573e-04, -3.1313e-02,  2.2957e-02,  1.6538e-03,  7.9095e-02,\n","          7.7163e-02, -6.7388e-03,  2.4739e-02,  4.9876e-02,  1.6863e-02,\n","          5.1561e-02, -1.6556e-02,  2.1927e-02,  1.6124e-02,  4.6513e-02,\n","          7.8291e-02, -6.9810e-03,  9.5160e-02,  1.2422e-02, -2.5172e-02,\n","          6.0622e-02,  1.3028e-02, -2.3430e-02,  8.9008e-02, -1.5255e-02,\n","          4.5574e-02, -7.0764e-03, -1.1097e-02, -8.1105e-03,  8.0369e-02,\n","          9.0634e-02,  1.4093e-02,  7.7515e-02, -1.6020e-02, -1.8651e-02,\n","          4.2314e-02,  1.0702e-01,  3.8876e-02,  5.2785e-02,  6.5990e-02,\n","          2.6752e-02, -5.0465e-02,  7.1634e-02,  1.5169e-02, -5.5964e-02,\n","          6.8781e-02,  4.3702e-02, -5.3110e-03,  6.2095e-02,  1.4812e-02,\n","          8.3641e-03,  6.2674e-02, -5.3070e-02,  4.4205e-03,  4.0622e-02,\n","          4.8805e-02], device='cuda:0'),\n"," '_gnn._readout.0.weight': tensor([[ 0.0319, -0.0077,  0.0176,  ..., -0.0992, -0.0522, -0.0616],\n","         [-0.0749,  0.0414,  0.1954,  ...,  0.0018,  0.1734, -0.1398],\n","         [ 0.1516,  0.0865,  0.0706,  ..., -0.0447, -0.0265,  0.0120],\n","         ...,\n","         [-0.0833,  0.0052,  0.1109,  ...,  0.0914, -0.1186,  0.2296],\n","         [-0.0833,  0.0031, -0.0166,  ..., -0.0685, -0.1237, -0.0697],\n","         [-0.0151, -0.0611,  0.0326,  ..., -0.0547,  0.0135, -0.1277]],\n","        device='cuda:0'),\n"," '_gnn._readout.0.bias': tensor([ 0.0067, -0.0811, -0.1166, -0.0647, -0.0219, -0.0694, -0.0704,  0.0215,\n","         -0.0324, -0.0305, -0.0459, -0.0031, -0.0428, -0.0349, -0.0391, -0.0307,\n","         -0.0374, -0.0044,  0.0239,  0.1172,  0.0251, -0.0650, -0.0135,  0.0058,\n","         -0.0543, -0.0256, -0.0011,  0.0170,  0.0204,  0.0034, -0.0907,  0.0403,\n","         -0.0017, -0.0174, -0.0380, -0.0386,  0.0119, -0.0376, -0.0473, -0.0096,\n","         -0.0121, -0.0796,  0.0006, -0.0391, -0.0265,  0.0324, -0.0651,  0.0011,\n","         -0.0194, -0.0096, -0.0017, -0.0089, -0.0325, -0.0036, -0.0157, -0.0279,\n","          0.0276, -0.0135,  0.0256,  0.0157, -0.0778, -0.0571, -0.0414, -0.0100,\n","          0.0666, -0.0537,  0.0302,  0.0338,  0.0005,  0.0151, -0.0195, -0.0878,\n","         -0.0468, -0.0557, -0.0219,  0.0137, -0.0326, -0.0018, -0.0046, -0.0032,\n","         -0.0153, -0.0404,  0.0080,  0.0546,  0.0277, -0.0221, -0.0177, -0.0298,\n","          0.2142,  0.0141,  0.0216, -0.0537, -0.0348, -0.0492, -0.0282,  0.0581,\n","         -0.0201, -0.0151, -0.0630, -0.0469, -0.0332, -0.0235, -0.0504,  0.0717,\n","         -0.0141, -0.0847, -0.0510, -0.0432, -0.0269, -0.0729, -0.0244, -0.0155,\n","          0.0291, -0.0368,  0.0079, -0.0717,  0.0169, -0.0116,  0.0233,  0.0327,\n","         -0.0053,  0.0152, -0.0560, -0.0095,  0.0144, -0.0974,  0.0068, -0.0911],\n","        device='cuda:0'),\n"," '_tasks.0._affine.weight': tensor([[-8.2597e-02, -5.8553e-02, -3.7914e-02,  7.4411e-02, -1.8289e-02,\n","          -1.2413e-01,  1.0945e-01, -2.5040e-02,  9.1893e-02,  8.2980e-02,\n","           3.4739e-02, -4.7544e-02, -1.8613e-02,  2.4164e-02, -1.4449e-01,\n","           7.0136e-02,  4.6942e-03,  2.0516e-02, -4.5437e-02, -8.2448e-04,\n","           1.5876e-02,  1.4153e-01, -3.7156e-02, -8.8227e-02, -9.1773e-02,\n","          -4.3693e-02, -1.8500e-02, -1.3024e-02,  4.8934e-02, -4.9434e-02,\n","          -9.8069e-02, -1.5154e-02,  2.5191e-02, -1.3407e-01, -1.2930e-01,\n","           3.3142e-03,  1.7752e-02, -2.5297e-02, -2.0878e-02, -2.2167e-03,\n","          -2.0044e-02,  3.0600e-02,  2.8915e-02, -1.1238e-02, -2.1092e-02,\n","           1.3429e-02, -1.2266e-01,  1.6777e-02,  4.9347e-02,  1.4683e-03,\n","          -6.3035e-02,  1.8177e-02, -3.1266e-02,  1.5532e-01, -1.2335e-02,\n","          -5.8001e-02,  9.6366e-03,  1.6047e-01,  2.7503e-03,  6.2951e-02,\n","           4.5167e-02, -2.0493e-01, -1.3539e-01,  6.7008e-02, -9.3785e-03,\n","          -5.3343e-02,  6.6001e-03,  1.5787e-02, -2.3814e-02, -2.3834e-03,\n","           2.6897e-01, -3.7819e-02,  1.0221e-01,  3.0027e-02, -1.7072e-01,\n","          -9.2460e-02, -1.2985e-01,  8.7642e-02,  8.2145e-03, -4.6029e-03,\n","          -9.0852e-02, -8.7500e-02,  1.5121e-01, -2.4884e-02, -2.8025e-02,\n","           8.2375e-02, -2.1454e-04, -5.0602e-02, -3.9358e-04,  1.3763e-01,\n","           4.3716e-02,  3.8783e-03, -1.1799e-01, -8.5654e-02, -9.5998e-02,\n","          -3.0089e-03,  3.7797e-02, -9.4503e-03,  5.9124e-03, -3.0286e-02,\n","           6.9774e-02,  1.0467e-01, -8.5472e-02, -1.5199e-02, -3.6688e-02,\n","           9.5326e-02,  1.4526e-01,  9.7719e-02, -1.2025e-01, -8.3593e-03,\n","           8.5633e-02,  1.2113e-01,  1.1915e-01, -1.6346e-01,  1.1933e-01,\n","          -8.2245e-02, -1.0608e-01, -3.6727e-02,  6.3169e-02,  9.7589e-03,\n","           1.7219e-02,  2.0658e-02,  6.5809e-02,  4.9900e-02,  5.7884e-02,\n","           2.1416e-02,  2.6079e-02, -1.0429e-03],\n","         [ 6.9765e-02, -1.0302e-01, -8.6638e-02, -7.7154e-02, -1.6168e-01,\n","          -3.4960e-02,  4.0855e-04,  1.0171e-01, -1.7847e-02,  5.3389e-02,\n","           2.6061e-02, -3.7283e-01, -3.0504e-02,  1.8582e-03,  8.1737e-02,\n","          -2.3475e-01,  1.2634e-01,  1.6101e-03,  3.6718e-02, -4.6340e-02,\n","           9.8308e-03, -3.9410e-02,  4.2418e-02,  2.4438e-02,  1.5682e-01,\n","          -4.7761e-02, -6.5603e-03, -1.4331e-01,  6.2521e-02,  1.0813e-01,\n","          -7.4578e-02, -2.4184e-03,  6.3487e-02, -1.1540e-02,  4.9701e-02,\n","           1.5876e-01,  5.2986e-03,  1.7801e-02,  2.6525e-03,  2.3956e-02,\n","          -2.0738e-01, -4.4090e-02,  1.8758e-02, -5.0791e-02,  3.6995e-02,\n","          -4.3731e-03,  7.0698e-02, -2.6633e-02, -8.6686e-02,  4.7509e-03,\n","           6.9927e-02,  2.3053e-02, -1.9258e-03,  4.7974e-02,  2.1419e-03,\n","          -1.1839e-01,  4.5425e-03,  1.4763e-02,  3.0708e-02, -3.0352e-01,\n","          -5.3746e-02,  8.0103e-02,  3.6847e-02, -5.1455e-04,  8.5193e-03,\n","          -7.0982e-03,  7.2168e-03,  7.3675e-03,  6.0589e-02,  2.5570e-02,\n","           6.3582e-02, -8.1093e-02, -6.8267e-02, -1.0501e-01, -8.1840e-02,\n","           5.2280e-02, -1.5048e-01, -1.8731e-02, -5.4208e-03,  1.2375e-01,\n","           4.8034e-02, -9.0022e-04,  6.4265e-02, -1.7392e-03,  2.8052e-03,\n","           4.1219e-02, -6.2725e-02, -3.5219e-02, -3.4627e-02, -1.3680e-01,\n","           1.5599e-02, -1.7315e-02,  2.3168e-02,  1.3459e-02, -7.7606e-02,\n","          -3.3475e-03, -9.4246e-04, -3.0015e-02, -1.3218e-01, -7.5437e-02,\n","           1.3422e-01,  1.2180e-02, -3.5838e-02,  1.0610e-02,  2.0652e-02,\n","          -8.5850e-02,  1.8103e-02,  5.7237e-02, -2.2532e-03, -8.4948e-02,\n","           1.3249e-01,  5.9470e-02,  4.2128e-02, -3.4456e-03,  2.6067e-02,\n","          -7.1166e-02,  1.4706e-02,  1.4242e-01,  3.4112e-02,  2.0636e-03,\n","           4.3080e-02, -1.7369e-01, -1.0601e-01,  3.4086e-02,  4.9337e-02,\n","          -8.3916e-02, -3.1281e-03, -1.1952e-02],\n","         [-4.0939e-03, -4.9567e-02,  5.2535e-02, -7.7419e-02,  1.0069e-03,\n","           8.0872e-02, -6.5128e-02, -1.6724e-02,  1.0588e-01, -1.3091e-01,\n","           1.5340e-02,  6.3774e-02, -2.0792e-01, -2.2444e-02,  1.0993e-01,\n","           5.9718e-02, -5.2019e-03, -1.3342e-02, -3.5067e-02,  1.3701e-04,\n","           1.3177e-02, -1.8131e-02,  1.2691e-01, -9.8540e-02, -2.3159e-02,\n","           1.3065e-01, -3.8432e-02, -2.0571e-02, -9.1139e-02, -1.9819e-01,\n","          -3.0040e-02, -2.1268e-02,  8.3517e-02, -1.1579e-01, -1.4131e-02,\n","           1.2245e-01, -1.0584e-02,  2.8721e-02,  1.4160e-02, -3.1255e-04,\n","           1.3551e-01,  2.9196e-02, -2.1596e-02, -1.7694e-02,  1.1277e-01,\n","          -2.4536e-02,  8.7106e-02,  1.4086e-01,  4.3453e-02, -1.3436e-01,\n","           1.8077e-02,  2.1627e-02,  1.8303e-02, -1.7906e-02, -2.1026e-02,\n","           3.4138e-02, -6.8285e-02, -2.1502e-02, -1.5666e-03, -1.7516e-01,\n","           9.6510e-02, -1.7383e-02,  2.3603e-01, -1.4219e-01,  1.3956e-02,\n","          -1.3152e-01, -1.9201e-02,  1.0639e-02, -9.8892e-02, -5.7596e-02,\n","          -3.5211e-02, -9.8355e-02,  5.6036e-02, -4.8160e-02, -1.0493e-01,\n","          -1.1417e-01,  5.4722e-02, -9.2426e-02,  3.7178e-02, -1.2515e-01,\n","           6.1769e-02, -4.4924e-03, -1.9539e-01,  5.2451e-03,  1.7168e-02,\n","           2.4761e-01, -1.9923e-03,  5.9102e-02, -2.1182e-04, -6.4073e-02,\n","           1.2104e-01, -1.3174e-01, -6.9486e-02,  1.1442e-01, -3.6555e-02,\n","           2.6258e-02, -8.4618e-03,  2.7075e-02,  1.6144e-02, -3.3784e-03,\n","           6.0735e-02,  2.4606e-02, -8.5431e-02, -1.1673e-02, -2.9420e-02,\n","          -1.9122e-02,  4.4900e-02,  8.7239e-02,  4.3335e-02,  8.7931e-02,\n","          -5.1070e-02, -2.1275e-02,  6.0395e-02, -2.2173e-02, -7.5233e-02,\n","           1.8386e-02, -5.6259e-02,  9.9726e-02,  2.4707e-02,  2.0791e-02,\n","           1.2444e-02,  8.2318e-02,  5.0513e-02,  7.5885e-02,  1.3956e-01,\n","          -8.4603e-02,  1.0725e-02,  9.2813e-04]], device='cuda:0'),\n"," '_tasks.0._affine.bias': tensor([ 0.0008, -0.0079, -0.0014], device='cuda:0')}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model_large_mapped_state_dict"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["OrderedDict([('_gnn._conv_layers.0.nn.0.weight',\n","              tensor([[-0.0590,  0.0250, -0.0824,  ...,  0.0752,  0.1129,  0.0989],\n","                      [ 0.1492, -0.0458, -0.1469,  ...,  0.0652,  0.0014,  0.1690],\n","                      [-0.1397, -0.0107,  0.0575,  ..., -0.0207, -0.0671, -0.0339],\n","                      ...,\n","                      [ 0.0143,  0.0851,  0.0321,  ...,  0.1410,  0.1231, -0.0308],\n","                      [ 0.0979,  0.0097,  0.1237,  ..., -0.0466,  0.0295, -0.0333],\n","                      [-0.0362,  0.1529, -0.1175,  ...,  0.1245,  0.0384, -0.0468]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.0.nn.0.bias',\n","              tensor([ 0.1443,  0.0476,  0.1328,  0.1072, -0.0953, -0.0385,  0.0445,  0.0620,\n","                       0.0998,  0.0543,  0.0197, -0.0371,  0.1526,  0.0691,  0.1502, -0.1699,\n","                      -0.1003,  0.1492,  0.1033, -0.1461, -0.1065,  0.0933, -0.0070, -0.0206,\n","                      -0.1021,  0.0419, -0.0666,  0.0072,  0.1412,  0.0866, -0.0388,  0.0650,\n","                      -0.0958,  0.0821,  0.0944,  0.0730,  0.0204,  0.1461, -0.0131,  0.1414,\n","                       0.0357, -0.1383,  0.0769,  0.1685,  0.1695, -0.1519,  0.1098,  0.0035,\n","                       0.1618, -0.1637,  0.0721,  0.1621,  0.0108,  0.1200,  0.1232,  0.1295,\n","                       0.0849,  0.0321, -0.0908, -0.1336, -0.1519,  0.0547, -0.0644, -0.1301,\n","                      -0.1056, -0.1653, -0.0592,  0.0377, -0.0821,  0.0663,  0.0874, -0.0552,\n","                       0.1384, -0.1505, -0.0192, -0.0458,  0.1675,  0.0889, -0.0630,  0.0299,\n","                      -0.0746,  0.0805, -0.0127,  0.1078,  0.1326, -0.0859,  0.1513, -0.0695,\n","                       0.1309, -0.0047,  0.0913,  0.1538,  0.0745,  0.1417, -0.1450, -0.0925,\n","                      -0.1229,  0.0584, -0.1216, -0.1664,  0.1522, -0.1582, -0.0961, -0.1259,\n","                      -0.0511, -0.1458, -0.0499, -0.1596, -0.0187, -0.0875,  0.0862, -0.0420,\n","                      -0.0503,  0.1605,  0.1533, -0.1359,  0.1713,  0.0867,  0.0018, -0.0555,\n","                      -0.0907, -0.0090, -0.1612, -0.1491,  0.0889, -0.0086, -0.1593, -0.0927,\n","                      -0.1295, -0.1194, -0.1414,  0.0512, -0.0649, -0.0682,  0.0234,  0.1053,\n","                      -0.0216, -0.0936, -0.1299,  0.1529,  0.1320, -0.1208,  0.1527,  0.0714,\n","                       0.0520,  0.0101, -0.0304,  0.0991,  0.1006,  0.1547,  0.0534, -0.0066,\n","                      -0.1463, -0.0959,  0.0138, -0.0764,  0.1047, -0.0478,  0.1020, -0.0590,\n","                      -0.1555,  0.0507, -0.1371,  0.0298,  0.0897, -0.1532,  0.0350,  0.0604,\n","                      -0.1567, -0.0754, -0.0882, -0.0838, -0.0801,  0.1535, -0.0769,  0.0293,\n","                      -0.0170, -0.0487,  0.1292,  0.1376, -0.0282, -0.0849,  0.1392,  0.0355,\n","                      -0.0982, -0.0244,  0.1367, -0.1138, -0.1084,  0.0985, -0.1504, -0.0562,\n","                       0.0030,  0.1616,  0.0527,  0.1706, -0.1711, -0.0782, -0.1019, -0.1113,\n","                      -0.1244,  0.0348,  0.0054, -0.1481,  0.1048, -0.0892, -0.1213, -0.0367,\n","                       0.0148, -0.0691, -0.1060,  0.0879,  0.0362,  0.0388,  0.1092, -0.1303,\n","                      -0.1621, -0.0947,  0.1651, -0.0885,  0.0536,  0.0714, -0.0262,  0.1130,\n","                      -0.0332, -0.1668, -0.0447, -0.0899,  0.0670, -0.1170, -0.0475,  0.0291,\n","                       0.1329, -0.1143,  0.0291,  0.0887,  0.1696, -0.1641, -0.0929, -0.0788,\n","                       0.1343,  0.1429,  0.0243, -0.1266, -0.0102, -0.1712,  0.0871,  0.1028,\n","                       0.0723, -0.1404,  0.1590,  0.1247,  0.0249,  0.1413,  0.0951, -0.0107],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.0.nn.2.weight',\n","              tensor([[ 0.0587, -0.0023,  0.0308,  ..., -0.0484, -0.0157, -0.0567],\n","                      [ 0.0331, -0.0062, -0.0571,  ...,  0.0137, -0.0041,  0.0596],\n","                      [-0.0464, -0.0308,  0.0288,  ..., -0.0366, -0.0298, -0.0087],\n","                      ...,\n","                      [ 0.0463,  0.0573,  0.0098,  ...,  0.0453,  0.0156,  0.0436],\n","                      [-0.0093,  0.0469,  0.0512,  ...,  0.0083, -0.0048,  0.0549],\n","                      [ 0.0090,  0.0036, -0.0596,  ..., -0.0376,  0.0388,  0.0604]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.0.nn.2.bias',\n","              tensor([-5.6331e-02, -4.7917e-03, -2.1245e-02,  1.9083e-02,  8.7432e-03,\n","                      -5.9198e-02, -4.6751e-02,  1.4081e-02,  3.4198e-02, -4.7241e-02,\n","                       1.3191e-02, -1.7430e-02,  2.1249e-02,  3.6420e-03, -4.1010e-02,\n","                      -5.4059e-02,  4.4620e-02,  2.9807e-02,  5.3955e-02, -8.7655e-03,\n","                       4.3864e-02,  2.4334e-02, -5.8516e-02,  4.5021e-02, -4.3678e-02,\n","                      -4.4581e-02,  6.1205e-02,  5.3366e-02,  1.0644e-02,  8.4624e-03,\n","                      -2.4538e-02, -1.6031e-02,  4.4345e-02,  4.3793e-02,  4.4134e-02,\n","                       2.3212e-02,  6.2046e-02, -3.4840e-02,  1.0116e-03, -3.0662e-02,\n","                       4.0346e-02,  5.3425e-02, -1.6906e-02, -3.6555e-02, -3.8541e-05,\n","                       4.0472e-02, -6.1434e-02, -3.0793e-02, -3.9589e-02,  2.7760e-02,\n","                      -1.2390e-02,  2.5848e-02, -3.2683e-02, -4.8609e-02,  5.4498e-02,\n","                      -5.4624e-02, -1.1066e-02, -3.0425e-02,  3.4707e-02, -7.6965e-03,\n","                       2.0590e-02, -1.6427e-02,  1.9391e-02,  2.3097e-02, -7.1821e-04,\n","                       7.0626e-03,  2.5266e-02, -5.6782e-02,  3.0710e-02,  4.2638e-02,\n","                       5.7602e-02,  8.2318e-03,  5.5935e-02, -1.2548e-02, -5.4260e-02,\n","                       2.1187e-02,  3.9383e-02,  6.6948e-03,  1.8347e-02, -4.5501e-02,\n","                      -2.4654e-02,  3.5772e-02,  2.6507e-02,  3.5153e-04,  4.5370e-04,\n","                       1.5498e-02, -5.3891e-03,  1.6359e-02,  5.9549e-02, -5.3774e-02,\n","                      -1.0008e-03, -6.0304e-02,  3.3924e-02,  3.4205e-02, -2.9894e-02,\n","                       4.0768e-02,  3.4006e-02,  2.6469e-02,  2.1637e-02,  4.9500e-02,\n","                      -5.2664e-02,  5.4507e-03, -5.2918e-02,  3.5696e-02, -3.9262e-02,\n","                       2.0022e-03, -2.6057e-02, -1.9062e-02,  4.6738e-02, -3.0473e-02,\n","                      -2.8294e-02, -6.7964e-03, -2.9311e-02, -4.5277e-02, -4.7444e-02,\n","                       1.5183e-04, -5.3132e-02, -3.3482e-02, -3.8593e-02, -2.1896e-02,\n","                      -2.7220e-02,  6.9830e-03,  1.6484e-03, -2.2354e-02,  2.1434e-02,\n","                       2.6783e-02,  5.0103e-02, -5.4380e-02, -3.3598e-02,  1.5369e-03,\n","                      -4.3058e-02, -2.5257e-02, -5.1263e-02, -1.1869e-02,  5.0215e-02,\n","                       1.6434e-02,  5.5387e-02,  5.8937e-02,  5.3174e-02,  3.8933e-02,\n","                       2.4287e-03,  6.2123e-03,  5.4376e-02,  2.8985e-02,  2.7721e-02,\n","                      -5.7181e-03, -4.7868e-02,  4.3992e-02,  2.8451e-03,  1.8528e-02,\n","                       5.7068e-02, -4.3137e-02,  4.7848e-02,  3.1414e-02, -6.1358e-02,\n","                       2.5633e-02, -2.8253e-02, -1.0626e-02,  3.8861e-02,  1.2494e-02,\n","                      -5.0403e-02,  2.4826e-02, -1.5000e-02, -3.3742e-02,  1.5007e-02,\n","                       4.4584e-02,  1.9445e-02, -3.5449e-02,  4.3178e-03,  3.2475e-02,\n","                      -1.8659e-02, -4.2482e-02, -4.8574e-03, -2.7162e-02, -1.1405e-02,\n","                       2.2149e-02, -2.8475e-02,  3.4124e-02,  4.4912e-02,  5.7360e-02,\n","                      -6.1504e-02,  2.2749e-02, -3.5655e-02, -3.7884e-02, -2.1838e-02,\n","                      -4.2246e-02, -7.0476e-03, -3.8217e-02, -1.2150e-02,  4.4339e-02,\n","                       2.0193e-02, -6.8960e-03, -4.8670e-02, -1.0346e-02,  5.6436e-02,\n","                       2.2071e-02,  6.0442e-02,  2.0694e-04, -5.5869e-02,  4.8480e-02,\n","                      -3.3774e-02, -2.3160e-02,  5.8344e-02, -4.9717e-02, -2.1283e-03,\n","                       3.6323e-02,  6.2069e-02,  3.6988e-02,  3.2644e-02, -5.7140e-02,\n","                      -1.0548e-02,  2.9066e-02,  3.2245e-02, -4.8411e-02,  5.4223e-02,\n","                       2.9209e-02,  4.8994e-02, -3.2982e-02, -4.5810e-02, -1.8936e-03,\n","                       3.8110e-02,  4.8720e-02,  2.3547e-03,  6.1998e-02, -4.6096e-03,\n","                      -6.1781e-03, -1.0145e-02, -6.1602e-02, -2.5357e-02, -2.3505e-02,\n","                       4.7254e-03,  9.6378e-03,  2.3828e-02, -2.0538e-02,  1.3730e-02,\n","                       3.4126e-02, -2.9967e-02, -2.6074e-02,  4.0955e-02, -3.5659e-02,\n","                      -1.1796e-02, -5.6557e-02,  5.8075e-02,  1.6985e-02, -1.5816e-02,\n","                       1.8390e-02, -4.2485e-02,  3.5706e-02,  2.2741e-03, -3.0601e-02,\n","                       3.0115e-02,  2.2431e-02, -4.8438e-02,  6.8534e-03,  9.5170e-03,\n","                      -1.1458e-02,  9.8237e-04, -5.2534e-02,  5.4459e-02,  3.8761e-02,\n","                       5.1677e-02,  1.4488e-02, -4.5837e-02,  5.6191e-03,  4.8739e-02,\n","                       4.7907e-02, -2.8303e-02, -1.1747e-02,  7.7773e-03, -4.7245e-02,\n","                      -2.9678e-02, -4.6048e-02,  3.2166e-02, -2.5235e-02, -3.6433e-02,\n","                       5.8591e-02, -4.4574e-02, -1.4021e-03, -2.0002e-03, -5.3438e-02,\n","                      -1.3693e-02, -1.0328e-02,  5.4087e-02,  1.2176e-02,  8.7924e-04,\n","                      -1.4217e-02,  3.6705e-02,  4.4767e-02, -3.3282e-02, -2.2118e-02,\n","                      -6.0269e-02, -5.7216e-02, -9.3927e-03, -5.9778e-03, -6.1989e-02,\n","                      -1.8599e-02, -2.8482e-02, -5.9244e-02,  3.0159e-03, -4.9719e-02,\n","                       4.5652e-02,  9.7653e-04, -5.6003e-02, -4.5425e-02, -6.0949e-02,\n","                      -3.7844e-03, -5.8630e-02, -2.3955e-02,  5.8874e-02,  2.2636e-02,\n","                       9.0743e-04,  5.6450e-02, -4.5830e-02,  2.8452e-02, -6.9738e-03,\n","                      -1.2676e-03, -4.5640e-02, -5.2143e-02, -5.2195e-02, -3.1714e-02,\n","                      -1.6263e-02, -1.0810e-02, -4.5155e-02, -4.2186e-02,  4.3671e-02,\n","                      -5.8513e-02,  1.7171e-02, -1.1981e-02, -3.6721e-02, -2.3573e-02,\n","                       3.0574e-03,  3.4945e-02,  3.9924e-02, -2.2837e-02,  4.1294e-02,\n","                       6.2307e-02, -2.9495e-02,  1.8955e-02, -3.8471e-02,  4.5325e-02,\n","                      -6.1296e-02,  1.4152e-02,  1.3509e-02,  5.5321e-02, -2.4930e-02,\n","                      -4.8186e-02, -2.2707e-03, -3.5137e-02,  1.6811e-02, -4.3172e-02,\n","                      -2.4385e-02, -1.7192e-02,  5.8232e-02,  1.1218e-03, -5.5443e-02,\n","                       3.1227e-03,  2.4959e-02, -4.8759e-03, -2.8377e-02,  4.7138e-02,\n","                       5.9567e-02, -5.3969e-02,  5.2174e-02, -4.2293e-02, -4.9448e-02,\n","                      -2.3478e-02,  5.9201e-02,  1.0654e-02,  2.2035e-02,  3.8629e-02,\n","                      -2.3278e-02, -1.8230e-03,  4.4707e-02,  5.3418e-02,  5.7929e-02,\n","                      -1.0374e-02, -3.3562e-02,  1.7245e-02,  5.3310e-05, -2.9057e-02,\n","                       1.0991e-02,  2.8180e-02,  3.0398e-02,  4.9006e-03, -3.4461e-02,\n","                      -2.7505e-02,  1.1458e-02,  5.2035e-02, -4.5397e-02, -2.9378e-02,\n","                       5.2322e-02,  3.5934e-02,  5.7976e-02, -1.3730e-02,  4.2150e-02,\n","                      -7.4405e-03, -6.1592e-02,  5.4595e-02, -5.2713e-04,  4.2796e-02,\n","                      -1.2454e-02,  3.4335e-03,  5.9703e-02,  2.9053e-03, -1.4082e-02,\n","                      -2.9695e-02,  3.8204e-02, -3.2744e-02, -7.3008e-05,  2.5011e-02,\n","                       1.5775e-02,  3.3511e-02, -4.5609e-02, -3.3967e-02,  9.8137e-03,\n","                       5.7153e-02,  5.5004e-02, -3.1860e-02, -4.7102e-02,  7.0248e-03,\n","                      -3.6136e-02,  8.8515e-03, -4.1003e-02,  4.3255e-02, -1.8949e-02,\n","                       1.8296e-02,  5.8267e-02, -3.4851e-02,  1.7425e-02,  1.5366e-02,\n","                      -5.5896e-03, -2.6357e-03, -1.9318e-02,  2.2656e-02,  5.3149e-02,\n","                      -2.2276e-02, -5.7497e-03, -3.5906e-02, -2.3427e-02,  4.5138e-02,\n","                      -4.1511e-02, -3.3121e-02,  1.3682e-02, -5.6722e-02, -9.0778e-03,\n","                       7.1156e-03, -4.6449e-02,  1.5716e-02, -2.1137e-02, -7.8131e-03,\n","                       5.7391e-02, -4.0271e-02,  3.7347e-02, -1.6312e-02,  5.9861e-02,\n","                      -6.0632e-02,  4.1085e-02, -6.0950e-02, -1.6390e-02,  4.5207e-02,\n","                      -4.9037e-02, -1.8171e-02, -5.5160e-02, -1.2739e-02,  3.6728e-02,\n","                       5.7278e-02,  1.4489e-02,  4.2359e-02,  6.0034e-02, -2.3858e-02,\n","                       6.0858e-02,  3.4888e-02,  5.7496e-02,  3.0983e-02,  5.0185e-02,\n","                      -4.4839e-02,  5.3455e-02,  1.8251e-02,  1.2304e-02,  3.3887e-02,\n","                      -3.8725e-02, -7.5098e-03, -6.2031e-02,  3.1927e-02,  5.6981e-03,\n","                      -2.3664e-02, -6.1851e-02,  8.1860e-03,  5.3766e-02, -1.3249e-03,\n","                      -4.1656e-02, -5.1641e-02,  1.4312e-02, -3.6733e-02, -1.6234e-02,\n","                       4.8434e-02,  1.5519e-02,  2.2782e-02,  1.2268e-02,  4.9094e-02,\n","                      -5.3921e-02,  3.6843e-02,  3.1551e-02,  3.7333e-02,  1.3812e-03,\n","                      -4.2404e-02, -4.1345e-03,  4.3084e-02,  4.6998e-02,  4.0971e-02,\n","                       5.3491e-02,  2.2689e-02], device='cuda:0')),\n","             ('_gnn._conv_layers.1.nn.0.weight',\n","              tensor([[-0.0150,  0.0073, -0.0126,  ..., -0.0129, -0.0292,  0.0207],\n","                      [-0.0125,  0.0236,  0.0250,  ...,  0.0183, -0.0235, -0.0132],\n","                      [ 0.0178,  0.0156, -0.0051,  ..., -0.0251, -0.0235,  0.0059],\n","                      ...,\n","                      [-0.0101,  0.0024, -0.0292,  ...,  0.0163,  0.0108, -0.0256],\n","                      [ 0.0259,  0.0250,  0.0222,  ..., -0.0134, -0.0301, -0.0299],\n","                      [ 0.0296, -0.0306, -0.0040,  ..., -0.0090,  0.0198,  0.0203]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.1.nn.0.bias',\n","              tensor([ 6.9780e-03, -1.7690e-02, -8.6202e-03,  7.5336e-03, -1.4912e-02,\n","                      -2.1184e-02, -7.0046e-03, -1.7010e-02,  1.8535e-02, -3.3382e-03,\n","                      -2.9631e-02,  1.0387e-02, -2.5185e-02, -5.4940e-03, -6.5698e-03,\n","                      -1.0808e-02,  1.6781e-02,  1.6535e-02,  1.0819e-02, -2.5386e-02,\n","                       9.2858e-03, -1.5278e-02, -1.0712e-02, -7.2515e-03,  2.2504e-02,\n","                      -1.6360e-02, -1.2046e-02, -1.4970e-02,  1.9171e-02, -1.9911e-02,\n","                       7.9826e-03,  1.8501e-02, -1.2063e-02, -3.0446e-02, -3.5024e-03,\n","                       2.3951e-02,  2.1605e-03,  1.7668e-02,  8.9298e-03,  1.3094e-03,\n","                       5.8974e-03,  2.9776e-02,  2.1970e-02,  2.0736e-02,  2.4049e-02,\n","                       5.5122e-03,  2.2534e-02, -1.6338e-02,  1.7028e-02,  1.3183e-02,\n","                      -2.0712e-02,  2.4803e-02, -1.2759e-02, -1.1942e-02,  1.9479e-02,\n","                       5.0986e-03, -3.0812e-02, -1.6113e-02, -1.9670e-02, -2.8006e-02,\n","                       8.5451e-03,  2.3719e-02, -2.2882e-02, -1.1983e-02, -2.1117e-02,\n","                       1.8111e-02,  1.0641e-02, -2.3709e-02,  1.5056e-02,  4.4757e-03,\n","                      -4.8466e-03,  8.8250e-03,  1.2194e-02, -4.5111e-03,  5.1474e-03,\n","                      -1.5229e-02, -2.6440e-02,  3.9088e-03, -1.9659e-02, -1.5572e-02,\n","                      -1.5505e-02, -2.7553e-02, -1.5077e-02,  8.5941e-03,  2.3065e-02,\n","                       2.2193e-02,  1.6089e-02,  9.7333e-03,  2.3362e-02, -2.8071e-02,\n","                      -1.4939e-02,  1.1411e-03, -2.1277e-02,  1.2267e-02, -2.8159e-02,\n","                       2.3468e-02, -1.7118e-02,  2.7915e-02, -1.6392e-02, -2.9173e-02,\n","                       1.4636e-02,  6.4723e-03,  1.1394e-02,  7.2596e-03, -2.2852e-02,\n","                       2.6511e-02, -5.1209e-03,  5.1026e-03,  6.4173e-03,  2.4073e-03,\n","                       1.7686e-02,  9.6387e-03,  9.0214e-03, -1.7464e-02,  8.9261e-04,\n","                      -2.3711e-02,  3.6509e-03,  9.4141e-03,  4.5768e-03, -2.2383e-02,\n","                      -2.2591e-03,  5.8909e-03, -1.6736e-02, -1.5023e-02,  2.7116e-02,\n","                      -1.5949e-02, -9.3155e-03,  2.0300e-02,  2.7120e-02,  8.4827e-03,\n","                      -1.8325e-02,  1.1035e-02,  2.6579e-02, -1.2515e-03, -2.5018e-02,\n","                      -6.3214e-03,  3.1256e-04,  6.2953e-03,  1.7893e-02, -2.5063e-02,\n","                      -1.9647e-02, -4.7366e-03, -1.4534e-02,  6.1896e-03, -2.1666e-03,\n","                      -2.0110e-02,  6.1556e-03, -2.6149e-02, -2.4771e-02,  7.0373e-03,\n","                       2.5467e-03,  2.5423e-02,  2.2070e-02,  1.8719e-02, -1.3082e-02,\n","                      -2.3777e-02,  1.8119e-02,  2.4420e-02, -1.4970e-02, -2.1833e-02,\n","                      -2.1013e-02,  1.2188e-02,  1.0204e-02,  1.5027e-02,  1.4395e-02,\n","                      -3.0583e-04,  4.9814e-03, -2.5705e-02, -1.2772e-02,  1.5507e-02,\n","                      -1.1806e-02,  2.7168e-02, -1.7882e-02, -3.7198e-03,  2.7212e-02,\n","                      -1.2050e-02,  2.8191e-02, -5.0292e-03, -1.3676e-02,  2.2559e-02,\n","                      -1.1426e-02,  1.6325e-02,  1.1396e-02,  2.1490e-02, -2.8600e-02,\n","                      -2.3304e-02,  4.0319e-04,  2.1357e-02,  2.0066e-02,  4.2471e-04,\n","                       7.3717e-03,  1.5806e-02, -1.4439e-02,  2.1477e-02,  1.3545e-02,\n","                       2.7061e-02,  9.6862e-03, -1.4035e-04,  2.0888e-02,  2.6173e-02,\n","                       2.8196e-02,  3.0351e-02,  2.8966e-02, -5.4815e-03,  1.6386e-02,\n","                       1.1392e-02,  9.0313e-03, -9.3911e-03, -1.6070e-02, -8.3410e-03,\n","                      -2.0143e-02,  1.2698e-02, -1.5380e-02, -1.3600e-03,  2.4955e-02,\n","                      -2.5062e-03,  4.4554e-03,  5.1940e-03,  1.7304e-02,  7.4838e-03,\n","                       3.0609e-02,  1.3686e-02,  2.5235e-02,  2.7498e-02, -2.3848e-02,\n","                      -2.6365e-02, -9.2164e-03,  3.6968e-03,  2.0360e-02,  2.6142e-02,\n","                      -2.6489e-02, -2.3501e-02, -2.0733e-02,  1.4322e-02,  1.8723e-02,\n","                       1.8057e-02, -2.0269e-04, -7.7985e-03, -6.8210e-03, -8.6765e-03,\n","                      -1.7479e-02, -1.5825e-02, -2.0758e-02, -6.2904e-03,  2.8171e-02,\n","                       1.2840e-02,  1.6142e-02,  8.7077e-03,  1.3737e-02, -2.7599e-02,\n","                       7.7570e-03, -2.3424e-03,  1.7961e-02,  2.1100e-02, -2.9132e-02,\n","                      -9.1641e-03, -4.6429e-04,  1.9328e-02, -5.8567e-03,  1.7145e-02,\n","                       1.1594e-02,  1.9830e-02, -8.7106e-03, -4.9058e-03,  7.2174e-03,\n","                      -2.3243e-02,  1.8710e-02, -2.9909e-02, -1.9735e-02,  4.4919e-03,\n","                      -2.4949e-02,  6.7520e-03, -5.4462e-03,  1.0812e-02, -2.5527e-02,\n","                       2.0359e-02,  9.8201e-03,  1.6252e-02,  3.0132e-02,  1.7569e-02,\n","                       3.7368e-03,  1.7951e-02, -2.1616e-02, -1.5753e-02, -3.4129e-03,\n","                       2.7733e-02, -2.1340e-02,  2.0089e-02, -1.8928e-02, -9.1444e-03,\n","                       2.4896e-02, -2.9133e-02, -4.5818e-03,  8.0655e-04,  2.8824e-02,\n","                       1.7854e-03,  5.7461e-03, -2.8351e-02, -1.3928e-02, -1.9069e-02,\n","                      -5.9537e-03,  5.4951e-03, -1.3807e-02, -1.4921e-02,  1.2189e-02,\n","                      -2.9421e-02, -2.2622e-03,  2.1089e-02, -3.2132e-03,  6.6981e-04,\n","                       3.0470e-02,  2.0424e-02, -2.2386e-02, -1.9266e-02,  1.8036e-03,\n","                       2.9376e-02,  1.1907e-02,  1.1631e-02, -1.9040e-02,  2.3338e-02,\n","                       1.7255e-02, -5.7665e-03, -3.0157e-02, -8.4756e-03, -1.9913e-02,\n","                       4.7248e-03,  2.9505e-02,  1.2392e-02,  1.2619e-02,  9.9324e-04,\n","                       8.4964e-03, -2.1066e-02,  2.3203e-02, -1.2763e-02,  2.8290e-02,\n","                      -2.2962e-02, -1.1005e-02, -4.8085e-03, -1.2359e-02,  9.3037e-03,\n","                       1.6671e-02, -2.1065e-02,  1.8053e-03,  6.7603e-03,  1.8121e-02,\n","                      -1.7047e-02, -1.9071e-02, -1.9811e-02, -4.9251e-03, -8.3991e-03,\n","                       9.8080e-03,  1.4417e-02, -2.4814e-02, -1.5788e-02, -6.6450e-03,\n","                      -1.7429e-02, -5.3526e-03, -4.1696e-03,  2.2279e-02, -1.7666e-02,\n","                      -1.8137e-02,  2.3971e-02, -2.6518e-02,  1.6841e-02,  2.4171e-02,\n","                      -2.8384e-02,  2.7885e-02,  1.5797e-02,  1.6151e-02,  1.3915e-02,\n","                       9.4364e-03, -1.4742e-02,  1.5139e-02,  8.6212e-03, -1.7080e-02,\n","                       1.6474e-02,  6.7866e-03,  1.7641e-03,  2.3876e-02,  1.7385e-02,\n","                      -2.3367e-02, -2.5591e-02,  2.2754e-03, -9.1453e-03,  2.5230e-02,\n","                      -2.4571e-02,  3.4311e-03, -2.7501e-02, -2.7159e-03, -4.9582e-03,\n","                       2.8622e-02, -2.2442e-02, -1.0299e-02, -1.9388e-02,  1.1997e-02,\n","                       2.2982e-02, -1.6381e-02, -1.6291e-02, -2.7901e-02, -3.0227e-02,\n","                      -1.7475e-02, -6.9857e-03, -2.7963e-02,  3.1102e-02, -1.7960e-02,\n","                       1.6198e-02,  2.0956e-02, -1.7291e-02,  1.9954e-02, -2.3787e-03,\n","                       1.6371e-02, -7.7463e-03,  2.2116e-02,  2.7988e-02, -3.0251e-02,\n","                       3.1154e-04, -3.0946e-02,  1.0942e-02, -2.5864e-02,  3.0871e-02,\n","                      -2.5455e-02, -9.3307e-03,  5.9160e-03, -2.5413e-02,  1.8458e-03,\n","                       1.4520e-02,  3.0516e-02, -3.8888e-03, -4.8398e-03,  3.7130e-03,\n","                      -6.4019e-03, -1.8062e-03,  1.0420e-02, -1.9414e-02,  2.4523e-02,\n","                      -2.0636e-02,  7.7416e-03, -2.1490e-02,  2.0226e-02,  1.4723e-02,\n","                      -2.5129e-03, -2.5916e-02,  8.5032e-03, -1.6176e-02,  1.9241e-02,\n","                      -9.8826e-03,  3.1930e-03,  2.0144e-02,  2.3846e-02,  2.6777e-02,\n","                       2.2368e-02,  2.5314e-02, -2.6869e-02, -2.3249e-02, -1.7598e-02,\n","                       1.6045e-02,  2.5969e-02,  1.5081e-02,  1.6280e-02,  2.4816e-02,\n","                       5.8546e-03, -1.0465e-02, -9.8596e-03, -2.1541e-02, -9.0906e-03,\n","                       1.5550e-03, -1.3958e-02, -1.5145e-03, -9.6100e-03, -1.0982e-02,\n","                       1.1265e-02, -1.7102e-03,  2.6943e-02,  2.0786e-02,  2.9099e-02,\n","                       1.5668e-02, -8.9744e-04, -1.6763e-02, -2.3149e-02,  1.0448e-02,\n","                       5.3658e-03,  2.8002e-02,  5.0428e-03,  9.1985e-03,  1.0926e-03,\n","                      -1.7438e-02,  4.9666e-03,  1.5258e-02,  2.4666e-02, -1.7231e-02,\n","                      -2.6981e-02,  3.0942e-02, -3.1037e-02,  3.2152e-03, -1.8208e-02,\n","                      -1.3048e-02,  2.1359e-02,  2.7975e-02,  6.6246e-03,  1.2274e-02,\n","                      -1.1101e-02,  2.4020e-02,  3.0510e-02, -1.0346e-02,  1.5731e-02,\n","                       1.1543e-02,  1.0150e-02,  2.5212e-02,  5.2462e-03, -2.4538e-02,\n","                      -2.7300e-03,  2.2342e-02, -1.7025e-02, -2.5092e-03,  2.5083e-02,\n","                       1.0165e-02, -7.1711e-04, -1.4474e-02,  1.0835e-02, -1.0186e-02,\n","                      -1.8233e-02,  6.1294e-03,  2.3098e-02,  6.8467e-03, -2.0031e-02,\n","                       1.6656e-02,  5.0957e-03,  1.6729e-02,  1.0773e-02, -2.6676e-02,\n","                      -8.4951e-03,  1.9418e-02, -8.1770e-03, -1.7953e-02,  1.2180e-02,\n","                       2.5585e-02,  6.8539e-05, -2.8263e-02,  1.2602e-02, -1.1913e-02,\n","                       1.1381e-02, -8.3440e-04,  2.4970e-02, -1.2658e-02,  1.3976e-02,\n","                      -2.6902e-02,  2.0600e-02,  2.3907e-03,  2.6728e-02,  2.8934e-02,\n","                      -1.5750e-03,  2.4082e-03, -1.0758e-02, -2.8414e-02,  9.1114e-03,\n","                      -7.5560e-03,  1.0133e-02,  3.3372e-03, -4.9191e-03,  2.8657e-02,\n","                       2.0451e-03, -2.7791e-02, -2.9399e-02,  2.6248e-02,  1.4106e-02,\n","                       2.2687e-02,  8.5260e-03, -1.4710e-02,  9.1268e-03,  2.7484e-02,\n","                      -1.1952e-02,  4.5979e-03, -2.8713e-02, -2.7572e-02,  1.8289e-02,\n","                       2.4436e-03,  3.9085e-03, -2.5213e-02, -4.5124e-03, -3.0354e-02,\n","                      -2.0349e-02,  3.1034e-02, -1.7721e-02, -2.7033e-02, -2.3828e-02,\n","                       2.6950e-02, -1.8043e-02,  2.6325e-02,  1.1699e-02,  1.1075e-02,\n","                       1.0639e-02, -2.3289e-02, -3.0689e-02, -7.5176e-03, -1.0770e-02,\n","                      -4.5889e-03, -2.2681e-02, -4.1884e-04, -1.6219e-02,  1.2127e-02,\n","                      -1.7050e-02, -2.0915e-02,  1.9401e-03,  1.2725e-02, -2.4868e-02,\n","                       2.7575e-02, -2.3620e-02, -2.7079e-02,  2.0639e-02, -2.6293e-02,\n","                      -2.7623e-02,  1.5925e-02,  5.6355e-03,  1.7739e-02, -1.7722e-02,\n","                      -2.2271e-02, -1.3394e-02, -1.6777e-02, -2.9094e-02, -2.2609e-02,\n","                      -2.1176e-02,  3.7319e-03, -1.6244e-02,  2.6876e-02,  2.7909e-02,\n","                       8.9383e-03,  2.2238e-02,  2.5040e-02,  2.3776e-02, -2.7650e-02,\n","                      -2.8992e-02, -8.5356e-04,  2.0192e-03, -1.2189e-03, -1.1496e-02,\n","                      -3.0770e-02,  1.7174e-02, -1.6696e-02,  1.6598e-02,  2.1391e-02,\n","                       1.4405e-02,  2.5222e-02, -1.6730e-02, -1.3997e-02,  6.2678e-04,\n","                       2.7279e-02, -1.2332e-02, -1.2244e-02, -1.2204e-02, -7.4086e-03,\n","                      -2.7657e-02,  1.0243e-03, -9.3213e-03,  1.8415e-02, -1.7914e-02,\n","                       2.5075e-03,  1.9083e-02, -3.0198e-02,  2.3227e-02, -2.3547e-02,\n","                      -3.5874e-03,  2.9760e-02, -2.5408e-04, -2.9697e-02,  2.0330e-03,\n","                      -1.6996e-02, -1.6794e-02,  6.9877e-03,  9.2301e-03, -2.7075e-02,\n","                       1.5846e-02, -1.5068e-02], device='cuda:0')),\n","             ('_gnn._conv_layers.1.nn.2.weight',\n","              tensor([[ 3.0168e-02,  2.9344e-02,  7.9335e-03,  ..., -3.4585e-02,\n","                       -3.0998e-02,  2.8768e-02],\n","                      [-2.6225e-02, -1.3663e-02, -3.6242e-02,  ..., -8.8936e-03,\n","                        1.9063e-03, -9.6220e-04],\n","                      [ 1.8916e-02,  4.6752e-05, -7.5940e-03,  ...,  6.9770e-03,\n","                        2.8000e-02,  3.3195e-02],\n","                      ...,\n","                      [ 1.0207e-02, -1.1867e-02,  1.9199e-02,  ..., -2.0905e-02,\n","                        7.0653e-03, -2.4647e-02],\n","                      [-2.3784e-02, -3.8389e-02, -3.8430e-02,  ..., -2.4401e-02,\n","                        9.8254e-03, -1.4764e-03],\n","                      [ 3.3821e-02, -2.0079e-02,  3.4356e-02,  ..., -2.2615e-02,\n","                       -1.0006e-02, -2.5299e-02]], device='cuda:0')),\n","             ('_gnn._conv_layers.1.nn.2.bias',\n","              tensor([ 1.8251e-02,  5.8316e-03, -1.0456e-02,  2.5579e-02, -1.6572e-03,\n","                      -1.8802e-02, -1.8665e-02, -1.5500e-02,  1.5653e-02,  2.3643e-02,\n","                       4.5794e-03, -2.9914e-02, -3.1051e-02,  1.8774e-02, -1.8867e-02,\n","                      -1.0797e-02,  3.5755e-02, -2.0669e-02,  3.7166e-02, -2.1499e-02,\n","                       2.0315e-02, -1.5952e-02, -3.0659e-02,  3.0205e-03, -5.5456e-03,\n","                       3.8298e-02, -3.2890e-02, -5.1826e-03,  9.1824e-03,  1.1275e-02,\n","                       3.5988e-02,  3.5116e-02,  1.9945e-02,  9.6493e-03,  2.8911e-02,\n","                       3.4998e-02,  3.6676e-02, -1.3567e-02, -2.4692e-02,  2.4131e-03,\n","                       3.0367e-02,  3.5852e-02, -3.5110e-02, -2.4431e-02,  2.3601e-02,\n","                      -2.4509e-02, -3.0664e-03, -3.4771e-02,  3.1907e-02, -3.5402e-02,\n","                       9.7963e-03, -3.8068e-02, -1.1456e-02,  1.6426e-02, -2.7842e-02,\n","                       1.7803e-02, -1.2946e-02,  2.3042e-02, -1.1799e-02, -9.4767e-03,\n","                      -1.5641e-02, -2.5631e-03, -2.0521e-02, -2.1719e-03,  3.2290e-02,\n","                       2.5446e-02,  9.0755e-03,  2.5912e-02,  2.3566e-02, -1.3826e-02,\n","                       2.3298e-02,  5.1743e-03,  2.2356e-02,  3.7479e-02, -2.5467e-02,\n","                       1.2164e-02,  2.9187e-02,  2.9179e-03, -3.0868e-02, -2.5959e-02,\n","                      -2.8068e-02, -2.1570e-02, -3.2104e-02, -4.2405e-03,  3.6311e-02,\n","                      -6.8910e-03,  1.8876e-02,  3.1234e-02, -2.6318e-02, -2.4333e-02,\n","                      -3.5045e-02, -9.6137e-03, -7.0663e-03,  1.4722e-02,  2.8649e-02,\n","                      -1.6600e-02, -1.7950e-02, -1.7041e-02, -2.3694e-02,  1.3365e-02,\n","                       1.6152e-02, -9.2731e-03, -2.1665e-02, -1.9101e-02, -3.5638e-02,\n","                      -1.4364e-02, -3.5944e-02, -2.9095e-02, -2.6590e-02,  2.7418e-02,\n","                      -1.0423e-02,  1.6611e-02,  1.8913e-02, -2.2903e-02,  1.2907e-02,\n","                       1.9475e-02,  3.8314e-02, -2.9954e-02, -7.7039e-03,  3.5924e-02,\n","                      -2.7773e-02, -3.0727e-02, -3.6471e-02,  3.8012e-02, -1.5455e-02,\n","                      -8.4840e-04,  5.3943e-03, -2.9271e-02, -7.9915e-03, -1.2708e-04,\n","                       1.5138e-02,  3.8325e-02,  3.7266e-02,  2.2115e-02, -5.4544e-04,\n","                      -1.8306e-02,  2.4531e-02,  1.8514e-02,  3.2309e-02, -3.0904e-02,\n","                      -4.9341e-03, -3.3704e-02,  3.3421e-02, -6.1439e-03, -4.2359e-03,\n","                       2.7649e-02,  2.3510e-02,  1.3948e-02, -9.3601e-03,  2.6448e-02,\n","                      -2.6310e-02, -1.1916e-02,  6.2991e-03,  1.5816e-02, -1.1489e-02,\n","                       6.0039e-03,  1.0239e-02, -2.1427e-02, -3.7166e-02,  7.1211e-03,\n","                      -2.8330e-02, -1.8828e-02, -2.4830e-02,  3.6210e-02,  3.6105e-02,\n","                      -3.2777e-02,  1.8760e-02,  3.4657e-02, -2.1466e-02, -1.5572e-02,\n","                      -2.2323e-02, -7.0640e-03, -6.6156e-04,  7.0246e-03, -1.9704e-02,\n","                       3.7919e-02, -1.3872e-02,  8.8737e-03,  5.2999e-03,  3.3923e-02,\n","                      -3.1917e-02, -3.7160e-02,  1.2696e-02,  2.3329e-02, -8.4125e-03,\n","                      -1.2153e-02, -2.8552e-02, -7.2535e-03,  1.4787e-02,  2.1344e-02,\n","                       2.3000e-02, -1.3864e-02, -2.4541e-02, -1.9808e-02,  2.1797e-02,\n","                       1.5218e-02, -3.6359e-02, -3.2168e-02, -1.7951e-02,  1.5843e-03,\n","                       8.8068e-03,  1.8595e-02,  4.1360e-03,  3.2697e-02, -5.4101e-03,\n","                      -2.0929e-02,  3.8083e-02, -2.6022e-02, -5.6371e-03,  1.1345e-03,\n","                       3.2036e-02, -4.5160e-03,  3.0367e-02,  3.5844e-02, -1.9230e-02,\n","                       3.6679e-02,  2.1576e-02, -1.3501e-02, -1.9523e-02,  2.4607e-02,\n","                       1.7610e-02, -8.2641e-03, -3.5597e-02,  3.2063e-02,  1.9238e-02,\n","                       1.4489e-02, -2.0655e-03, -3.6192e-02, -6.6404e-03, -2.0750e-02,\n","                       3.4029e-02, -1.1454e-03,  7.8264e-03,  2.2729e-03, -1.0594e-02,\n","                      -2.2164e-02,  2.6766e-02, -2.3724e-02, -3.2475e-02,  1.4732e-03,\n","                      -3.1108e-02, -7.2853e-05,  3.2968e-02, -1.6555e-02, -2.4026e-04,\n","                      -1.7217e-03,  2.4004e-02, -1.5566e-02,  2.0485e-02,  3.6963e-02,\n","                      -5.7523e-03,  3.6643e-02,  9.4783e-03,  1.9096e-02,  2.7334e-02,\n","                       1.4670e-02, -2.4368e-02, -1.2218e-02,  1.3062e-02, -3.7832e-02,\n","                       2.1555e-02,  2.0538e-02, -2.2154e-02,  2.8741e-02,  2.7349e-02,\n","                       3.3050e-02,  2.2198e-02, -1.3194e-02,  2.9711e-02, -2.6545e-02,\n","                      -4.3682e-03, -3.6136e-02,  1.9441e-02,  3.3739e-02, -1.2968e-02,\n","                       1.3150e-02, -2.1339e-02,  2.8577e-03, -3.8081e-02, -1.6803e-02,\n","                       1.7467e-02, -1.0015e-03, -2.0418e-03, -3.4584e-02,  1.4846e-02,\n","                       3.6719e-02, -5.0893e-03, -1.3436e-02, -3.7370e-02, -2.5816e-02,\n","                      -2.3127e-03, -2.4291e-02, -1.4413e-02,  9.6761e-03,  3.1402e-03,\n","                       3.0419e-03,  2.4957e-02, -1.9620e-02, -1.2209e-02,  2.0597e-02,\n","                       2.5135e-02, -2.1580e-02, -6.3232e-03, -1.6599e-02,  5.1881e-03,\n","                       1.8344e-02,  4.4374e-03,  1.6013e-05,  6.9752e-03,  2.8192e-03,\n","                       5.3425e-03,  2.7004e-02, -2.6368e-02,  2.9467e-02,  1.9083e-02,\n","                       2.3664e-02,  5.0344e-03,  4.8989e-03,  3.3323e-02, -1.1691e-04,\n","                       3.6566e-02,  3.7023e-03, -1.8169e-02, -2.7410e-02,  8.5984e-03,\n","                       3.5154e-02,  3.2505e-02, -1.0703e-02,  2.4915e-02, -1.9193e-02,\n","                      -4.9553e-03, -9.7252e-03,  1.5764e-02, -3.6603e-02,  2.9734e-02,\n","                       8.1379e-03, -1.4825e-02,  3.4537e-02,  1.2942e-02, -1.2013e-02,\n","                       2.0845e-04,  2.0661e-02, -2.4051e-02, -8.2367e-03,  3.0179e-02,\n","                       1.5136e-02,  2.2544e-02,  3.3572e-04, -1.9149e-04,  3.5889e-02,\n","                       7.1397e-03,  3.1996e-03,  2.9421e-02, -1.7514e-02,  1.9837e-02,\n","                      -2.8086e-02, -1.8473e-02, -2.6502e-02, -6.9138e-03, -3.3356e-02,\n","                      -3.7698e-02,  2.4318e-02, -1.4442e-02, -6.4709e-03,  3.5896e-02,\n","                      -7.6320e-03,  1.6390e-02, -3.0913e-02, -7.2024e-04, -9.7304e-04,\n","                       1.9805e-02,  1.3538e-02,  2.6742e-02, -9.1994e-03,  1.7422e-02,\n","                       3.2046e-02,  9.1147e-03,  5.4556e-03,  2.2677e-02,  2.9673e-02,\n","                       2.4935e-02,  2.9051e-02, -4.9772e-03, -1.8666e-02,  8.2997e-03,\n","                      -2.8030e-02, -6.7090e-03, -2.4323e-02,  5.5404e-03,  2.1200e-02,\n","                      -3.8036e-02,  2.2107e-02,  3.4510e-02, -1.4373e-02, -3.2977e-02,\n","                      -9.8796e-03,  8.8322e-03, -1.0322e-03,  2.4033e-02, -4.5349e-03,\n","                       3.7661e-02, -1.0866e-02, -1.4666e-02, -3.6185e-02, -4.8000e-03,\n","                       3.6627e-02, -3.4413e-02,  3.5174e-02,  2.2790e-02, -9.3658e-03,\n","                      -2.2527e-03,  1.1056e-02,  1.5196e-02,  5.2137e-03,  5.5209e-03,\n","                       2.5631e-02,  2.1864e-02, -2.4694e-02, -3.2246e-02,  3.0121e-02,\n","                      -3.4539e-02,  1.3910e-02,  2.7032e-02, -1.5722e-03,  3.0103e-02,\n","                       3.9746e-03, -2.1751e-02,  1.9121e-02,  3.2373e-02, -2.1103e-02,\n","                       3.1112e-02, -1.4075e-02,  1.3079e-03, -2.5966e-02, -2.7607e-02,\n","                       3.3241e-02,  2.7664e-02, -2.6824e-02, -1.6025e-02, -3.2050e-02,\n","                      -3.1652e-02,  1.5696e-03, -2.8650e-02,  2.0237e-02, -1.2808e-02,\n","                      -3.5054e-02, -3.6186e-02, -2.6223e-02, -3.2082e-03, -2.8741e-02,\n","                      -1.7298e-02, -2.9471e-02, -1.4053e-02,  1.9790e-02,  2.9193e-02,\n","                       2.4690e-02,  4.9194e-03, -9.2279e-03,  2.6399e-02,  1.2060e-02,\n","                      -1.6604e-02,  3.6677e-03,  1.2869e-02, -2.4398e-02, -1.8957e-03,\n","                       3.7912e-02,  3.1819e-02,  1.2046e-03, -2.3867e-02, -2.1962e-02,\n","                      -3.6248e-02,  1.4918e-02, -9.6877e-03,  2.4884e-02,  2.6591e-02,\n","                       3.7593e-02,  4.9355e-03, -1.0089e-02,  1.3757e-02,  3.0887e-02,\n","                      -2.7415e-02, -2.7095e-02,  2.5956e-02,  3.2983e-03, -3.1512e-02,\n","                       3.2109e-02, -7.7431e-03, -3.7539e-03,  3.4906e-02, -2.7383e-02,\n","                       2.7868e-02,  2.6050e-02,  2.0692e-03, -2.0393e-02, -2.0021e-04,\n","                      -3.5714e-02,  3.6706e-02, -2.8138e-02,  6.1639e-03, -5.2016e-04,\n","                      -3.2205e-02, -1.9149e-02, -5.6931e-03, -2.1143e-02, -2.1034e-02,\n","                       2.2678e-02, -3.7379e-02, -3.8109e-02, -2.7861e-02, -2.3694e-02,\n","                       4.9237e-04, -1.2054e-02], device='cuda:0')),\n","             ('_gnn._conv_layers.2.nn.0.weight',\n","              tensor([[-9.3708e-03, -1.2815e-02, -6.1631e-04,  ...,  2.8310e-02,\n","                       -2.0021e-02, -1.8097e-02],\n","                      [ 2.2310e-02, -1.4499e-02,  1.3808e-02,  ...,  2.7364e-02,\n","                        3.2471e-05,  3.5430e-03],\n","                      [ 1.1062e-02,  4.0015e-03,  8.2742e-04,  ...,  2.3253e-02,\n","                        2.7544e-02,  1.8918e-02],\n","                      ...,\n","                      [ 2.0831e-02, -2.3077e-02,  1.3918e-03,  ...,  6.8792e-03,\n","                       -2.3189e-02, -1.2476e-02],\n","                      [ 2.9766e-02,  1.2956e-02,  1.5774e-02,  ..., -1.9920e-02,\n","                       -6.4828e-03, -1.6551e-02],\n","                      [ 8.2150e-04,  3.7511e-03, -2.5429e-02,  ..., -9.2536e-03,\n","                        2.1099e-02, -2.7097e-02]], device='cuda:0')),\n","             ('_gnn._conv_layers.2.nn.0.bias',\n","              tensor([ 3.0742e-04,  1.6478e-02,  3.0255e-03,  2.7019e-02, -1.6015e-02,\n","                      -2.5989e-02,  2.7526e-02,  8.4079e-03,  1.0604e-02, -6.3376e-03,\n","                       3.7969e-03,  2.9656e-02,  2.4228e-02, -5.2137e-03, -1.0530e-02,\n","                       3.7008e-03,  1.5994e-02, -1.5627e-02, -1.2889e-02,  6.9827e-03,\n","                      -6.0767e-03, -2.4189e-02,  5.5306e-03, -2.0631e-02, -2.4516e-02,\n","                       1.1622e-02, -1.7978e-02, -2.2906e-02, -3.0899e-02, -1.7110e-02,\n","                       3.5117e-03, -1.1471e-02, -3.2655e-03,  1.5358e-02,  2.4525e-02,\n","                      -2.6250e-02, -1.6404e-02, -1.3929e-02,  7.1122e-03,  3.1037e-02,\n","                       6.8427e-03,  5.8381e-03,  1.2535e-02, -8.7580e-03,  5.0760e-03,\n","                       1.1577e-02,  1.6033e-02,  2.3850e-02, -1.8043e-03, -1.7210e-02,\n","                      -2.7147e-02, -8.0231e-03, -4.0639e-03,  8.0393e-03,  1.9207e-02,\n","                      -2.2911e-02,  2.4691e-02,  1.6686e-02, -1.5085e-02,  1.1992e-02,\n","                       1.1673e-02, -1.5002e-03, -3.0503e-02,  2.2348e-02,  2.9664e-02,\n","                      -2.0604e-03, -2.2877e-02,  8.2437e-03, -1.1951e-02, -2.7765e-02,\n","                       1.3766e-02,  1.7358e-02,  5.9228e-03, -2.9598e-02,  6.6599e-03,\n","                       1.9708e-02,  2.1154e-02, -5.5114e-03, -2.0120e-02, -2.2103e-02,\n","                      -1.7885e-02, -1.9883e-02, -2.4769e-03, -2.8953e-02,  1.9830e-02,\n","                      -1.1501e-02,  2.0876e-02,  7.1503e-03,  5.4874e-03, -6.9094e-04,\n","                      -2.7430e-03,  1.7555e-02, -1.9462e-02, -2.0416e-02,  9.0679e-04,\n","                      -1.7389e-02,  1.7233e-02,  1.5504e-02, -9.2362e-03, -1.6058e-02,\n","                      -2.4790e-02, -2.1664e-02, -2.5672e-02,  1.1086e-02,  5.8604e-03,\n","                       7.2185e-03,  1.4195e-02,  2.8229e-02,  2.6597e-02,  1.6713e-02,\n","                      -2.1539e-02, -5.4232e-03, -1.5788e-02, -1.1650e-02, -1.0690e-02,\n","                       4.5025e-03, -9.0178e-03, -1.6113e-02, -2.7515e-02,  5.9705e-03,\n","                      -7.5069e-03, -4.1843e-03,  3.9594e-03, -2.9956e-02,  1.2516e-02,\n","                       7.8435e-03,  2.8515e-02, -1.6696e-02,  1.0235e-02, -8.2877e-03,\n","                      -1.1494e-02, -2.6282e-02, -2.6585e-02,  1.9185e-03, -2.4606e-02,\n","                       6.4481e-03, -1.8243e-02,  6.5960e-03, -1.4001e-02,  2.1811e-02,\n","                      -1.7323e-02, -3.3513e-03, -1.8702e-02,  2.6608e-02, -2.9221e-02,\n","                       2.4417e-02, -2.0258e-03,  2.3529e-02, -9.1727e-03,  2.3495e-02,\n","                       5.7957e-03, -2.4804e-02,  2.0512e-02, -2.1317e-02, -2.3677e-02,\n","                      -1.3075e-02, -1.5773e-02,  2.0195e-02, -9.0892e-03, -2.1394e-02,\n","                       1.2616e-02, -1.5791e-02,  2.5127e-02,  1.8583e-02,  2.4610e-02,\n","                      -9.6177e-03, -2.8557e-02, -2.6275e-02, -9.0677e-03,  2.0355e-02,\n","                      -8.8554e-03, -1.1981e-02, -5.2884e-03,  1.4783e-02, -1.5079e-03,\n","                      -5.4821e-03,  2.3117e-02,  8.5242e-03,  1.5555e-02,  5.6380e-03,\n","                       9.4337e-03, -3.0082e-03,  2.2064e-02, -2.1301e-02,  8.7855e-03,\n","                       1.2655e-02,  2.4476e-02,  4.3063e-04, -2.7828e-02,  2.3136e-02,\n","                      -2.3140e-02, -2.8655e-02, -1.1158e-03,  9.1977e-03,  1.7520e-02,\n","                       8.1339e-03, -2.5232e-02,  8.6035e-03, -3.0573e-02, -2.8423e-02,\n","                      -9.1177e-03, -1.5671e-02,  6.7998e-04, -2.0947e-02,  1.4591e-02,\n","                       1.6671e-02,  8.4559e-03,  7.0236e-03, -3.0468e-02, -1.4983e-02,\n","                       1.1068e-02,  3.6795e-03,  2.6368e-02, -1.9724e-03,  2.0487e-02,\n","                       2.5084e-02, -9.3360e-03, -2.3852e-03, -1.2095e-02,  1.6496e-02,\n","                       2.6731e-02, -2.8154e-02,  2.7031e-03,  2.5142e-02,  1.7442e-03,\n","                      -2.2227e-02,  2.9417e-02,  2.9513e-03, -2.0246e-02, -7.2477e-03,\n","                      -2.4868e-02, -9.9219e-03,  2.6615e-02, -2.7719e-04, -4.1401e-03,\n","                      -1.5235e-02, -5.6097e-03,  2.4076e-02, -2.0378e-02,  7.0643e-03,\n","                       1.1294e-03, -1.8079e-02,  1.6573e-02, -2.6924e-02, -1.1060e-02,\n","                       2.1599e-02,  1.4701e-02, -2.8087e-02,  2.4838e-02,  3.0032e-03,\n","                       8.3255e-03,  2.3950e-02, -1.6328e-02,  5.1338e-03,  2.7344e-02,\n","                       2.4821e-02, -2.9666e-02, -2.9972e-03, -1.8485e-03,  8.8896e-03,\n","                       1.3796e-03, -1.5801e-02,  2.0294e-02,  2.5410e-02,  2.3393e-02,\n","                      -4.8973e-03,  2.8807e-02, -2.9469e-02, -2.0611e-03,  7.0150e-03,\n","                       2.5192e-02, -2.7137e-02, -2.0711e-02, -1.4046e-02,  2.0567e-02,\n","                       2.7949e-02, -1.8312e-02,  1.2219e-02, -1.9620e-02,  2.0324e-02,\n","                      -1.4275e-02,  2.7887e-03, -2.5888e-02, -9.3858e-03, -2.5045e-02,\n","                      -2.2513e-02,  1.4974e-02,  1.6141e-02, -1.6472e-02,  2.4334e-02,\n","                      -1.8617e-03, -1.5965e-02, -3.0509e-02, -1.2725e-03, -1.2326e-02,\n","                      -2.4168e-05,  2.2493e-02, -1.4055e-02, -3.6187e-03,  1.6427e-03,\n","                       7.5331e-03,  2.9681e-02, -9.5672e-03, -1.3007e-02, -2.0561e-02,\n","                       1.6939e-02, -1.9908e-02,  2.9368e-02,  2.5446e-02,  5.5841e-03,\n","                       1.1999e-02,  2.2455e-02, -1.3160e-02, -1.0904e-02, -8.4620e-03,\n","                       9.1536e-03, -1.9164e-02,  4.6623e-03, -1.7890e-02,  2.4931e-02,\n","                      -2.6217e-02, -1.2985e-02,  1.6426e-02,  1.9254e-02,  2.7591e-02,\n","                      -5.0017e-03,  1.4166e-02,  1.0718e-03, -1.9228e-02,  2.7000e-02,\n","                       1.7592e-02, -1.8546e-02, -4.8870e-03, -9.3482e-03, -9.6821e-03,\n","                       6.6453e-03, -2.5363e-02, -2.2832e-02, -1.3044e-03, -2.1526e-02,\n","                       7.5346e-04,  1.8491e-02,  3.0544e-02,  2.5569e-02, -4.7200e-03,\n","                      -1.6760e-02, -9.8305e-03,  1.4902e-02, -3.0113e-02,  2.9036e-02,\n","                       1.7558e-02, -2.9439e-02, -2.7476e-02,  1.0162e-02, -1.2215e-02,\n","                       2.1765e-02, -7.6355e-03, -6.6521e-03, -2.7756e-02, -7.2882e-03,\n","                       2.1385e-02, -8.9831e-03, -2.7157e-02,  1.0150e-02, -1.7158e-02,\n","                       8.4996e-03, -2.0887e-03, -1.9255e-02,  2.8775e-02,  5.0136e-03,\n","                       1.6056e-02,  7.9323e-03, -1.2828e-02,  1.3079e-02,  3.5455e-03,\n","                      -1.6866e-02,  1.0591e-02, -7.6261e-03, -3.0898e-02,  1.6784e-02,\n","                       3.0314e-02, -2.6370e-03, -2.5916e-03, -1.6539e-02,  2.0635e-02,\n","                       2.5020e-02, -2.5994e-02, -5.2063e-03, -2.1349e-02, -1.9891e-02,\n","                      -2.1506e-02, -2.5233e-02,  1.8708e-02, -7.1031e-03,  5.0550e-03,\n","                       1.0917e-02, -2.4732e-02, -1.8794e-02, -1.7454e-02, -1.4520e-02,\n","                      -9.7592e-03, -2.2602e-02, -2.1601e-02, -2.1601e-02, -2.3482e-02,\n","                      -2.8274e-02, -3.2922e-03, -7.4991e-03,  2.4017e-02,  3.4657e-03,\n","                      -3.6441e-03, -5.4279e-03, -2.7267e-02,  3.6619e-03, -2.0084e-02,\n","                      -2.5105e-02, -2.5844e-02, -2.4405e-02,  2.6021e-02,  9.8110e-04,\n","                       1.8822e-02,  3.9183e-03,  3.7042e-03, -5.8146e-03,  8.9125e-03,\n","                       2.0029e-02, -2.5050e-02,  1.6967e-02, -8.6217e-03, -2.2215e-02,\n","                      -9.4302e-03,  1.8980e-02, -1.3764e-02, -1.3732e-03,  3.1265e-02,\n","                      -5.3726e-03,  1.3298e-02,  1.3359e-02, -1.4809e-02, -2.2299e-02,\n","                      -5.0577e-03,  7.1104e-03,  1.9972e-02, -2.4125e-02, -4.0625e-03,\n","                       1.5896e-02, -1.1936e-02,  2.2538e-02, -2.0214e-02, -7.5443e-03,\n","                      -6.8026e-03, -2.0532e-02, -3.0481e-02, -3.7308e-03, -1.7650e-02,\n","                       1.9009e-02,  2.4418e-02,  5.1748e-03, -1.9661e-02, -9.9951e-04,\n","                      -1.0912e-02, -2.9746e-02,  2.8034e-02,  2.4485e-02, -2.1025e-02,\n","                      -1.1971e-02, -1.7140e-02,  1.4052e-02, -1.4265e-02, -1.3297e-02,\n","                      -1.7085e-02, -1.8061e-02, -2.6133e-03, -1.6117e-02,  1.3888e-02,\n","                       1.5739e-02, -6.3342e-03,  2.5588e-02, -1.1063e-02, -2.8478e-02,\n","                      -9.4677e-03, -2.1236e-02,  2.9305e-02,  1.9177e-02, -6.0059e-03,\n","                       8.7736e-03, -9.1755e-03, -1.3590e-02,  7.0450e-03,  3.1052e-02,\n","                      -1.9474e-03,  2.6626e-02,  2.9742e-02, -5.5505e-03,  6.1069e-03,\n","                      -1.5084e-02, -1.0745e-03,  1.5993e-02,  6.3256e-03, -1.9684e-02,\n","                      -5.4586e-03,  2.1912e-02, -3.2676e-04,  2.0729e-02, -1.5228e-02,\n","                      -4.2621e-04,  1.1190e-02, -7.6457e-03, -1.6294e-02,  1.8883e-02,\n","                      -1.6357e-02, -2.6509e-02,  4.4890e-03, -1.9410e-02,  1.0034e-02,\n","                       1.0623e-02, -1.1043e-02,  2.5842e-03, -7.8161e-03,  1.0160e-02,\n","                      -1.0020e-02,  1.2407e-02,  1.6622e-02,  1.4057e-02, -1.5212e-02,\n","                      -5.8195e-03, -3.0217e-02,  2.6022e-02,  1.1818e-02, -1.7791e-03,\n","                      -1.2474e-02,  2.0353e-02,  1.9044e-02, -1.0272e-03, -2.7628e-02,\n","                       1.5240e-02, -3.0587e-02, -1.7224e-02,  8.4559e-04,  1.7671e-02,\n","                       9.5121e-03,  1.2636e-04, -1.8639e-02, -2.8185e-02,  9.3085e-03,\n","                      -1.6313e-03,  5.9184e-03, -6.0779e-03, -2.0891e-02,  2.8623e-02,\n","                      -2.0067e-02,  9.2750e-03,  1.8616e-02,  3.1128e-02, -2.4196e-02,\n","                      -3.0863e-02, -4.4879e-03, -8.3568e-03,  9.5322e-03,  2.9171e-03,\n","                      -1.7841e-03,  2.1629e-02,  2.8353e-02, -5.0287e-04, -1.5076e-03,\n","                      -1.2796e-02, -3.0233e-02, -1.7175e-02, -1.1444e-02, -1.9754e-02,\n","                      -1.5988e-03,  1.9102e-02, -3.8498e-03,  1.2745e-02,  1.2113e-02,\n","                      -1.7672e-02, -6.6768e-03, -2.2542e-02,  7.1075e-03,  1.1597e-02,\n","                       2.7050e-02,  2.1700e-02,  2.9348e-02, -3.2799e-03, -9.6934e-03,\n","                       1.8732e-02, -3.0629e-02, -2.3906e-02, -2.8408e-02,  2.8799e-02,\n","                      -2.5351e-02, -2.4935e-02, -2.0196e-02,  1.8012e-02,  1.4168e-02,\n","                      -1.5797e-02, -2.9192e-02, -8.4574e-03, -2.4220e-02,  3.5542e-03,\n","                       1.1203e-03,  2.8526e-02,  2.0922e-02, -3.7589e-03, -2.6697e-02,\n","                      -3.9637e-03, -1.9891e-02,  2.0454e-02,  6.6839e-03,  1.8763e-02,\n","                      -2.5474e-02,  2.6001e-02,  1.8858e-02, -1.1362e-02,  2.1028e-02,\n","                      -1.9234e-02, -3.0496e-02,  2.6858e-02, -3.1209e-02,  1.8979e-02,\n","                      -3.0442e-02, -2.0300e-03, -2.0267e-02, -5.9804e-03,  4.5035e-03,\n","                      -1.6840e-03, -1.4542e-02, -2.6023e-02,  1.8121e-02,  2.5712e-02,\n","                      -1.2036e-02,  3.4954e-04,  1.7635e-03,  2.9303e-02,  2.3268e-02,\n","                       2.6035e-03, -1.9461e-02,  1.8305e-02, -2.9720e-02, -4.7574e-03,\n","                      -2.4095e-02,  2.6781e-02, -1.8338e-02, -2.8583e-02,  2.3408e-02,\n","                      -1.2291e-02,  2.4974e-03,  5.6944e-03, -9.1963e-03,  8.7375e-03,\n","                      -1.1051e-02,  5.7856e-03,  2.3515e-03,  1.5553e-02, -7.7958e-03,\n","                       1.6357e-02,  1.8271e-02, -1.2667e-02, -1.1884e-02,  1.9084e-02,\n","                       1.8371e-02,  1.0577e-02,  2.3874e-02, -1.5744e-02, -2.5245e-02,\n","                       1.8627e-02, -1.2301e-02,  1.1759e-02,  2.1735e-02,  1.1450e-02,\n","                       2.4459e-02, -1.3467e-02], device='cuda:0')),\n","             ('_gnn._conv_layers.2.nn.2.weight',\n","              tensor([[-0.0307, -0.0341,  0.0270,  ...,  0.0030,  0.0106,  0.0194],\n","                      [-0.0316,  0.0242, -0.0247,  ..., -0.0138,  0.0159,  0.0063],\n","                      [-0.0354, -0.0208,  0.0016,  ...,  0.0099,  0.0308,  0.0087],\n","                      ...,\n","                      [-0.0217, -0.0285,  0.0238,  ..., -0.0309,  0.0005, -0.0372],\n","                      [ 0.0136,  0.0357, -0.0243,  ..., -0.0255, -0.0140,  0.0083],\n","                      [-0.0073,  0.0067, -0.0003,  ..., -0.0313,  0.0045,  0.0246]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.2.nn.2.bias',\n","              tensor([-1.6634e-02,  1.3949e-02, -2.5783e-02,  2.6730e-02,  1.5671e-02,\n","                      -1.3538e-02,  1.4043e-02,  1.0922e-02, -9.8364e-03,  3.8195e-02,\n","                       8.8534e-03, -3.0714e-02,  2.9108e-03,  3.6949e-03,  2.5121e-02,\n","                       3.4556e-02,  3.7600e-02, -1.6454e-02,  1.1629e-02, -1.0128e-02,\n","                      -5.3415e-03,  2.4491e-02, -1.3055e-02, -3.0661e-02,  1.3734e-02,\n","                      -5.6134e-03, -1.7235e-02, -3.2120e-02, -2.5054e-02,  2.4144e-02,\n","                       1.6203e-02,  1.5217e-02,  2.2334e-02, -2.6714e-02,  8.4863e-03,\n","                       5.1587e-03, -2.6444e-02, -2.0770e-02,  2.7909e-02,  1.3371e-02,\n","                      -2.9982e-02, -2.1126e-02, -2.3372e-02, -3.5781e-02, -9.6351e-04,\n","                      -3.0202e-02, -3.1505e-02,  3.0660e-02, -1.4278e-02,  2.2712e-03,\n","                      -1.1488e-02, -6.8144e-03,  2.7251e-02,  1.9990e-02,  2.5735e-02,\n","                       1.5578e-02, -3.7659e-02, -2.2008e-02,  2.3599e-02, -3.4339e-02,\n","                       9.8158e-03, -3.5683e-02,  3.1865e-02,  1.3616e-02,  1.0013e-02,\n","                      -1.1767e-02, -1.8241e-02,  1.4429e-02,  1.0038e-02, -3.0058e-02,\n","                       1.3347e-02, -1.6120e-02, -1.7764e-02,  5.8834e-03,  2.9027e-02,\n","                      -5.2985e-03, -1.3143e-02,  1.0362e-02,  9.5888e-04,  2.7221e-02,\n","                      -7.7390e-03, -9.3212e-03, -1.8690e-02,  2.4737e-02, -1.2217e-02,\n","                      -5.3330e-03, -3.7344e-02, -3.8139e-02, -1.9522e-02, -1.5761e-02,\n","                       3.5296e-02, -6.8149e-03, -6.8308e-03, -1.7669e-02,  1.1352e-02,\n","                      -2.1197e-02, -2.8831e-02, -2.1699e-02,  1.2500e-02, -3.6799e-02,\n","                      -3.4854e-02,  1.6945e-02, -3.5122e-02,  3.7504e-02,  6.5555e-04,\n","                      -3.7468e-02,  2.1306e-02,  1.4124e-02,  2.6883e-02,  2.5410e-02,\n","                       2.5735e-02,  3.2308e-02,  1.8984e-02, -1.2710e-02, -3.0158e-02,\n","                      -2.5445e-02,  1.4729e-02, -9.0809e-03, -1.1860e-02,  1.7891e-02,\n","                       3.4616e-02,  3.4617e-02,  2.1118e-02, -1.1175e-03,  2.5695e-03,\n","                       2.9176e-02, -3.7809e-02,  3.3443e-02, -2.1794e-02,  4.2738e-03,\n","                      -1.8378e-02,  3.4886e-02,  3.6865e-03, -9.7875e-03,  3.1656e-02,\n","                       3.7676e-03,  3.2438e-02,  3.1576e-02,  3.7966e-02,  2.9282e-03,\n","                       3.8185e-02, -2.4986e-02,  9.0829e-03, -3.5647e-02,  3.6366e-02,\n","                      -2.7692e-02,  9.5186e-03,  1.4225e-02,  2.2638e-02,  3.6946e-02,\n","                       2.4809e-02,  1.9922e-02, -1.6592e-02, -3.5150e-02, -2.9253e-02,\n","                       2.5619e-02,  1.8802e-02, -1.4453e-03, -2.0536e-02,  7.8411e-03,\n","                       1.4716e-02, -1.1782e-02, -1.5904e-02, -2.9082e-02,  2.0340e-02,\n","                       3.5343e-02, -3.5362e-02, -2.2790e-02, -1.5862e-02, -2.7176e-02,\n","                       2.4454e-02,  1.3148e-02, -1.6340e-02, -1.0927e-02, -3.0405e-02,\n","                       3.2808e-02,  3.6655e-02,  2.0575e-02,  2.0869e-03,  3.2668e-02,\n","                      -2.8397e-02, -3.3082e-02,  3.3754e-02,  1.1697e-02,  3.5316e-02,\n","                      -1.8668e-02,  2.5129e-02, -3.0603e-02,  8.6350e-03,  3.3862e-02,\n","                      -1.2988e-02, -3.3051e-02, -3.6088e-02, -8.1201e-03,  2.2563e-02,\n","                      -3.5940e-02,  2.3003e-02, -2.9722e-03,  3.5660e-02, -2.8593e-02,\n","                      -2.3721e-02,  4.6484e-03, -2.3872e-02,  4.4440e-03,  3.7196e-02,\n","                      -1.3832e-02, -2.0305e-02, -1.1643e-03, -3.2231e-02,  9.3285e-03,\n","                       1.9915e-02,  1.4847e-03,  1.4634e-02,  7.2451e-03, -1.2770e-02,\n","                      -2.5841e-02,  1.2437e-02,  3.0829e-05, -1.4433e-02, -9.1374e-03,\n","                       1.2755e-02, -1.6031e-02,  3.0147e-02,  1.6458e-02, -7.5715e-03,\n","                       2.7139e-02, -3.3467e-03, -8.3414e-03, -2.5165e-02, -2.1719e-02,\n","                       1.4092e-02,  1.7297e-02,  2.8934e-02, -9.0841e-03,  1.8619e-02,\n","                      -1.9966e-02, -3.5936e-03,  9.6353e-03, -2.2286e-02, -2.3593e-02,\n","                       1.7132e-02,  3.7431e-02, -8.7393e-03,  2.6124e-02,  3.0184e-02,\n","                       2.9931e-02,  6.8675e-03,  1.5233e-02, -1.5474e-02, -2.7051e-02,\n","                      -7.9698e-03, -3.0363e-02,  1.3189e-02,  1.5402e-02, -2.7447e-02,\n","                       1.9434e-02, -3.3848e-02, -5.2586e-03, -3.7687e-02,  2.6929e-02,\n","                       1.0516e-02, -3.5973e-02,  1.6624e-02, -1.4921e-02, -2.8318e-02,\n","                      -2.2077e-02, -5.9040e-03,  4.5953e-03,  2.7813e-02, -2.6846e-02,\n","                       3.8297e-02,  1.2168e-03,  2.4514e-02, -2.9652e-02,  3.4857e-02,\n","                      -2.6013e-02,  4.5664e-03, -8.5917e-03, -1.8817e-02,  2.9539e-02,\n","                      -3.7238e-02,  1.9114e-02, -2.1816e-02,  3.0575e-02, -1.9903e-02,\n","                       2.9839e-02, -2.4674e-02, -6.8380e-04, -3.7346e-02, -2.4827e-02,\n","                      -1.6875e-02,  1.7074e-02, -3.5435e-02, -3.1725e-02,  2.1984e-02,\n","                       1.1999e-02,  1.8843e-02,  1.9938e-02,  2.7743e-02, -2.7359e-02,\n","                       3.6138e-03, -3.4675e-02,  1.7884e-02, -2.4937e-02,  5.7325e-03,\n","                      -2.7200e-02, -4.0239e-03,  1.4393e-02, -1.4204e-03,  4.1817e-03,\n","                      -2.3038e-02, -1.7669e-02, -3.4731e-02,  1.8093e-02, -1.2377e-02,\n","                      -3.1577e-02, -1.1858e-02, -2.9992e-02,  3.4360e-02,  3.3115e-03,\n","                      -3.5706e-02,  4.3073e-03, -2.4803e-02, -1.4285e-02,  1.2905e-02,\n","                       3.8118e-02,  2.3566e-02, -3.4506e-02, -1.8975e-02,  7.2435e-03,\n","                      -1.8358e-02,  7.0936e-03, -5.0539e-04,  3.5679e-02, -1.7527e-02,\n","                      -4.6431e-03, -1.9672e-02, -3.5944e-02,  7.0634e-03,  2.5295e-02,\n","                      -2.6338e-02,  3.0418e-02, -2.8641e-02, -3.0313e-02, -4.8837e-03,\n","                      -1.9008e-02, -3.0898e-02,  2.6838e-02,  1.5060e-02,  2.3018e-02,\n","                       1.1551e-02,  2.4871e-02, -2.3542e-02,  6.0659e-03,  2.2800e-02,\n","                      -1.3771e-03, -3.7977e-02, -8.5745e-03, -2.6927e-02, -2.6879e-02,\n","                       1.0092e-02,  3.1083e-02, -7.0821e-03, -9.1732e-03, -2.7486e-02,\n","                      -4.8184e-03,  8.4885e-03,  3.4933e-02, -3.8450e-02,  1.1211e-02,\n","                       1.6614e-02, -2.0482e-02,  3.6753e-03, -1.5339e-02,  3.3476e-02,\n","                      -6.8313e-03, -2.4567e-02, -2.4555e-02,  5.1026e-03, -3.4123e-02,\n","                      -2.0229e-02, -1.7033e-02,  8.3869e-03, -1.3582e-02,  8.2993e-03,\n","                      -3.7892e-02,  1.1889e-03, -2.9846e-02,  1.2569e-02, -5.3742e-03,\n","                       1.1608e-02, -2.8556e-02, -8.9635e-03, -2.7432e-02, -1.8627e-02,\n","                      -5.0976e-03, -3.4260e-02,  1.0807e-02,  3.1082e-02,  1.4369e-02,\n","                      -2.2664e-02, -1.5035e-02,  1.8007e-02,  1.3613e-02,  1.4354e-02,\n","                      -1.3467e-02,  3.1785e-02, -3.7136e-02, -3.5654e-02,  2.6683e-02,\n","                      -2.6555e-02, -2.3786e-02, -3.0347e-02,  1.5303e-02,  1.7542e-02,\n","                      -2.6787e-02,  9.3259e-03,  3.4192e-02, -2.1701e-02,  1.3592e-02,\n","                      -2.1333e-02, -1.1303e-02,  1.2084e-02,  1.1571e-02,  7.8074e-03,\n","                       5.2168e-03,  4.5954e-03,  7.5383e-03,  2.5956e-02,  1.3460e-02,\n","                       1.8164e-02,  3.6153e-02, -1.1187e-02, -2.8757e-02, -3.4901e-02,\n","                       2.0300e-02, -1.9441e-02, -1.1270e-02, -2.2810e-02,  3.4703e-02,\n","                      -7.1793e-03, -3.3232e-02,  1.1967e-02, -3.0404e-02,  6.3152e-03,\n","                      -6.1800e-03, -5.2171e-03,  1.2520e-03, -2.2466e-02, -3.7617e-02,\n","                      -3.8037e-02,  3.8590e-02,  3.0836e-02,  9.1543e-03,  2.9486e-02,\n","                      -7.9983e-03,  3.0871e-02, -3.5776e-02,  2.6364e-02, -2.7961e-02,\n","                       1.5876e-02, -2.1949e-02, -2.9526e-02,  2.6284e-02,  6.3611e-03,\n","                      -1.1216e-02,  3.6253e-02,  3.6573e-02,  9.1087e-03, -1.5783e-03,\n","                      -2.1535e-02, -2.2093e-02, -9.5636e-03, -2.3367e-02,  1.6660e-02,\n","                      -9.7584e-03,  2.7035e-02,  2.3470e-02, -2.8394e-02, -8.1391e-03,\n","                      -1.2422e-02,  1.7663e-02,  3.4255e-02,  2.7922e-02, -6.0435e-03,\n","                      -5.8797e-03, -3.6677e-02, -1.8979e-02,  3.2567e-02,  1.9278e-02,\n","                      -3.2598e-02, -3.5547e-02, -1.6141e-02, -1.2894e-02, -3.8439e-02,\n","                       2.1922e-02, -1.1737e-02, -1.2549e-02, -1.6800e-02, -2.3862e-02,\n","                      -3.7951e-02,  1.2876e-02,  1.9468e-03,  3.8402e-02, -1.3918e-02,\n","                      -3.6059e-02,  1.4356e-02, -2.9740e-02, -1.1984e-02,  3.8393e-02,\n","                      -2.0547e-02,  1.4486e-02], device='cuda:0')),\n","             ('_gnn._conv_layers.3.nn.0.weight',\n","              tensor([[-0.0041, -0.0088, -0.0120,  ...,  0.0257, -0.0159,  0.0024],\n","                      [ 0.0066, -0.0180, -0.0134,  ..., -0.0146,  0.0094, -0.0029],\n","                      [ 0.0300, -0.0231,  0.0155,  ...,  0.0013,  0.0028,  0.0030],\n","                      ...,\n","                      [ 0.0135, -0.0279,  0.0257,  ..., -0.0015,  0.0303,  0.0040],\n","                      [-0.0129,  0.0089,  0.0069,  ...,  0.0095, -0.0246,  0.0223],\n","                      [ 0.0157,  0.0071, -0.0237,  ..., -0.0282, -0.0049,  0.0013]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.3.nn.0.bias',\n","              tensor([ 0.0050,  0.0211,  0.0245,  0.0145, -0.0279, -0.0118, -0.0147,  0.0149,\n","                       0.0177,  0.0304, -0.0294, -0.0134,  0.0230,  0.0165, -0.0165, -0.0089,\n","                      -0.0118, -0.0112,  0.0202,  0.0096,  0.0195, -0.0019, -0.0045, -0.0079,\n","                      -0.0275, -0.0237, -0.0180, -0.0117,  0.0157,  0.0071,  0.0150, -0.0271,\n","                      -0.0006, -0.0209,  0.0123, -0.0283, -0.0297, -0.0002, -0.0305, -0.0181,\n","                       0.0235,  0.0171,  0.0306,  0.0275,  0.0245, -0.0083, -0.0052, -0.0268,\n","                       0.0151,  0.0080, -0.0243, -0.0054,  0.0232,  0.0288,  0.0056,  0.0159,\n","                       0.0097, -0.0054, -0.0231,  0.0221, -0.0225, -0.0073, -0.0130,  0.0107,\n","                      -0.0286,  0.0027, -0.0306,  0.0235, -0.0067, -0.0211, -0.0182, -0.0134,\n","                      -0.0179,  0.0060, -0.0240, -0.0053, -0.0031, -0.0252,  0.0024, -0.0060,\n","                       0.0183, -0.0270, -0.0014, -0.0071, -0.0045,  0.0224, -0.0092, -0.0117,\n","                       0.0097, -0.0170, -0.0169,  0.0257, -0.0281, -0.0080,  0.0275, -0.0176,\n","                       0.0239, -0.0157, -0.0004,  0.0066, -0.0184, -0.0250,  0.0128, -0.0255,\n","                      -0.0255,  0.0286, -0.0093,  0.0004,  0.0011, -0.0309,  0.0192,  0.0053,\n","                       0.0124,  0.0069, -0.0298,  0.0079, -0.0271,  0.0309,  0.0267, -0.0016,\n","                       0.0076,  0.0038,  0.0225,  0.0140, -0.0270,  0.0282, -0.0239,  0.0027,\n","                      -0.0088, -0.0292, -0.0103, -0.0038,  0.0264, -0.0140, -0.0193,  0.0080,\n","                       0.0256,  0.0310, -0.0046, -0.0168, -0.0008,  0.0098,  0.0025,  0.0108,\n","                      -0.0095, -0.0105, -0.0230, -0.0038, -0.0223,  0.0287, -0.0175,  0.0065,\n","                      -0.0113,  0.0104, -0.0011, -0.0233, -0.0041,  0.0073,  0.0058, -0.0169,\n","                       0.0224, -0.0117, -0.0262,  0.0038, -0.0274,  0.0310,  0.0234,  0.0021,\n","                       0.0057, -0.0221,  0.0243, -0.0295, -0.0035, -0.0016, -0.0147, -0.0206,\n","                      -0.0309,  0.0073,  0.0044,  0.0139,  0.0028,  0.0095, -0.0031, -0.0176,\n","                       0.0154, -0.0125,  0.0113, -0.0029, -0.0121,  0.0154,  0.0088, -0.0178,\n","                       0.0118,  0.0151, -0.0190,  0.0277, -0.0091,  0.0311,  0.0103,  0.0229,\n","                      -0.0308, -0.0153, -0.0139,  0.0153,  0.0297,  0.0104, -0.0068,  0.0307,\n","                       0.0172, -0.0111, -0.0277,  0.0271,  0.0311,  0.0099, -0.0177, -0.0141,\n","                       0.0141,  0.0199, -0.0004, -0.0080, -0.0030, -0.0273, -0.0012, -0.0122,\n","                       0.0288,  0.0027, -0.0196, -0.0270,  0.0035, -0.0016,  0.0186, -0.0167,\n","                      -0.0305,  0.0069,  0.0188, -0.0122,  0.0087, -0.0057,  0.0063, -0.0182,\n","                       0.0274,  0.0092,  0.0094,  0.0283,  0.0009,  0.0250, -0.0171, -0.0102,\n","                      -0.0088,  0.0264,  0.0103,  0.0199,  0.0258, -0.0062, -0.0070,  0.0124,\n","                       0.0154,  0.0250, -0.0033,  0.0295, -0.0292,  0.0125,  0.0035,  0.0046,\n","                       0.0283, -0.0118, -0.0050,  0.0015,  0.0184,  0.0240, -0.0158,  0.0034,\n","                      -0.0086,  0.0037, -0.0257,  0.0089,  0.0047, -0.0003,  0.0072,  0.0007,\n","                      -0.0285,  0.0269,  0.0299,  0.0151,  0.0081, -0.0294,  0.0231,  0.0116,\n","                       0.0051,  0.0058,  0.0296, -0.0263, -0.0082,  0.0274,  0.0100, -0.0031,\n","                      -0.0200,  0.0210, -0.0054, -0.0311, -0.0094,  0.0134, -0.0021,  0.0241,\n","                      -0.0026, -0.0020,  0.0274, -0.0038, -0.0176, -0.0178, -0.0137,  0.0124,\n","                      -0.0160,  0.0211,  0.0033,  0.0165, -0.0059,  0.0120, -0.0190,  0.0283,\n","                       0.0254,  0.0259, -0.0172,  0.0207,  0.0258,  0.0122,  0.0178,  0.0060,\n","                      -0.0286,  0.0098, -0.0103,  0.0190, -0.0099, -0.0166,  0.0186,  0.0176,\n","                      -0.0195,  0.0148,  0.0141, -0.0262,  0.0029,  0.0052, -0.0049, -0.0300,\n","                      -0.0149, -0.0252, -0.0091,  0.0282, -0.0177, -0.0237,  0.0003,  0.0065,\n","                       0.0092,  0.0100,  0.0238, -0.0202,  0.0003, -0.0043, -0.0235, -0.0201,\n","                       0.0079,  0.0198, -0.0188, -0.0274, -0.0079,  0.0123, -0.0183, -0.0222,\n","                      -0.0154,  0.0127, -0.0046, -0.0256,  0.0304,  0.0027, -0.0172, -0.0064,\n","                       0.0277,  0.0278, -0.0197, -0.0252,  0.0236,  0.0174, -0.0297, -0.0261,\n","                      -0.0298,  0.0193,  0.0199, -0.0234,  0.0281, -0.0127,  0.0026, -0.0235,\n","                      -0.0075,  0.0267, -0.0208,  0.0015,  0.0261, -0.0062, -0.0148,  0.0066,\n","                       0.0214, -0.0309, -0.0196, -0.0053,  0.0301, -0.0058,  0.0055, -0.0290,\n","                      -0.0119,  0.0170,  0.0290,  0.0160, -0.0174,  0.0110, -0.0020, -0.0268,\n","                      -0.0290, -0.0144, -0.0015,  0.0259,  0.0045, -0.0291, -0.0109,  0.0226,\n","                       0.0290,  0.0232,  0.0005, -0.0043, -0.0136,  0.0009,  0.0249, -0.0146,\n","                      -0.0039, -0.0065, -0.0288,  0.0030, -0.0146, -0.0065, -0.0218,  0.0007,\n","                       0.0235, -0.0216,  0.0255,  0.0236,  0.0201,  0.0260, -0.0248,  0.0079,\n","                      -0.0011, -0.0300,  0.0072,  0.0303, -0.0003, -0.0181, -0.0150, -0.0213,\n","                      -0.0002, -0.0226,  0.0120, -0.0240, -0.0030, -0.0272, -0.0273,  0.0232,\n","                       0.0022,  0.0044,  0.0018,  0.0246,  0.0252, -0.0101, -0.0122,  0.0073,\n","                      -0.0235,  0.0064,  0.0061,  0.0086,  0.0231, -0.0058,  0.0244,  0.0054,\n","                      -0.0147, -0.0248,  0.0001, -0.0039,  0.0082, -0.0064, -0.0230,  0.0193,\n","                      -0.0112, -0.0048, -0.0072, -0.0295,  0.0171,  0.0109,  0.0178,  0.0103,\n","                       0.0196,  0.0256,  0.0230, -0.0025, -0.0017,  0.0071,  0.0013, -0.0296,\n","                       0.0257, -0.0128,  0.0255,  0.0190,  0.0005,  0.0054,  0.0140, -0.0280,\n","                       0.0091, -0.0057,  0.0142,  0.0295, -0.0178,  0.0144,  0.0206,  0.0137,\n","                      -0.0270,  0.0183, -0.0137,  0.0223,  0.0103, -0.0085,  0.0127,  0.0059,\n","                       0.0048, -0.0245, -0.0047, -0.0150, -0.0267,  0.0203, -0.0295, -0.0129,\n","                       0.0160,  0.0065,  0.0184,  0.0148, -0.0294,  0.0231, -0.0270, -0.0228,\n","                      -0.0193, -0.0140,  0.0077,  0.0295, -0.0058, -0.0208, -0.0089,  0.0015,\n","                       0.0247,  0.0067, -0.0242, -0.0169, -0.0157,  0.0109, -0.0272, -0.0131,\n","                      -0.0284,  0.0230, -0.0025,  0.0150, -0.0242,  0.0053,  0.0208, -0.0240,\n","                       0.0036, -0.0216,  0.0007,  0.0198,  0.0214, -0.0154,  0.0078, -0.0119,\n","                      -0.0139, -0.0187, -0.0213, -0.0293, -0.0118, -0.0273, -0.0072, -0.0037,\n","                      -0.0097,  0.0052, -0.0013,  0.0202,  0.0166, -0.0050,  0.0042, -0.0027,\n","                       0.0161, -0.0195, -0.0258,  0.0309,  0.0217, -0.0264,  0.0065, -0.0079,\n","                       0.0068,  0.0122,  0.0142,  0.0084,  0.0045, -0.0254, -0.0129,  0.0190,\n","                      -0.0091, -0.0011,  0.0133, -0.0022,  0.0287, -0.0032, -0.0156,  0.0264,\n","                       0.0134,  0.0068,  0.0115, -0.0286,  0.0040,  0.0108,  0.0061,  0.0310,\n","                       0.0298,  0.0063,  0.0035,  0.0255, -0.0251,  0.0023,  0.0248, -0.0003,\n","                      -0.0180, -0.0165, -0.0292,  0.0282,  0.0038,  0.0210,  0.0202, -0.0098,\n","                       0.0270, -0.0269, -0.0124, -0.0203,  0.0150, -0.0011, -0.0263,  0.0062,\n","                      -0.0091,  0.0032,  0.0232, -0.0150,  0.0208, -0.0171, -0.0124,  0.0188,\n","                       0.0122,  0.0073, -0.0024, -0.0033, -0.0059, -0.0059, -0.0096,  0.0188,\n","                       0.0297, -0.0056,  0.0284,  0.0121,  0.0291, -0.0086, -0.0036, -0.0205],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.3.nn.2.weight',\n","              tensor([[ 0.0383,  0.0202, -0.0335,  ...,  0.0242, -0.0082, -0.0027],\n","                      [ 0.0365, -0.0252,  0.0085,  ...,  0.0103,  0.0088,  0.0334],\n","                      [-0.0004,  0.0200,  0.0031,  ..., -0.0216, -0.0269,  0.0277],\n","                      ...,\n","                      [-0.0297, -0.0052, -0.0184,  ..., -0.0042, -0.0078,  0.0320],\n","                      [ 0.0231, -0.0287,  0.0214,  ..., -0.0084,  0.0088,  0.0229],\n","                      [ 0.0347,  0.0272, -0.0165,  ..., -0.0296,  0.0357,  0.0017]],\n","                     device='cuda:0')),\n","             ('_gnn._conv_layers.3.nn.2.bias',\n","              tensor([ 3.2864e-02, -3.4936e-02,  3.4483e-02, -1.4133e-02,  1.0533e-02,\n","                      -2.1287e-02, -3.7351e-02, -1.2304e-02, -1.0438e-02,  3.2925e-02,\n","                       4.8153e-03, -3.3551e-02,  3.3700e-02, -1.5186e-02, -1.3680e-02,\n","                       3.0072e-02,  7.3020e-04, -1.7090e-02, -1.4836e-02, -2.8841e-03,\n","                       7.7124e-03,  2.8422e-02, -1.7838e-03,  1.8793e-02,  3.7313e-02,\n","                       8.2263e-04,  2.4841e-02, -1.4539e-02,  2.2693e-02, -3.0688e-02,\n","                      -2.4252e-02,  1.7126e-02,  2.6161e-02, -2.5208e-03, -3.8574e-03,\n","                       1.9865e-02,  6.6752e-03, -1.9620e-02, -1.7405e-02,  1.4774e-02,\n","                       2.3139e-02, -3.8520e-02, -1.4420e-02, -3.5739e-02, -1.4042e-02,\n","                      -1.6976e-03,  1.7058e-02, -7.1874e-04, -2.9593e-02,  2.4708e-02,\n","                       5.4257e-03,  2.4581e-02,  3.4770e-02,  2.2459e-02, -1.8980e-02,\n","                      -1.1711e-03, -1.1195e-02, -2.2083e-03,  2.8650e-02, -2.8893e-02,\n","                       1.5610e-02,  3.4076e-02, -1.7912e-02, -3.1229e-02,  3.3729e-02,\n","                       2.6880e-02,  1.9929e-02,  8.3259e-03,  1.4529e-02,  2.3107e-02,\n","                      -1.9100e-02, -1.9963e-02,  3.0259e-02, -9.4643e-03,  3.0559e-03,\n","                      -1.5284e-02,  3.3101e-02,  1.5906e-02, -2.2392e-02,  5.6704e-03,\n","                       2.3106e-02, -7.6880e-03, -2.1971e-02,  1.0785e-02, -2.9531e-02,\n","                      -2.8971e-02, -9.6531e-03,  3.4135e-02,  3.0303e-02,  2.4812e-02,\n","                       3.1495e-02, -1.4751e-02,  1.8230e-02, -7.4346e-03, -1.2526e-04,\n","                      -1.1788e-02, -2.9708e-02,  1.1089e-02,  5.0381e-03, -4.4557e-03,\n","                      -2.1102e-02, -1.9529e-02, -1.3539e-02,  2.5218e-02,  3.0924e-02,\n","                      -3.2282e-02, -1.9436e-02,  9.3322e-03, -4.7711e-03, -1.7004e-02,\n","                      -3.5544e-02,  3.1667e-02,  2.7665e-03, -9.7773e-03,  2.9639e-02,\n","                       1.6287e-02,  2.9452e-03, -3.0571e-02, -2.4130e-02,  4.9868e-03,\n","                      -1.8497e-03, -2.6382e-02,  1.9525e-02,  1.3686e-02, -2.6756e-02,\n","                       3.3527e-02, -3.7932e-02, -1.9784e-02, -2.1865e-02,  2.8144e-02,\n","                      -3.5338e-03,  1.5859e-03, -4.3833e-03, -6.9454e-03,  1.3862e-02,\n","                      -7.9896e-03, -3.5236e-02,  3.0456e-02, -3.0502e-02,  6.9602e-03,\n","                      -1.5297e-02, -3.2648e-02, -2.7088e-02,  3.6666e-02,  3.7986e-03,\n","                      -3.3820e-02,  5.0909e-03,  2.7259e-02, -2.2654e-02, -1.3710e-02,\n","                      -2.4403e-02,  3.6867e-02,  2.3605e-02, -2.0201e-02,  2.1836e-02,\n","                      -2.9603e-02,  3.3889e-02,  2.4241e-03, -3.3597e-02,  1.6963e-02,\n","                       3.5499e-03,  4.3510e-03,  3.1358e-02, -2.5664e-02,  2.0822e-02,\n","                       9.6539e-03, -3.6269e-02, -6.2278e-03, -2.9177e-03, -1.4369e-02,\n","                       2.2469e-02,  1.2323e-02,  3.5454e-03, -6.8307e-03,  2.1199e-02,\n","                      -1.3093e-02, -1.0287e-02, -3.5802e-02, -1.7873e-02,  2.5835e-02,\n","                       1.2179e-03, -1.2060e-02,  1.2040e-02, -2.0867e-02,  2.7354e-02,\n","                       1.1424e-02,  1.7775e-02,  2.6025e-02,  4.4608e-03, -1.5712e-04,\n","                       2.7094e-02, -1.5793e-02,  3.2543e-02,  3.7614e-02, -2.1852e-03,\n","                      -3.6109e-02,  7.2363e-03, -4.0120e-03,  3.0055e-02,  1.8112e-02,\n","                      -6.1789e-03, -5.0987e-03, -1.7041e-02, -2.9828e-02, -2.3193e-03,\n","                       2.7592e-02,  2.6386e-02, -1.1885e-02,  1.2813e-02, -3.0783e-02,\n","                      -2.8167e-02, -3.6330e-02,  3.6517e-02,  1.0599e-02,  1.2951e-02,\n","                      -9.7841e-03,  3.0724e-02, -1.8101e-02,  9.0622e-03,  1.3863e-02,\n","                       4.1459e-03, -2.6357e-02, -3.2813e-02, -2.6780e-02,  2.7480e-04,\n","                      -1.5835e-02, -2.6736e-02,  2.3313e-02, -3.0518e-02, -3.8392e-02,\n","                      -3.3189e-02, -1.4939e-02,  1.3485e-02,  9.4804e-03, -1.2010e-02,\n","                       3.1792e-02, -1.9712e-02, -8.2779e-03,  1.8663e-03,  1.2244e-02,\n","                       3.7414e-02,  2.0763e-02,  1.8069e-02, -4.0955e-03,  1.2296e-02,\n","                       2.5916e-03, -2.5910e-02,  2.2026e-02,  2.5733e-02, -2.2125e-02,\n","                       2.6359e-02, -3.3004e-02,  3.3320e-02, -3.2075e-03,  3.2125e-02,\n","                      -1.5890e-03, -1.9436e-05,  1.0986e-02,  1.0471e-02, -1.2637e-02,\n","                       2.7763e-02, -3.7262e-02,  2.8981e-02,  1.8322e-02,  2.7835e-02,\n","                       2.1731e-02,  8.6135e-03, -1.4018e-02,  2.9889e-02, -1.2996e-02,\n","                      -2.4771e-02,  2.4174e-02,  1.0325e-02,  2.7646e-02,  1.4036e-02,\n","                       1.2113e-02, -6.9608e-03, -2.7161e-02, -2.5806e-03, -2.9330e-02,\n","                       3.6813e-02, -3.2378e-02,  7.0693e-03,  1.7565e-02,  2.8097e-02,\n","                      -2.5401e-02,  3.7694e-02,  2.9730e-02, -3.1181e-02,  3.3840e-04,\n","                      -1.6449e-02, -6.5606e-03, -9.3963e-03, -1.2419e-02,  1.7829e-02,\n","                       2.7045e-03, -9.3056e-03, -3.0948e-03, -1.7921e-02,  3.4883e-02,\n","                       1.0071e-02, -3.3141e-02, -3.8170e-02, -3.6697e-02,  1.6284e-02,\n","                       9.9644e-03,  1.7299e-02,  7.2926e-03, -1.1349e-02,  3.7034e-02,\n","                      -2.6688e-02, -3.4574e-02,  1.8994e-02, -8.0682e-03,  1.1229e-03,\n","                       3.2852e-02,  6.6920e-03,  2.8739e-02,  1.6984e-03, -2.1517e-02,\n","                       3.6999e-02,  2.9801e-03, -3.2034e-02, -2.8054e-02, -2.6285e-02,\n","                      -1.5356e-02,  2.8135e-02,  3.2140e-02,  1.3002e-02, -2.7770e-02,\n","                      -1.5114e-02,  3.5743e-02,  1.8868e-02,  1.8105e-02, -2.6882e-02,\n","                      -8.6890e-03,  1.7195e-02,  3.2939e-03,  3.9851e-04, -2.9001e-02,\n","                       3.4212e-02, -3.2529e-02,  2.0769e-02, -2.6009e-03, -2.3198e-02,\n","                       3.7955e-02,  3.7296e-02, -2.9549e-02,  4.7931e-03,  1.9652e-02,\n","                      -2.3311e-02, -3.3729e-02,  2.7333e-02,  3.7043e-02, -6.9165e-03,\n","                       3.9421e-03, -9.9569e-03,  2.9916e-02,  8.1695e-03,  2.5433e-02,\n","                      -1.4440e-02,  2.8035e-02,  1.9903e-02, -2.9354e-02, -6.8950e-03,\n","                       1.0364e-02,  6.6385e-03,  1.9866e-02,  3.3659e-02,  4.1232e-03,\n","                      -2.1273e-02, -7.3196e-03,  2.4370e-02,  3.3214e-03,  1.1487e-02,\n","                       2.5007e-02,  2.9283e-02,  1.2771e-02,  1.1562e-02, -1.2368e-02,\n","                      -6.0808e-03, -2.0092e-02, -2.0573e-04,  1.1564e-02,  2.5155e-03,\n","                      -1.1554e-02, -1.5451e-03,  1.6943e-02,  2.6072e-03, -2.3609e-02,\n","                       1.1519e-02,  3.5529e-02,  3.1269e-02,  1.3593e-02, -1.1783e-02,\n","                       1.6291e-02,  3.6610e-02, -3.5180e-02, -5.0971e-03, -8.1146e-03,\n","                       9.2468e-03,  3.1322e-02, -1.6447e-02, -2.3332e-02,  1.3831e-02,\n","                       3.0199e-02,  2.2485e-02, -1.5856e-02,  2.5956e-02,  9.6417e-03,\n","                       9.3419e-03,  1.0228e-02,  3.3352e-02, -3.7871e-02,  1.0267e-02,\n","                      -3.2362e-02, -3.2589e-02,  7.8772e-03, -1.3620e-02, -3.4164e-02,\n","                      -1.3725e-02, -2.1644e-02, -2.2868e-02, -9.2138e-03,  1.3424e-02,\n","                      -3.0204e-02, -3.3174e-02, -3.4669e-03,  1.0747e-03, -2.2124e-02,\n","                      -1.0881e-02, -6.0054e-03,  1.8638e-02, -2.8487e-02,  3.4016e-02,\n","                      -2.4209e-02,  3.2562e-02,  4.9946e-03, -1.9473e-02, -2.3269e-02,\n","                      -3.2619e-02, -3.6817e-02,  1.1521e-02, -3.6832e-02,  1.0021e-03,\n","                      -2.4099e-02, -5.1179e-03,  3.1236e-02,  3.5518e-02,  2.1834e-02,\n","                       3.0576e-02,  2.1435e-02, -3.5064e-02, -1.8963e-02, -1.6690e-03,\n","                       1.5527e-02, -3.6874e-02, -1.9970e-02, -1.6754e-02,  1.4261e-02,\n","                       1.0894e-02,  1.5291e-02, -2.4782e-02,  2.4633e-02,  5.3262e-03,\n","                      -2.4118e-02, -1.1408e-02, -2.8662e-03,  1.2198e-02,  3.4720e-02,\n","                      -1.7776e-02, -3.6209e-02, -2.7122e-02, -2.7226e-02,  2.4333e-02,\n","                       2.0723e-02,  2.5215e-02, -1.8847e-02,  6.1488e-03,  2.7603e-02,\n","                       2.4883e-02,  3.8377e-02, -3.2452e-02, -3.6190e-02, -2.7800e-02,\n","                      -1.2662e-02,  4.1442e-03,  7.3032e-03, -3.7835e-02, -3.8012e-02,\n","                       3.2042e-02, -9.1809e-04,  3.4862e-02,  1.2982e-03,  3.3417e-02,\n","                       1.1650e-02, -8.4792e-03,  2.9023e-02, -6.5795e-04, -3.3476e-02,\n","                       2.4750e-02,  2.2507e-02, -1.7685e-02, -2.2773e-02, -1.5876e-03,\n","                      -4.5632e-03,  3.6418e-02, -3.8157e-02,  1.0296e-02, -2.9089e-02,\n","                       2.8234e-02, -3.3061e-02], device='cuda:0')),\n","             ('_gnn._post_processing.0.weight',\n","              tensor([[-0.0052, -0.0163,  0.0161,  ...,  0.0180,  0.0095,  0.0068],\n","                      [-0.0132, -0.0197,  0.0122,  ...,  0.0015, -0.0207,  0.0199],\n","                      [-0.0016, -0.0219,  0.0002,  ..., -0.0101,  0.0044, -0.0173],\n","                      ...,\n","                      [ 0.0209,  0.0157, -0.0090,  ...,  0.0104,  0.0164, -0.0177],\n","                      [ 0.0042, -0.0091,  0.0142,  ...,  0.0024, -0.0131, -0.0033],\n","                      [-0.0131, -0.0049,  0.0192,  ...,  0.0082,  0.0160,  0.0050]],\n","                     device='cuda:0')),\n","             ('_gnn._post_processing.0.bias',\n","              tensor([-7.5466e-03, -1.1713e-02, -1.7424e-02,  1.6422e-02, -8.0192e-04,\n","                      -6.6785e-03,  1.4217e-02,  2.1166e-02,  1.5073e-02, -1.0172e-02,\n","                       1.5355e-03,  9.3217e-03,  9.7367e-04, -9.1000e-03, -1.7689e-02,\n","                       3.1030e-04, -4.2498e-03, -1.7903e-02, -7.0353e-03,  8.1773e-03,\n","                       1.5435e-02, -2.1198e-02,  1.2141e-02,  1.9038e-02, -1.7871e-02,\n","                      -9.6031e-03, -1.7414e-02, -1.9461e-02, -1.6927e-02, -1.6052e-02,\n","                      -4.1217e-03,  1.8431e-02, -1.7608e-03,  1.4629e-02, -1.9142e-03,\n","                      -8.1691e-03,  7.5267e-03,  1.8397e-02, -1.7882e-02, -1.2359e-02,\n","                       1.3320e-02,  5.0107e-03,  9.7191e-03,  4.1212e-03, -1.8103e-02,\n","                      -1.5265e-02,  1.2468e-02,  1.2127e-02, -6.8055e-04, -1.4276e-02,\n","                      -1.3290e-02, -6.8825e-03, -1.5858e-02,  1.1809e-02,  1.2750e-02,\n","                       2.1360e-05,  1.9042e-03,  2.1809e-02,  1.1631e-02,  2.0832e-03,\n","                      -9.0129e-03, -8.3240e-03,  1.1146e-02,  7.2927e-03,  1.1333e-02,\n","                       2.0666e-03,  7.7779e-03,  1.6647e-02,  9.5281e-03,  1.0887e-02,\n","                       1.9421e-02, -1.1396e-02,  1.2178e-02, -1.7242e-03, -8.6362e-03,\n","                       2.0358e-02, -3.7705e-03,  1.6980e-02,  2.1204e-02, -5.4666e-06,\n","                       5.6365e-03,  2.0698e-02,  1.1493e-02,  1.1580e-02,  1.3542e-02,\n","                      -2.1610e-03, -4.6268e-03, -1.3782e-02,  1.9545e-02, -9.3045e-03,\n","                      -9.8013e-03, -1.9318e-03,  4.4105e-03, -6.2514e-03, -1.8503e-02,\n","                      -1.4846e-02,  1.6328e-02, -2.0918e-02, -8.7280e-03,  1.4388e-02,\n","                       1.1989e-02, -7.7534e-05, -1.3112e-02,  1.3371e-02,  1.5912e-02,\n","                      -1.2487e-02,  8.8765e-03, -1.9728e-02, -9.3168e-03,  2.1643e-02,\n","                       1.1117e-02, -5.2550e-03, -3.3358e-03, -2.1817e-02,  6.1067e-03,\n","                       5.7019e-03,  2.1568e-02, -1.6747e-02,  6.9551e-03, -1.6593e-02,\n","                      -2.0516e-02, -3.6092e-03,  1.1184e-02,  3.7772e-03,  7.6986e-03,\n","                      -1.2729e-02, -2.1291e-02, -1.0359e-02, -1.6997e-02,  1.3344e-02,\n","                      -1.1231e-02, -1.4738e-02, -1.7939e-02,  1.4070e-02, -1.8524e-02,\n","                      -1.5485e-02, -1.6669e-02, -1.5645e-03, -1.4235e-02, -2.5598e-03,\n","                      -1.4805e-02,  1.2210e-02,  1.0090e-02,  1.0145e-02, -1.5846e-02,\n","                       5.6453e-03,  1.1872e-02,  1.5050e-02,  1.2800e-02, -1.1161e-02,\n","                      -1.3508e-02, -1.7756e-02, -1.4792e-02, -5.1242e-03, -1.2005e-02,\n","                      -1.6558e-02,  1.2553e-02, -4.9984e-03,  1.2515e-04, -5.8343e-03,\n","                      -5.4676e-04, -8.5434e-03,  5.5456e-03, -8.0100e-03, -6.6079e-03,\n","                       1.0971e-02,  8.1540e-03, -1.1476e-02, -2.0429e-03, -2.0110e-02,\n","                      -1.7523e-02, -1.8181e-02,  8.1825e-03, -4.2850e-04, -2.1691e-03,\n","                      -5.1116e-04, -8.8199e-03,  1.3651e-02, -1.2002e-02, -2.2255e-03,\n","                      -4.4466e-04,  2.1360e-02, -5.6547e-03,  2.4674e-04,  8.5624e-03,\n","                      -2.1930e-02, -6.9235e-03, -2.8500e-03, -1.2962e-02,  1.9709e-02,\n","                       6.1375e-03, -6.1316e-03,  5.4800e-03, -5.8328e-03,  6.0650e-03,\n","                       1.4354e-02, -6.7007e-03,  8.7497e-03, -1.4142e-02, -1.5663e-02,\n","                      -1.2291e-02,  2.1040e-02, -7.9962e-03, -1.2060e-02,  2.0891e-02,\n","                       1.8514e-02, -5.2427e-03, -1.7163e-03, -2.0208e-02,  1.3888e-02,\n","                       9.8003e-03,  1.7580e-02, -1.8379e-02,  4.0250e-03,  1.0915e-02,\n","                       2.1591e-02,  7.0281e-03,  5.3214e-03,  2.0529e-02,  9.6404e-04,\n","                      -2.5464e-03, -2.8770e-03, -1.7742e-02, -6.9972e-03,  8.8032e-03,\n","                      -3.4052e-03, -1.2830e-02,  2.1938e-02,  3.9101e-03, -2.0793e-02,\n","                       8.4278e-03,  1.3627e-02,  3.1433e-03,  1.6751e-02, -1.0901e-02,\n","                      -1.2807e-02, -3.4334e-03, -1.0143e-03, -1.6148e-02, -5.1090e-03,\n","                       1.0985e-02, -1.1080e-03,  1.7463e-02,  1.8575e-02, -1.9020e-03,\n","                      -1.9980e-02,  1.3781e-02, -1.7330e-02,  5.4815e-03, -3.0278e-03,\n","                       2.9478e-03,  1.4312e-02,  9.7055e-03,  1.7655e-02, -6.7967e-03,\n","                      -1.0945e-03,  9.1881e-03,  1.8658e-02, -7.6401e-03, -2.1646e-02,\n","                      -3.5201e-03,  2.0115e-02, -1.6632e-02,  6.2813e-03, -7.1189e-03,\n","                       2.0875e-03,  5.8965e-03, -1.8644e-02, -2.0219e-02, -1.9816e-02,\n","                      -2.0538e-02,  8.9708e-03, -2.0222e-02,  2.0471e-02,  3.0389e-03,\n","                      -2.1882e-02,  9.6233e-03,  1.4746e-02, -6.4498e-03, -5.4487e-03,\n","                      -1.6635e-02, -2.0919e-02, -6.2147e-05, -1.0011e-02,  1.7551e-02,\n","                      -2.3231e-03,  6.8227e-04, -1.8860e-02,  9.4942e-03,  5.7348e-03,\n","                      -6.9052e-03, -1.9296e-03, -1.2635e-03, -1.3749e-02,  2.1970e-04,\n","                      -5.8870e-04,  2.1370e-02, -1.1303e-02,  8.0516e-03,  7.2595e-03,\n","                       1.7351e-02,  1.5732e-02, -8.8336e-03, -9.5662e-03, -1.4425e-02,\n","                      -1.7981e-02, -1.0415e-02, -1.9438e-02,  4.6266e-03,  1.1997e-02,\n","                       1.1854e-02,  1.6435e-02, -1.4845e-04, -1.5673e-02, -1.1432e-02,\n","                       2.6188e-03,  1.8114e-02,  4.0177e-03,  1.6091e-02,  1.2791e-03,\n","                       1.5010e-02, -1.5966e-02, -4.8598e-03,  8.6124e-03, -4.3504e-03,\n","                      -6.1091e-03,  2.9082e-03,  3.2048e-03, -7.8143e-03,  1.0185e-02,\n","                      -1.2845e-02, -1.3466e-02,  2.9067e-03, -7.9833e-03, -1.9660e-02,\n","                       2.7659e-03], device='cuda:0')),\n","             ('_gnn._post_processing.2.weight',\n","              tensor([[ 0.0140,  0.0166, -0.0011,  ..., -0.0487, -0.0084, -0.0175],\n","                      [-0.0371, -0.0394, -0.0160,  ...,  0.0017,  0.0386,  0.0423],\n","                      [-0.0325,  0.0115, -0.0535,  ...,  0.0238,  0.0336,  0.0407],\n","                      ...,\n","                      [ 0.0130,  0.0451,  0.0058,  ..., -0.0269,  0.0505, -0.0142],\n","                      [-0.0418,  0.0089,  0.0399,  ..., -0.0028, -0.0148, -0.0294],\n","                      [ 0.0478, -0.0496, -0.0179,  ...,  0.0038,  0.0273, -0.0117]],\n","                     device='cuda:0')),\n","             ('_gnn._post_processing.2.bias',\n","              tensor([-0.0123,  0.0005,  0.0362,  0.0446, -0.0245,  0.0452,  0.0056,  0.0507,\n","                      -0.0108,  0.0451,  0.0019,  0.0154, -0.0508, -0.0444, -0.0489, -0.0522,\n","                       0.0111, -0.0368, -0.0333,  0.0213,  0.0237,  0.0540, -0.0024, -0.0358,\n","                      -0.0064, -0.0082, -0.0330, -0.0421, -0.0192, -0.0328,  0.0523, -0.0419,\n","                      -0.0287,  0.0300,  0.0108, -0.0422, -0.0330, -0.0480,  0.0419, -0.0230,\n","                      -0.0371,  0.0349, -0.0057,  0.0404, -0.0010,  0.0042,  0.0139, -0.0104,\n","                      -0.0473,  0.0380, -0.0329,  0.0423, -0.0426, -0.0071, -0.0228, -0.0429,\n","                       0.0545, -0.0334, -0.0489,  0.0289,  0.0256, -0.0438,  0.0419,  0.0091,\n","                      -0.0088, -0.0022,  0.0017, -0.0240, -0.0035, -0.0185, -0.0529, -0.0476,\n","                      -0.0076, -0.0180,  0.0047,  0.0228,  0.0505,  0.0496,  0.0286,  0.0257,\n","                       0.0262,  0.0294,  0.0139,  0.0003, -0.0436, -0.0260, -0.0141, -0.0190,\n","                      -0.0116,  0.0080, -0.0331,  0.0470,  0.0342,  0.0309, -0.0294, -0.0299,\n","                       0.0191,  0.0134, -0.0544,  0.0474, -0.0327, -0.0538,  0.0418,  0.0041,\n","                       0.0320,  0.0177, -0.0425,  0.0480,  0.0317,  0.0345, -0.0053,  0.0170,\n","                       0.0516, -0.0289,  0.0542, -0.0431,  0.0126, -0.0480, -0.0488, -0.0319,\n","                       0.0317,  0.0038, -0.0546, -0.0246,  0.0376, -0.0502, -0.0158,  0.0017,\n","                      -0.0223, -0.0188, -0.0329, -0.0139,  0.0214, -0.0376, -0.0325, -0.0071,\n","                       0.0357,  0.0401, -0.0021,  0.0053, -0.0426,  0.0501, -0.0149,  0.0450,\n","                      -0.0149, -0.0150, -0.0126,  0.0351, -0.0157, -0.0451, -0.0444,  0.0486,\n","                      -0.0043,  0.0282,  0.0349,  0.0481, -0.0319, -0.0397,  0.0224,  0.0049,\n","                       0.0145,  0.0330,  0.0113,  0.0444, -0.0100,  0.0164, -0.0202, -0.0444,\n","                       0.0108, -0.0342,  0.0110, -0.0028,  0.0544, -0.0217, -0.0213, -0.0154,\n","                       0.0456, -0.0095, -0.0303,  0.0527,  0.0077, -0.0490,  0.0487, -0.0391,\n","                      -0.0347,  0.0070,  0.0366, -0.0208, -0.0472, -0.0495,  0.0262, -0.0543,\n","                      -0.0014,  0.0004, -0.0008,  0.0059, -0.0377,  0.0080,  0.0331,  0.0067,\n","                       0.0075,  0.0424,  0.0545,  0.0305, -0.0203,  0.0228, -0.0095,  0.0045,\n","                       0.0300,  0.0203,  0.0373, -0.0458,  0.0039,  0.0026,  0.0443,  0.0480,\n","                      -0.0514,  0.0345, -0.0468,  0.0056, -0.0497,  0.0509, -0.0106, -0.0213,\n","                       0.0534,  0.0360,  0.0534, -0.0217,  0.0537, -0.0400,  0.0191, -0.0132,\n","                       0.0328,  0.0510,  0.0032, -0.0113, -0.0425, -0.0503, -0.0040,  0.0330,\n","                      -0.0455, -0.0374, -0.0382,  0.0115, -0.0394,  0.0437, -0.0071,  0.0220,\n","                       0.0332, -0.0438,  0.0155, -0.0004,  0.0065, -0.0459,  0.0267, -0.0484],\n","                     device='cuda:0')),\n","             ('_gnn._readout.0.weight',\n","              tensor([[-0.0241, -0.0349, -0.0036,  ...,  0.0246, -0.0083,  0.0322],\n","                      [ 0.0257,  0.0098,  0.0186,  ...,  0.0119,  0.0336, -0.0121],\n","                      [ 0.0215, -0.0354, -0.0025,  ...,  0.0196,  0.0229, -0.0325],\n","                      ...,\n","                      [-0.0296, -0.0322,  0.0301,  ...,  0.0295, -0.0189, -0.0347],\n","                      [-0.0155,  0.0203,  0.0200,  ..., -0.0069,  0.0280,  0.0119],\n","                      [ 0.0315,  0.0088,  0.0356,  ...,  0.0077, -0.0137, -0.0271]],\n","                     device='cuda:0')),\n","             ('_gnn._readout.0.bias',\n","              tensor([ 0.0202, -0.0085,  0.0078, -0.0030, -0.0345,  0.0358, -0.0153,  0.0116,\n","                       0.0252, -0.0309,  0.0006, -0.0162,  0.0237, -0.0194,  0.0290, -0.0240,\n","                      -0.0181,  0.0138, -0.0175, -0.0355, -0.0095,  0.0320, -0.0353, -0.0057,\n","                       0.0288, -0.0173, -0.0150,  0.0180,  0.0278, -0.0067, -0.0018,  0.0011,\n","                       0.0053,  0.0184, -0.0171,  0.0163,  0.0098,  0.0306, -0.0099, -0.0351,\n","                      -0.0107, -0.0174,  0.0096, -0.0345, -0.0358,  0.0301, -0.0301, -0.0310,\n","                      -0.0019, -0.0252,  0.0051,  0.0350, -0.0315,  0.0323,  0.0170,  0.0129,\n","                       0.0264, -0.0293,  0.0327,  0.0229, -0.0180,  0.0306,  0.0145,  0.0318,\n","                      -0.0256,  0.0160,  0.0134,  0.0310,  0.0235, -0.0304, -0.0059,  0.0138,\n","                      -0.0317,  0.0064,  0.0336, -0.0171, -0.0177,  0.0010, -0.0171, -0.0128,\n","                      -0.0324, -0.0196, -0.0325,  0.0212,  0.0325,  0.0114, -0.0116,  0.0110,\n","                       0.0289, -0.0091,  0.0235,  0.0169,  0.0089, -0.0012, -0.0131,  0.0196,\n","                      -0.0222, -0.0177, -0.0229,  0.0032,  0.0046,  0.0160,  0.0084,  0.0272,\n","                      -0.0119,  0.0213, -0.0226,  0.0181, -0.0228, -0.0265, -0.0302, -0.0256,\n","                      -0.0135,  0.0181,  0.0152, -0.0306,  0.0310,  0.0231,  0.0211,  0.0140,\n","                      -0.0328,  0.0292, -0.0277, -0.0177,  0.0198, -0.0307,  0.0137,  0.0118],\n","                     device='cuda:0')),\n","             ('_tasks.0._affine.weight',\n","              tensor([[-4.3806e-02,  6.9782e-02, -2.0861e-02,  5.4338e-02,  5.4371e-04,\n","                       -8.7313e-02,  3.8802e-02, -5.2354e-03, -4.0663e-02, -8.7879e-02,\n","                        6.7298e-02,  5.1902e-02,  6.5293e-02, -1.4188e-02, -8.3544e-02,\n","                       -3.0617e-02,  4.2499e-02, -7.4213e-02,  2.5971e-02, -8.0392e-02,\n","                       -1.1062e-02, -4.3036e-02, -7.3995e-02, -6.7751e-02,  2.3828e-02,\n","                        8.6771e-02, -3.2853e-02, -3.7062e-02, -3.3546e-02, -2.7030e-02,\n","                       -8.4549e-02, -3.2338e-02,  2.5229e-02,  7.1333e-02, -1.8684e-02,\n","                        6.6633e-03,  1.2459e-03,  5.7873e-02, -6.8254e-02,  4.1374e-02,\n","                       -5.7142e-02, -3.1520e-02,  1.5780e-05, -3.0563e-02, -7.5187e-02,\n","                        6.4913e-02,  4.0725e-02, -7.9089e-02, -5.3053e-03,  4.8696e-02,\n","                       -2.1168e-02,  1.3937e-02, -3.0782e-02, -6.7283e-02,  1.7809e-02,\n","                        8.0277e-02,  3.3548e-02, -6.9700e-02, -5.6632e-02, -5.5353e-02,\n","                       -7.7131e-02, -5.9035e-02,  8.2308e-02,  4.6642e-02,  7.2341e-02,\n","                        2.7103e-02,  7.1591e-02, -4.7638e-02,  7.6344e-02, -5.0049e-02,\n","                        6.2165e-02, -5.2036e-02, -6.8844e-02, -4.1868e-02, -1.0807e-02,\n","                       -5.1174e-02, -6.1326e-02,  5.8242e-02,  5.9234e-02,  7.2117e-02,\n","                       -4.5132e-02, -8.7965e-02,  6.7440e-02, -2.2329e-02, -8.4696e-02,\n","                        4.1753e-02,  8.7896e-02,  3.1504e-03,  2.2538e-02, -3.9051e-03,\n","                        7.3864e-02,  1.8815e-02,  8.7491e-02, -6.7851e-02, -4.6609e-02,\n","                        4.1054e-02,  7.7934e-02, -6.7374e-02,  7.1372e-02, -7.1702e-02,\n","                       -2.4991e-02, -1.2557e-02, -8.3065e-02, -3.7759e-02, -7.2632e-02,\n","                        3.4345e-02,  5.1612e-02,  5.3952e-02, -2.0990e-02,  4.6590e-02,\n","                        6.8832e-02,  4.2062e-02, -1.9391e-02, -9.7158e-03, -7.2592e-02,\n","                        8.2512e-02,  7.5109e-02,  8.0348e-02,  7.7256e-02,  4.4047e-02,\n","                       -6.0179e-03,  1.7802e-03,  6.2312e-02, -1.2258e-02, -5.3982e-02,\n","                       -2.4083e-02, -7.6164e-02, -3.1791e-02],\n","                      [-3.3884e-02,  8.4392e-02,  6.5147e-02,  7.3706e-02,  6.0991e-03,\n","                       -1.2748e-02,  7.9281e-02,  1.2424e-02,  4.9320e-02, -4.0052e-02,\n","                       -7.3148e-02,  7.1321e-02,  6.2167e-02, -3.7798e-02,  3.7167e-02,\n","                       -4.7976e-02, -8.5729e-02,  1.7883e-02, -8.9630e-03,  3.3412e-02,\n","                       -3.5924e-02, -2.5536e-02,  8.6344e-04, -7.7127e-02,  1.9267e-02,\n","                        9.1836e-04,  8.2997e-02, -5.9514e-02, -8.6179e-03,  3.3463e-02,\n","                       -6.8541e-02,  4.1371e-03,  2.1909e-02,  7.3232e-02,  6.2141e-02,\n","                        2.4073e-02,  8.1936e-03,  8.6924e-02,  8.5718e-03, -7.8046e-03,\n","                        6.9155e-02, -2.7301e-02, -7.4528e-02,  6.8909e-02, -7.6196e-02,\n","                        7.2560e-02,  2.4987e-02, -1.0157e-02,  1.4376e-02,  1.0859e-02,\n","                        8.4612e-02, -4.2340e-02, -4.1148e-02,  6.0829e-02,  8.0744e-03,\n","                       -5.5320e-02,  2.2980e-02, -8.1238e-03, -2.4911e-02,  1.8479e-02,\n","                        1.8665e-02,  3.8556e-03,  7.8043e-02,  6.6409e-02, -4.1562e-02,\n","                        5.0143e-02, -6.0848e-02,  5.6457e-02, -5.7577e-02, -8.2083e-02,\n","                       -6.8908e-02, -3.6307e-02, -5.5097e-02,  1.6693e-02,  6.4939e-02,\n","                        4.1452e-02,  3.1525e-02, -3.4335e-02,  5.2503e-02,  1.7810e-03,\n","                        5.0514e-02,  2.7754e-02,  5.8122e-02, -5.0950e-02, -8.7707e-02,\n","                       -9.1241e-03,  4.3868e-02, -5.1959e-02, -3.8400e-02,  5.3103e-02,\n","                       -3.6146e-03,  7.4879e-02, -3.0598e-03, -2.7947e-02, -6.4758e-02,\n","                        7.6761e-02,  4.8834e-02, -2.4843e-02, -7.8581e-03, -3.4569e-02,\n","                       -3.2308e-02, -3.4362e-02,  5.6784e-04, -5.6473e-02,  1.8076e-03,\n","                       -1.5241e-02, -2.8900e-02,  8.7445e-02,  4.9612e-04, -4.6080e-02,\n","                        2.0062e-02,  7.1799e-02,  6.2714e-02, -1.0335e-02, -4.9610e-02,\n","                        4.2499e-02, -8.5307e-03,  5.1133e-02,  5.4668e-02,  5.5438e-02,\n","                        4.9397e-03, -4.2875e-02, -3.1180e-02, -5.0277e-02, -7.7994e-02,\n","                       -4.3219e-02, -1.9105e-02,  6.2144e-02],\n","                      [ 8.4525e-02,  3.4284e-02,  5.4638e-02, -5.9788e-02,  6.7538e-02,\n","                       -3.6144e-02,  4.9388e-02,  3.5589e-03, -1.0314e-02,  4.1713e-02,\n","                       -2.2120e-02, -8.5627e-03, -6.7979e-02,  1.7307e-02,  8.4746e-03,\n","                        1.4107e-02,  5.2599e-02,  8.0562e-02,  2.0904e-02,  4.3522e-02,\n","                        1.8806e-02, -3.0714e-02, -5.3178e-02, -5.6442e-03, -4.3120e-02,\n","                       -6.3429e-02,  7.8742e-02, -2.1324e-02,  8.6744e-02, -4.4332e-03,\n","                       -2.0102e-02,  2.6370e-02, -2.2343e-02,  4.5077e-02,  6.6521e-02,\n","                        2.1491e-02, -4.0504e-02, -2.3716e-02, -7.6983e-02, -6.9327e-02,\n","                        7.0361e-02, -6.1550e-02, -9.2637e-03,  2.2159e-03,  3.1256e-02,\n","                        5.6628e-02,  3.2244e-02, -4.6180e-02,  8.2946e-02, -3.1260e-02,\n","                        9.9612e-03,  4.1622e-02, -4.0768e-03,  6.8035e-02, -5.6795e-02,\n","                        8.2747e-02,  1.5516e-02, -6.9629e-02, -3.6358e-02, -6.2820e-02,\n","                        1.7231e-02, -1.3769e-02,  6.1702e-02, -1.5743e-02,  2.1519e-02,\n","                        5.8224e-03, -1.3031e-02,  3.0373e-02, -6.5154e-03,  7.9181e-02,\n","                       -5.8301e-02, -8.0587e-02,  6.5652e-02,  8.3105e-02, -4.7373e-02,\n","                        3.9844e-02,  6.2072e-02,  8.3686e-02,  6.3303e-02,  2.0344e-03,\n","                        8.6883e-02,  5.8206e-02, -8.6502e-02, -7.1388e-02, -4.6185e-02,\n","                        4.1337e-02,  4.8634e-02, -8.0544e-02,  4.8039e-02, -5.3625e-02,\n","                       -6.2220e-03, -3.7965e-02, -1.0632e-02,  3.9958e-02, -3.8098e-02,\n","                       -2.4869e-02, -5.9035e-02, -7.5857e-02, -1.3465e-02,  5.4129e-02,\n","                        8.3415e-02, -7.1967e-03, -3.6890e-02, -7.2424e-02,  6.5420e-02,\n","                        1.9181e-02,  4.3825e-02, -1.8820e-03, -2.6353e-02,  4.0127e-02,\n","                       -7.7077e-02,  2.8778e-02,  5.6484e-02,  1.2427e-02, -6.4364e-02,\n","                       -5.0394e-02,  6.8578e-04,  1.1081e-02, -2.1776e-02,  2.5853e-02,\n","                       -2.6509e-02,  4.3796e-02, -7.7149e-02, -2.9923e-02, -8.6989e-02,\n","                        2.2317e-02, -7.4012e-02, -2.4076e-02]], device='cuda:0')),\n","             ('_tasks.0._affine.bias',\n","              tensor([-0.0518,  0.0180, -0.0118], device='cuda:0'))])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model_large.state_dict()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model_large_mapped = deepcopy(model_large)\n","model_large_mapped.load_state_dict(model_large_mapped_state_dict)"]},{"cell_type":"markdown","metadata":{},"source":["## Inference & Evaluation\n","\n","With a trained model loaded into memory, we can now apply the model to batch_51. The following cells will start inference (or load in a csv with predictions, if you're in a hurry) and plot the results. "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T08:46:55.299249Z","iopub.status.busy":"2023-02-03T08:46:55.298831Z","iopub.status.idle":"2023-02-03T08:46:55.307103Z","shell.execute_reply":"2023-02-03T08:46:55.306088Z","shell.execute_reply.started":"2023-02-03T08:46:55.299205Z"},"trusted":true},"outputs":[],"source":["def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n","    df['true_x'] = np.cos(df['azimuth']) * np.sin(df['zenith'])\n","    df['true_y'] = np.sin(df['azimuth'])*np.sin(df['zenith'])\n","    df['true_z'] = np.cos(df['zenith'])\n","    return df\n","\n","def calculate_angular_error(df : pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n","    df['angular_error'] = np.arccos(df['true_x']*df['direction_x'] + df['true_y']*df['direction_y'] + df['true_z']*df['direction_z'])\n","    return df"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# %%capture\n","# df = calculate_angular_error(convert_to_3d(inference(model_small, config_small))) \n","# df.to_hdf('small.h5', key='df', mode='w')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Predicting DataLoader 0:  20%|        | 1355/6667 [02:12<08:40, 10.20it/s]"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.04 GiB (GPU 0; 16.00 GiB total capacity; 12.76 GiB already allocated; 0 bytes free; 14.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m calculate_angular_error(convert_to_3d(inference(model_large, config_large)))\n\u001b[1;32m      2\u001b[0m df\u001b[39m.\u001b[39mto_hdf(\u001b[39m'\u001b[39m\u001b[39mlarge.h5\u001b[39m\u001b[39m'\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdf\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m/workspace/icecube/icecube_utils.py:163\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    149\u001b[0m test_dataloader \u001b[39m=\u001b[39m make_dataloader(db \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39minference_database_path\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    150\u001b[0m                                         selection \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m# Entire database\u001b[39;00m\n\u001b[1;32m    151\u001b[0m                                         pulsemaps \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mpulsemap\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m                                         truth_table \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mtruth_table\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    160\u001b[0m                                         )\n\u001b[1;32m    162\u001b[0m \u001b[39m# Get predictions\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_as_dataframe(\n\u001b[1;32m    164\u001b[0m     gpus \u001b[39m=\u001b[39;49m [\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    165\u001b[0m     dataloader \u001b[39m=\u001b[39;49m test_dataloader,\n\u001b[1;32m    166\u001b[0m     prediction_columns\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mprediction_columns,\n\u001b[1;32m    167\u001b[0m     additional_attributes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49madditional_attributes,\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    169\u001b[0m \u001b[39m# Save predictions and model to file\u001b[39;00m\n\u001b[1;32m    170\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(config[\u001b[39m'\u001b[39m\u001b[39mbase_dir\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mtrain_model_without_configs\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/model.py:181\u001b[0m, in \u001b[0;36mModel.predict_as_dataframe\u001b[0;34m(self, dataloader, prediction_columns, node_level, additional_attributes, index_column, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[39mprint\u001b[39m(dataloader\u001b[39m.\u001b[39msampler)\n\u001b[1;32m    172\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mUserWarning\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataLoader has a `sampler` that is not `SequentialSampler`, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mindicating that shuffling is enabled. Using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`additional_attributes`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m     )\n\u001b[0;32m--> 181\u001b[0m predictions_torch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    182\u001b[0m     dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[1;32m    183\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    184\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mdistribution_strategy,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m predictions \u001b[39m=\u001b[39m (\n\u001b[1;32m    187\u001b[0m     torch\u001b[39m.\u001b[39mcat(predictions_torch, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(prediction_columns) \u001b[39m==\u001b[39m predictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], (\n\u001b[1;32m    190\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of provided column names (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(prediction_columns)\u001b[39m}\u001b[39;00m\u001b[39m) and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumber of output columns (\u001b[39m\u001b[39m{\u001b[39;00mpredictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m) don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m )\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/standard_model.py:173\u001b[0m, in \u001b[0;36mStandardModel.predict\u001b[0;34m(self, dataloader, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return predictions for `dataloader`.\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minference()\n\u001b[0;32m--> 173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    174\u001b[0m     dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[1;32m    175\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    176\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mdistribution_strategy,\n\u001b[1;32m    177\u001b[0m )\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/model.py:135\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, dataloader, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melif\u001b[39;00m gpus \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA `Trainer` instance has already been constructed, possibly \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwhen the model was trained. Will use this to get predictions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument `gpus = \u001b[39m\u001b[39m{\u001b[39;00mgpus\u001b[39m}\u001b[39;00m\u001b[39m` will be ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[0;32m--> 135\u001b[0m predictions_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_trainer\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m, dataloader)\n\u001b[1;32m    136\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(predictions_list), \u001b[39m\"\u001b[39m\u001b[39mGot no predictions\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m nb_outputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(predictions_list[\u001b[39m0\u001b[39m])\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:892\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    890\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    891\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 892\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    893\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    894\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:938\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    934\u001b[0m )\n\u001b[1;32m    936\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predicted_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    940\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1190\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_evaluate()\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_predict()\n\u001b[1;32m   1191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_train()\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1244\u001b[0m, in \u001b[0;36mTrainer._run_predict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1243\u001b[0m \u001b[39mwith\u001b[39;00m _evaluation_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_mode):\n\u001b[0;32m-> 1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/prediction_loop.py:100\u001b[0m, in \u001b[0;36mPredictionLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m dataloader_iter \u001b[39m=\u001b[39m \u001b[39menumerate\u001b[39m(dataloader)\n\u001b[1;32m     98\u001b[0m dl_max_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_batches[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_dataloader_idx]\n\u001b[0;32m--> 100\u001b[0m dl_predictions, dl_batch_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    101\u001b[0m     dataloader_iter, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_dataloader_idx, dl_max_batches, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_dataloaders\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictions\u001b[39m.\u001b[39mappend(dl_predictions)\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_batch_indices\u001b[39m.\u001b[39mappend(dl_batch_indices)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/prediction_epoch_loop.py:100\u001b[0m, in \u001b[0;36mPredictionEpochLoop.advance\u001b[0;34m(self, dataloader_iter, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m     96\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook(\u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, batch, dataloader_idx\u001b[39m=\u001b[39mdataloader_idx)\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m--> 100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/prediction_epoch_loop.py:129\u001b[0m, in \u001b[0;36mPredictionEpochLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_predict_batch_start\u001b[39m\u001b[39m\"\u001b[39m, batch, batch_idx, dataloader_idx)\n\u001b[1;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m--> 129\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:408\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpredict_step_context():\n\u001b[1;32m    407\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, PredictStep)\n\u001b[0;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1243\u001b[0m, in \u001b[0;36mLightningModule.predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_step\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, batch_idx: \u001b[39mint\u001b[39m, dataloader_idx: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1209\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[39m    calls :meth:`~pytorch_lightning.core.module.LightningModule.forward`. Override to add any processing logic.\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[39m        Predicted output\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/standard_model.py:92\u001b[0m, in \u001b[0;36mStandardModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coarsening(data)\n\u001b[1;32m     91\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detector(data)\n\u001b[0;32m---> 92\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gnn(data)\n\u001b[1;32m     93\u001b[0m preds \u001b[39m=\u001b[39m [task(x) \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks]\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m preds\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/gnn/dynedge.py:303\u001b[0m, in \u001b[0;36mDynEdge.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    301\u001b[0m skip_connections \u001b[39m=\u001b[39m [x]\n\u001b[1;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m conv_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_layers:\n\u001b[0;32m--> 303\u001b[0m     x, edge_index \u001b[39m=\u001b[39m conv_layer(x, edge_index, batch)\n\u001b[1;32m    304\u001b[0m     skip_connections\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    306\u001b[0m \u001b[39m# Skip-cat\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/workspace/icecube/lib/graphnet/src/graphnet/models/components/layers.py:52\u001b[0m, in \u001b[0;36mDynEdgeConv.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward pass.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Standard EdgeConv forward pass\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m x \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x, edge_index)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Recompute adjacency\u001b[39;00m\n\u001b[1;32m     55\u001b[0m edge_index \u001b[39m=\u001b[39m knn_graph(\n\u001b[1;32m     56\u001b[0m     x\u001b[39m=\u001b[39mx[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_subset],\n\u001b[1;32m     57\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_neighbors,\n\u001b[1;32m     58\u001b[0m     batch\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m     59\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch_geometric/nn/conv/edge_conv.py:61\u001b[0m, in \u001b[0;36mEdgeConv.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m     x: PairTensor \u001b[39m=\u001b[39m (x, x)\n\u001b[1;32m     60\u001b[0m \u001b[39m# propagate_type: (x: PairTensor)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch_geometric/nn/conv/message_passing.py:317\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 317\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    318\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    319\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch_geometric/nn/conv/edge_conv.py:64\u001b[0m, in \u001b[0;36mEdgeConv.message\u001b[0;34m(self, x_i, x_j)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmessage\u001b[39m(\u001b[39mself\u001b[39m, x_i: Tensor, x_j: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn(torch\u001b[39m.\u001b[39;49mcat([x_i, x_j \u001b[39m-\u001b[39;49m x_i], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/modules/activation.py:775\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mleaky_relu(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnegative_slope, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[0;32m~/miniconda3/envs/graphnet/lib/python3.8/site-packages/torch/nn/functional.py:1632\u001b[0m, in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mleaky_relu_(\u001b[39minput\u001b[39m, negative_slope)\n\u001b[1;32m   1631\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1632\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mleaky_relu(\u001b[39minput\u001b[39;49m, negative_slope)\n\u001b[1;32m   1633\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.04 GiB (GPU 0; 16.00 GiB total capacity; 12.76 GiB already allocated; 0 bytes free; 14.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["%%capture --no-stdout\n","df = calculate_angular_error(convert_to_3d(inference(model_large, config_large)))\n","df.to_hdf('large.h5', key='df', mode='w')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = calculate_angular_error(convert_to_3d(inference(model_large_mapped, config_large)))\n","df.to_hdf('large_mapped.h5', key='df', mode='w')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = {\n","    'small': pd.read_hdf('small.h5', key='df'),\n","    'large': pd.read_hdf('large.h5', key='df'),\n","    'large_mapped': pd.read_hdf('large_mapped.h5', key='df'),\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T08:56:19.735528Z","iopub.status.busy":"2023-02-03T08:56:19.735174Z","iopub.status.idle":"2023-02-03T08:56:19.981355Z","shell.execute_reply":"2023-02-03T08:56:19.980402Z","shell.execute_reply.started":"2023-02-03T08:56:19.735497Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7f6ee1b5e700>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 600x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["for name, result in results.items():\n","    fig = plt.figure(figsize = (6,6))\n","    plt.hist(result['angular_error'], \n","            bins = np.arange(0,np.pi*2, 0.05), \n","            histtype = 'step', \n","            label = f'mean angular error: {np.round(result[\"angular_error\"].mean(),2)}')\n","    plt.xlabel('Angular Error [rad.]', size = 15)\n","    plt.ylabel('Counts', size = 15)\n","    plt.title(f'{name}, Angular Error Distribution (Batch 51)', size = 15)\n","    plt.legend(frameon = False, fontsize = 15)"]},{"cell_type":"markdown","metadata":{},"source":["So the pre-trained dynedge seems to perform quite well. Another interesting feature of the reconstruction is that dynedge (when coupled with the[ DirectionReconstructionWithKappa](https://github.com/graphnet-team/graphnet/blob/7e857562898ebebebc9a105159fd3d4eb4994aea/src/graphnet/models/task/reconstruction.py#L45) is that dynedge estimated *kappa* the concentration parameter from the vonMisesFisher distribution. Kappa is analogus to sigma via sigma = 1/sqrt(kappa), and the quality of the direction estimate should be highly correlated with this parameter. "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-02-03T08:56:24.976250Z","iopub.status.busy":"2023-02-03T08:56:24.975565Z","iopub.status.idle":"2023-02-03T08:56:25.236386Z","shell.execute_reply":"2023-02-03T08:56:25.235418Z","shell.execute_reply.started":"2023-02-03T08:56:24.976211Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7f6f87ca58e0>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 600x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["cut_threshold = 0.5\n","for name, result in results.items():\n","    fig = plt.figure(figsize = (6,6))\n","    plt.hist(result['angular_error'][1/np.sqrt(result['direction_kappa']) <= cut_threshold], \n","            bins = np.arange(0,np.pi*2, 0.05), \n","            histtype = 'step', \n","            label = f'sigma <= {cut_threshold}: {np.round(result[\"angular_error\"][1/np.sqrt(result[\"direction_kappa\"]) <= cut_threshold].mean(),2)}')\n","\n","    plt.hist(result['angular_error'][1/np.sqrt(result['direction_kappa']) > cut_threshold], \n","            bins = np.arange(0,np.pi*2, 0.05), \n","            histtype = 'step', \n","            label = f'sigma > {cut_threshold}: {np.round(result[\"angular_error\"][1/np.sqrt(result[\"direction_kappa\"]) > cut_threshold].mean(),2)}')\n","    plt.xlabel('Angular Error [rad.]', size = 15)\n","    plt.ylabel('Counts', size = 15)\n","    plt.title(f'{name}, Angular Error Distribution (Batch 51)', size = 15)\n","    plt.legend(frameon = False, fontsize = 15)"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, the variable can be used to distinguish \"good\" and \"bad\" reconstructions with some confidence. "]},{"cell_type":"markdown","metadata":{},"source":["## A few hints for your neutrino data science journey!\n","\n","* The configuration of dynedge shown in this notebook is the so-called \"baseline\". It's not optimized for high energy neutrinos, so you might be able to squeeze out a bit more performance by tuning hyperparameters or making larger modifications; such as switching out the learning rate scheduler or choosing a different loss function, etc.\n","\n","* You can use the kappa variable to group events into different categories. Perhaps training a seperate reconstruction method for each performs better?\n","\n","* You may want to adjust the [ParquetDataset](https://github.com/graphnet-team/graphnet/blob/7e857562898ebebebc9a105159fd3d4eb4994aea/src/graphnet/data/parquet/parquet_dataset.py#L11) such that it works with the competition data. This would allow you to train / infer directly on the competition files (No conversion to sqlite needed). Feel free to contribute this to the repository!\n","\n","\n","Good luck!"]}],"metadata":{"kernelspec":{"display_name":"graphnet","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"8bdc725fb65c21f82e45edbbae76c938f412b1b7259c22cf88bbbf1e62e294f2"}}},"nbformat":4,"nbformat_minor":4}
