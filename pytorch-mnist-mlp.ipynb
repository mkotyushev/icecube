{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify MNIST digits using **PyTorch**. \n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.0+cu117  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data.  First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
    "    plt.title('Class: '+str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=50, weights=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden_size, bias=False)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10, bias=False)\n",
    "\n",
    "        if weights is not None:\n",
    "            assert len(weights) == 3\n",
    "            with torch.no_grad():\n",
    "                self.fc1.weight = nn.Parameter(weights[0])\n",
    "                self.fc2.weight = nn.Parameter(weights[1])\n",
    "                self.fc3.weight = nn.Parameter(weights[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        diff_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (first_model_param_name, first_model_param), \\\n",
    "                (second_model_param_name, second_model_param), \\\n",
    "                (diff_model_param_name, diff_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                other.named_parameters(),\n",
    "                diff_model.named_parameters()\n",
    "            ):\n",
    "                assert first_model_param_name == second_model_param_name == diff_model_param_name\n",
    "                assert first_model_param.shape == second_model_param.shape == diff_model_param.shape\n",
    "                diff_model_param.copy_(\n",
    "                    first_model_param +\n",
    "                    second_model_param\n",
    "                )\n",
    "        return diff_model\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        diff_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (first_model_param_name, first_model_param), \\\n",
    "                (second_model_param_name, second_model_param), \\\n",
    "                (diff_model_param_name, diff_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                other.named_parameters(),\n",
    "                diff_model.named_parameters()\n",
    "            ):\n",
    "                assert first_model_param_name == second_model_param_name == diff_model_param_name\n",
    "                assert first_model_param.shape == second_model_param.shape == diff_model_param.shape\n",
    "                diff_model_param.copy_(\n",
    "                    first_model_param -\n",
    "                    second_model_param\n",
    "                )\n",
    "        return diff_model\n",
    "\n",
    "    def __mul__(self, scalar):\n",
    "        result_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (_, model_param), \\\n",
    "                (_, result_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                result_model.named_parameters()\n",
    "            ):\n",
    "                result_model_param.copy_(\n",
    "                    model_param * scalar\n",
    "                )\n",
    "        return result_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net().to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# epochs = 10\n",
    "\n",
    "# lossv, accv = [], []\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(epoch)\n",
    "#     validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize how the training progressed. \n",
    "\n",
    "* *Loss* is a function of the difference of the network output and the target values.  We are minimizing the loss function during training so it should decrease over time.\n",
    "* *Accuracy* is the classification accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,3))\n",
    "# plt.plot(np.arange(1,epochs+1), lossv)\n",
    "# plt.title('validation loss')\n",
    "\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.plot(np.arange(1,epochs+1), accv)\n",
    "# plt.title('validation accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Modify the MLP model.  Try to improve the classification accuracy, or experiment with the effects of different parameters.  If you are interested in the state-of-the-art performance on permutation invariant MNIST, see e.g. this [recent paper](https://arxiv.org/abs/1507.02672) by Aalto University / The Curious AI Company researchers.\n",
    "\n",
    "You can also consult the PyTorch documentation at http://pytorch.org/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multiple MLPs with different hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305997\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.249061\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.664477\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.171325\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.718018\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.618686\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.072273\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.515163\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.633971\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.565271\n",
      "\n",
      "Validation set: Average loss: 0.4023, Accuracy: 8905/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.532290\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.652178\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.917626\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.464457\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.657096\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.624577\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.371708\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.202223\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.647639\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.385363\n",
      "\n",
      "Validation set: Average loss: 0.2834, Accuracy: 9194/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.585235\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.418978\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.650436\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.287857\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.764325\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.253303\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.525358\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.341244\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.706483\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.479829\n",
      "\n",
      "Validation set: Average loss: 0.2426, Accuracy: 9281/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.435174\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.331961\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.350902\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.485793\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.119117\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.348462\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.287284\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.375787\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.122142\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.399589\n",
      "\n",
      "Validation set: Average loss: 0.2113, Accuracy: 9384/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.414766\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.406750\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.363859\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.211660\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.356059\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.189301\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.358774\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.419176\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.490958\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.576385\n",
      "\n",
      "Validation set: Average loss: 0.1924, Accuracy: 9445/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.519507\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.162950\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.227209\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.428827\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.291234\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.381631\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.368735\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.438466\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.203178\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.061586\n",
      "\n",
      "Validation set: Average loss: 0.1791, Accuracy: 9471/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.274889\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.172660\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.128690\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.267137\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.522058\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.192036\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.408561\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.641697\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.431823\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.508775\n",
      "\n",
      "Validation set: Average loss: 0.1691, Accuracy: 9473/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.343489\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.225402\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.122435\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.324487\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.159670\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.313211\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.676045\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.352997\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.306340\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.131618\n",
      "\n",
      "Validation set: Average loss: 0.1609, Accuracy: 9510/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.192095\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.173942\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.121812\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.206129\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.374742\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.258183\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.167299\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.260951\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.333029\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.264975\n",
      "\n",
      "Validation set: Average loss: 0.1555, Accuracy: 9539/10000 (95%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.191081\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.561779\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.257636\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.514012\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.642323\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.146080\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.184358\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.198379\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.391277\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.259874\n",
      "\n",
      "Validation set: Average loss: 0.1503, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307827\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.179960\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.677487\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.276410\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.695211\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.830000\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.941468\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.868696\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.477314\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.748937\n",
      "\n",
      "Validation set: Average loss: 0.4047, Accuracy: 8906/10000 (89%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301214\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.045879\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.314692\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.809003\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.605779\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.790208\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.584508\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.477702\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.382506\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.376265\n",
      "\n",
      "Validation set: Average loss: 0.3439, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.404509\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.177644\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.423579\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.532312\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.620243\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.335248\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.432404\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.696573\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.299074\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.355489\n",
      "\n",
      "Validation set: Average loss: 0.2406, Accuracy: 9308/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.284657\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.313574\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.658066\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.525884\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.151681\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.178921\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.248743\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.357106\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.308425\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.439887\n",
      "\n",
      "Validation set: Average loss: 0.1986, Accuracy: 9419/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.189054\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.260239\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.188492\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.343786\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.268453\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.189472\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.200350\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.098673\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.115769\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.181773\n",
      "\n",
      "Validation set: Average loss: 0.1729, Accuracy: 9485/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.604817\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.467698\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.093047\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.285473\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.113432\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.292380\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.144900\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.096329\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.128656\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.213036\n",
      "\n",
      "Validation set: Average loss: 0.1520, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.113152\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.116232\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.242340\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.141708\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.076089\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.220526\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.333081\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.222721\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.220188\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.790015\n",
      "\n",
      "Validation set: Average loss: 0.1384, Accuracy: 9575/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.133834\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.297828\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.223984\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.036799\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.287438\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.119744\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.079889\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.129099\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.038752\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.046347\n",
      "\n",
      "Validation set: Average loss: 0.1298, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.316011\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.250285\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.190198\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.149675\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.178395\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.156080\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.100183\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.021059\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.049118\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.162035\n",
      "\n",
      "Validation set: Average loss: 0.1208, Accuracy: 9628/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.262666\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.079391\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.101988\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.062805\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.240195\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.333263\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.117131\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.183617\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.026005\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.401871\n",
      "\n",
      "Validation set: Average loss: 0.1218, Accuracy: 9639/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.095393\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.088745\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.033094\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.072452\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.055381\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.073412\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.062955\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.019533\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.171738\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.018178\n",
      "\n",
      "Validation set: Average loss: 0.1128, Accuracy: 9666/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303000\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.136733\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.206562\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.778968\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.862325\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.619744\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.715654\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.815563\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.322568\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.226454\n",
      "\n",
      "Validation set: Average loss: 0.3470, Accuracy: 9000/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS_SHORT = 1\n",
    "N_EPOCHS = 10\n",
    "N_MODELS = 2\n",
    "models = defaultdict(dict)\n",
    "for i in range(N_MODELS):\n",
    "    model = Net(hidden_size=30 * (i + 1)).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    models['untrained'][i] = copy.deepcopy(model).to('cpu')\n",
    "\n",
    "    lossv, accv = [], []\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        train(model, optimizer, criterion, epoch)\n",
    "        validate(model, criterion, lossv, accv)\n",
    "    \n",
    "    models['trained'][i] = model.to('cpu')\n",
    "\n",
    "    model = copy.deepcopy(models['untrained'][i]).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lossv, accv = [], []\n",
    "    for epoch in range(1, N_EPOCHS_SHORT + 1):\n",
    "        train(model, optimizer, criterion, epoch)\n",
    "        validate(model, criterion, lossv, accv)\n",
    "    \n",
    "    models['trained_short'][i] = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24720\n",
      "\n",
      "Validation set: Average loss: 2.3045, Accuracy: 871/10000 (9%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1503, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "51240\n",
      "\n",
      "Validation set: Average loss: 2.3043, Accuracy: 753/10000 (8%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1128, Accuracy: 9666/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_MODELS):\n",
    "    print(get_n_params(models['untrained'][i]))\n",
    "    validate(models['untrained'][i].to(device), criterion, [], [])\n",
    "    validate(models['trained'][i].to(device), criterion, [], [])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OT fusion to map smaller model to bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wasserstein_ensemble import get_wassersteinized_layers_modularized\n",
    "from parameters import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args = parser.parse_args('--gpu-id 1 --model-name mlpnet --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --exact --correction --ground-metric euclidean --weight-stats \\\n",
    "--activation-histograms --activation-mode raw --geom-ensemble-type acts --sweep-id 21 \\\n",
    "--act-num-samples 200 --ground-metric-normalize none --activation-seed 21 \\\n",
    "--prelu-acts --recheck-acc --load-models ./mnist_models --ckpt-type final \\\n",
    "--past-correction --not-squared --dist-normalize --print-distances --to-download'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.gpu_id = 0\n",
    "args.proper_marginals = True\n",
    "args.skip_last_layer = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic check with two trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0167],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.999637603759766 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.3408, 2.1974, 2.2829,  ..., 2.0500, 2.2092, 2.1877],\n",
      "        [2.0728, 2.1382, 2.3127,  ..., 2.0190, 2.1717, 2.4235],\n",
      "        [2.2759, 2.1885, 1.8003,  ..., 2.4965, 2.2957, 2.1979],\n",
      "        ...,\n",
      "        [2.1463, 2.3094, 1.6542,  ..., 2.3618, 2.1374, 2.1282],\n",
      "        [2.4199, 2.2944, 2.4653,  ..., 2.0897, 2.5399, 2.2458],\n",
      "        [2.2071, 2.5100, 2.3790,  ..., 2.2044, 2.2837, 2.6001]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.9226, 4.3090, 3.6802, 3.9583, 4.2312, 3.6796, 3.8025, 3.6663, 3.9324,\n",
      "         4.0999],\n",
      "        [5.0113, 3.0335, 4.3636, 4.4747, 4.5928, 4.7840, 4.6358, 4.1548, 4.3864,\n",
      "         4.7005],\n",
      "        [3.6329, 3.8209, 2.1290, 3.4469, 4.0255, 4.1743, 3.4312, 3.4711, 3.5293,\n",
      "         4.5883],\n",
      "        [4.1136, 3.8612, 3.3664, 2.3725, 4.3728, 3.5402, 4.3439, 3.6698, 4.1311,\n",
      "         3.5604],\n",
      "        [4.0744, 4.5320, 4.2914, 4.5777, 2.3361, 4.1385, 3.8454, 4.1614, 4.2329,\n",
      "         3.8914],\n",
      "        [3.4452, 3.6401, 4.1864, 3.5971, 4.0704, 2.1386, 3.4946, 4.1848, 3.7402,\n",
      "         3.6713],\n",
      "        [3.7107, 4.2423, 4.3653, 4.8715, 4.1898, 3.6521, 3.1570, 4.8500, 4.0268,\n",
      "         4.7728],\n",
      "        [4.0900, 3.8675, 3.3940, 3.5530, 3.7862, 4.5918, 4.4981, 2.4080, 4.1840,\n",
      "         4.0152],\n",
      "        [4.0709, 3.8132, 4.0921, 3.9593, 4.4401, 3.6599, 3.6798, 4.6083, 2.7985,\n",
      "         4.2432],\n",
      "        [4.2925, 4.4350, 4.2801, 4.0892, 3.8161, 4.2981, 4.5554, 3.8873, 3.8913,\n",
      "         2.6029]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "mu shape is (10,)\n",
      "nu shape is (10,)\n",
      "cpuM shape is (10, 10)\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n"
     ]
    }
   ],
   "source": [
    "_, aligned, _ = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60, 784]), torch.Size([60, 60]), torch.Size([10, 60]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned[0].shape, aligned[1].shape, aligned[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 2.3010, Accuracy: 1106/10000 (11%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.3554, Accuracy: 9542/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(30)\n",
    "validate(model.to(device), criterion, [], [])\n",
    "with torch.no_grad():\n",
    "    model.fc1.weight = nn.Parameter(aligned[0])\n",
    "    model.fc2.weight = nn.Parameter(aligned[1])\n",
    "    model.fc3.weight = nn.Parameter(aligned[2])\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0197,  0.0026, -0.0086,  ...,  0.0139,  0.0211, -0.0080],\n",
       "        [ 0.0229,  0.0041, -0.0075,  ...,  0.0052, -0.0145, -0.0315],\n",
       "        [-0.0270, -0.0153, -0.0229,  ..., -0.0144,  0.0126,  0.0059],\n",
       "        ...,\n",
       "        [-0.0269,  0.0328,  0.0289,  ..., -0.0211,  0.0264, -0.0116],\n",
       "        [-0.0291, -0.0253,  0.0140,  ...,  0.0342,  0.0201, -0.0349],\n",
       "        [ 0.0026,  0.0251, -0.0321,  ..., -0.0235,  0.0355,  0.0336]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['trained'][0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0080,  0.0211,  0.0319,  ..., -0.0145,  0.0116, -0.0025],\n",
       "        [ 0.0028,  0.0111, -0.0079,  ...,  0.0042, -0.0204, -0.0327],\n",
       "        [-0.0170,  0.0091, -0.0306,  ...,  0.0275,  0.0010,  0.0322],\n",
       "        ...,\n",
       "        [-0.0356,  0.0190, -0.0090,  ..., -0.0244,  0.0352, -0.0265],\n",
       "        [ 0.0356, -0.0008,  0.0066,  ...,  0.0232, -0.0141, -0.0236],\n",
       "        [ 0.0015, -0.0011,  0.0300,  ..., -0.0357,  0.0242, -0.0193]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['trained'][1].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0306, -0.0144,  0.0178,  ...,  0.0047,  0.0271, -0.0340],\n",
       "        [-0.0040,  0.0267, -0.0113,  ...,  0.0215, -0.0124,  0.0213],\n",
       "        [ 0.0044, -0.0259,  0.0275,  ...,  0.0112, -0.0187,  0.0122],\n",
       "        ...,\n",
       "        [ 0.0197,  0.0026, -0.0086,  ...,  0.0139,  0.0211, -0.0080],\n",
       "        [-0.0270, -0.0153, -0.0229,  ..., -0.0144,  0.0126,  0.0059],\n",
       "        [ 0.0229,  0.0041, -0.0075,  ...,  0.0052, -0.0145, -0.0315]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Ts are or to be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0167],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.999637603759766 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.3408, 2.1974, 2.2829,  ..., 2.0500, 2.2092, 2.1877],\n",
      "        [2.0728, 2.1382, 2.3127,  ..., 2.0190, 2.1717, 2.4235],\n",
      "        [2.2759, 2.1885, 1.8003,  ..., 2.4965, 2.2957, 2.1979],\n",
      "        ...,\n",
      "        [2.1463, 2.3094, 1.6542,  ..., 2.3618, 2.1374, 2.1282],\n",
      "        [2.4199, 2.2944, 2.4653,  ..., 2.0897, 2.5399, 2.2458],\n",
      "        [2.2071, 2.5100, 2.3790,  ..., 2.2044, 2.2837, 2.6001]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.9226, 4.3090, 3.6802, 3.9583, 4.2312, 3.6796, 3.8025, 3.6663, 3.9324,\n",
      "         4.0999],\n",
      "        [5.0113, 3.0335, 4.3636, 4.4747, 4.5928, 4.7840, 4.6358, 4.1548, 4.3864,\n",
      "         4.7005],\n",
      "        [3.6329, 3.8209, 2.1290, 3.4469, 4.0255, 4.1743, 3.4312, 3.4711, 3.5293,\n",
      "         4.5883],\n",
      "        [4.1136, 3.8612, 3.3664, 2.3725, 4.3728, 3.5402, 4.3439, 3.6698, 4.1311,\n",
      "         3.5604],\n",
      "        [4.0744, 4.5320, 4.2914, 4.5777, 2.3361, 4.1385, 3.8454, 4.1614, 4.2329,\n",
      "         3.8914],\n",
      "        [3.4452, 3.6401, 4.1864, 3.5971, 4.0704, 2.1386, 3.4946, 4.1848, 3.7402,\n",
      "         3.6713],\n",
      "        [3.7107, 4.2423, 4.3653, 4.8715, 4.1898, 3.6521, 3.1570, 4.8500, 4.0268,\n",
      "         4.7728],\n",
      "        [4.0900, 3.8675, 3.3940, 3.5530, 3.7862, 4.5918, 4.4981, 2.4080, 4.1840,\n",
      "         4.0152],\n",
      "        [4.0709, 3.8132, 4.0921, 3.9593, 4.4401, 3.6599, 3.6798, 4.6083, 2.7985,\n",
      "         4.2432],\n",
      "        [4.2925, 4.4350, 4.2801, 4.0892, 3.8161, 4.2981, 4.5554, 3.8873, 3.8913,\n",
      "         2.6029]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "mu shape is (10,)\n",
      "nu shape is (10,)\n",
      "cpuM shape is (10, 10)\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trace(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m _, aligned1, T_vars \u001b[39m=\u001b[39m get_wassersteinized_layers_modularized(args, [models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]])\n\u001b[0;32m----> 2\u001b[0m _, aligned2, _ \u001b[39m=\u001b[39m get_wassersteinized_layers_modularized(args, [models[\u001b[39m'\u001b[39;49m\u001b[39mtrained\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m], models[\u001b[39m'\u001b[39;49m\u001b[39mtrained\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m1\u001b[39;49m]], T_vars_pre_computed\u001b[39m=\u001b[39;49mT_vars)\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m a1, a2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(aligned1, aligned2):\n",
      "File \u001b[0;32m/workspace/icecube/lib/otfusion/wasserstein_ensemble.py:242\u001b[0m, in \u001b[0;36mget_wassersteinized_layers_modularized\u001b[0;34m(args, networks, T_vars_pre_computed, activations, eps, test_loader)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mthere goes the transport map at layer \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(idx), T_var)\n\u001b[1;32m    240\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRatio of trace to the matrix sum: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mtrace(T_var) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(T_var))\n\u001b[0;32m--> 242\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRatio of trace to the matrix sum: \u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39;49mtrace(T_var) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(T_var))\n\u001b[1;32m    243\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHere, trace is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and matrix sum is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(torch\u001b[39m.\u001b[39mtrace(T_var), torch\u001b[39m.\u001b[39msum(T_var)))\n\u001b[1;32m    244\u001b[0m \u001b[39msetattr\u001b[39m(args, \u001b[39m'\u001b[39m\u001b[39mtrace_sum_ratio_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(layer0_name), (torch\u001b[39m.\u001b[39mtrace(T_var) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39msum(T_var))\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mTypeError\u001b[0m: trace(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "_, aligned1, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])\n",
    "_, aligned2, _ = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]], T_vars_pre_computed=T_vars)\n",
    "with torch.no_grad():\n",
    "    for a1, a2 in zip(aligned1, aligned2):\n",
    "        assert np.allclose(a1.cpu().numpy(), a2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, -1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['untrained'][0].fc1.weight.get_device(), \\\n",
    "models['trained'][0].fc1.weight.get_device(), \\\n",
    "models['trained_short'][0].fc1.weight.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.9999878406524658 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.1071, 1.0789, 0.9867,  ..., 0.9548, 0.9456, 0.8918],\n",
      "        [0.9978, 1.0279, 0.9313,  ..., 0.9901, 0.9569, 0.9938],\n",
      "        [1.1027, 1.1420, 1.0611,  ..., 0.9338, 1.0837, 1.0432],\n",
      "        ...,\n",
      "        [0.9082, 0.9972, 1.0327,  ..., 0.8433, 1.0129, 1.0285],\n",
      "        [0.9546, 1.0253, 1.0130,  ..., 1.0359, 0.9792, 1.0109],\n",
      "        [1.0474, 1.0200, 0.9745,  ..., 0.9947, 1.0816, 1.0257]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9594, 1.0908, 0.9898, 0.9375, 1.0053, 0.8828, 0.9551, 1.0300, 0.9105,\n",
      "         1.0513],\n",
      "        [1.0849, 1.1084, 1.0947, 1.0910, 1.0643, 1.1344, 1.0093, 1.1063, 0.9429,\n",
      "         1.0754],\n",
      "        [1.0486, 1.0121, 1.0174, 1.1230, 1.0229, 1.0057, 0.9425, 1.0300, 1.0148,\n",
      "         1.0519],\n",
      "        [0.9407, 0.9538, 0.7663, 0.9315, 0.9690, 0.9388, 0.8746, 0.9301, 0.9491,\n",
      "         0.9515],\n",
      "        [0.9706, 1.0690, 0.9523, 1.0543, 1.0873, 0.9707, 1.0406, 1.0272, 1.0531,\n",
      "         0.9825],\n",
      "        [0.9484, 0.9760, 0.8834, 0.9275, 1.0424, 1.0755, 0.9629, 1.0371, 1.0064,\n",
      "         0.9088],\n",
      "        [0.9454, 1.0762, 1.0360, 1.0583, 1.0559, 1.1546, 1.1251, 1.1924, 1.0008,\n",
      "         1.1064],\n",
      "        [0.9741, 0.9680, 0.9242, 1.0317, 1.0123, 0.9122, 0.9901, 0.9501, 1.0025,\n",
      "         1.1212],\n",
      "        [1.0084, 1.0166, 0.9509, 1.0190, 1.0980, 1.0432, 1.0753, 1.0180, 0.9831,\n",
      "         1.0409],\n",
      "        [1.1134, 0.9465, 1.0373, 1.0479, 1.1103, 1.0572, 1.1073, 1.0036, 1.0583,\n",
      "         1.1979]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "mu shape is (10,)\n",
      "nu shape is (10,)\n",
      "cpuM shape is (10, 10)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.9999878406524658 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.9429, 1.9799, 1.8795,  ..., 1.9930, 1.9225, 1.8838],\n",
      "        [1.8992, 2.0017, 1.7271,  ..., 1.9118, 1.9418, 1.9025],\n",
      "        [2.2321, 2.2404, 2.1791,  ..., 2.1685, 2.2040, 2.1540],\n",
      "        ...,\n",
      "        [2.0652, 2.0758, 2.0691,  ..., 1.9935, 2.0663, 2.0023],\n",
      "        [2.1362, 2.1916, 2.1143,  ..., 2.3113, 2.1858, 2.1667],\n",
      "        [1.9678, 2.0463, 2.0492,  ..., 2.1121, 2.0540, 2.1057]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[3.2106, 3.2455, 3.2026, 3.0426, 3.1433, 3.1278, 3.0753, 3.2452, 3.2305,\n",
      "         3.2409],\n",
      "        [3.7235, 3.7865, 3.7775, 3.8719, 3.8003, 3.9016, 3.8220, 3.8684, 3.6199,\n",
      "         3.7890],\n",
      "        [2.9360, 2.9228, 2.9310, 3.0558, 2.8670, 2.8814, 2.8917, 2.9123, 2.9327,\n",
      "         3.0029],\n",
      "        [3.0249, 3.0121, 2.9271, 3.1899, 3.0012, 3.0911, 3.0750, 3.0537, 3.0873,\n",
      "         3.0697],\n",
      "        [3.4012, 3.4652, 3.3618, 3.3344, 3.4569, 3.2941, 3.4373, 3.2441, 3.4632,\n",
      "         3.3451],\n",
      "        [2.8532, 2.8773, 2.8619, 2.8241, 2.9282, 2.9566, 2.7559, 2.9965, 2.8331,\n",
      "         2.6888],\n",
      "        [3.4727, 3.6526, 3.6103, 3.5600, 3.5352, 3.6512, 3.6320, 3.6918, 3.5113,\n",
      "         3.4974],\n",
      "        [3.2869, 3.2176, 3.1480, 3.1976, 3.2413, 3.0693, 3.1728, 3.0937, 3.2257,\n",
      "         3.3280],\n",
      "        [3.2159, 3.1676, 3.2745, 3.2002, 3.2758, 3.2716, 3.2579, 3.2451, 3.1807,\n",
      "         3.2450],\n",
      "        [3.4524, 3.2750, 3.3624, 3.3540, 3.4528, 3.3574, 3.4369, 3.3056, 3.4749,\n",
      "         3.5017]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned_untrained[0].shape == torch.Size([60, 784])\n",
      "models['aligned_untrained'][i].fc1.weight.shape == torch.Size([60, 784])\n",
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0167],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.3096, 2.0382, 2.0615,  ..., 2.1705, 1.9846, 1.8338],\n",
      "        [1.9039, 2.0370, 2.0859,  ..., 1.8185, 2.0696, 2.1202],\n",
      "        [2.2738, 2.1906, 1.9073,  ..., 2.4674, 2.0615, 2.3778],\n",
      "        ...,\n",
      "        [2.0965, 2.0653, 1.7781,  ..., 2.3302, 1.9803, 2.2078],\n",
      "        [2.4625, 2.3067, 2.3754,  ..., 2.3503, 2.2583, 2.1674],\n",
      "        [2.1610, 2.2279, 2.3076,  ..., 2.0016, 2.2670, 2.1138]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "mu shape is (30,)\n",
      "nu shape is (60,)\n",
      "cpuM shape is (30, 60)\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.6956, 3.7711, 3.0211, 3.5767, 3.5352, 3.2517, 3.2610, 3.5924, 3.4143,\n",
      "         3.5944],\n",
      "        [4.4749, 3.3763, 4.1717, 3.9472, 4.3561, 4.0437, 4.1360, 3.6657, 4.0779,\n",
      "         3.9797],\n",
      "        [3.4108, 3.0533, 2.4913, 3.1042, 3.5534, 3.5592, 2.9598, 3.2815, 3.1547,\n",
      "         3.5153],\n",
      "        [3.7026, 3.4838, 3.0487, 2.6041, 3.5775, 3.1832, 3.6846, 3.1826, 3.3463,\n",
      "         3.1825],\n",
      "        [3.9601, 3.7272, 3.7855, 4.0148, 2.7845, 3.6420, 3.3537, 3.5525, 3.5427,\n",
      "         3.3578],\n",
      "        [2.7981, 3.4635, 3.5573, 2.9577, 3.2290, 2.2047, 3.0536, 3.6152, 2.9989,\n",
      "         3.1745],\n",
      "        [3.4984, 3.7690, 3.9189, 4.1049, 3.6905, 3.6838, 2.9186, 4.4766, 3.7835,\n",
      "         4.2105],\n",
      "        [3.5610, 3.6634, 3.3006, 3.3397, 3.5195, 3.6210, 3.8688, 2.3805, 3.5293,\n",
      "         3.1690],\n",
      "        [3.3552, 3.5712, 3.5509, 3.4672, 3.5187, 3.3228, 3.6135, 3.8233, 2.8536,\n",
      "         3.4699],\n",
      "        [3.7127, 3.7860, 3.5914, 3.7058, 3.5150, 3.7239, 3.9883, 3.2321, 3.4175,\n",
      "         3.0793]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "mu shape is (10,)\n",
      "nu shape is (10,)\n",
      "cpuM shape is (10, 10)\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([60, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([60, 784])\n",
      "Previous layer shape is  None\n",
      "Layer ('fc1.weight', 'fc1.weight') shape is torch.Size([30, 784]) and torch.Size([60, 784])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "Layer ('fc2.weight', 'fc2.weight') shape is torch.Size([30, 30]) and torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([30, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([30, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.9475, 1.8420, 1.8317,  ..., 1.9334, 1.6862, 1.7905],\n",
      "        [1.5161, 1.6015, 1.6141,  ..., 1.4871, 1.5628, 1.5303],\n",
      "        [1.7090, 1.6273, 1.5062,  ..., 1.7323, 1.6469, 1.7795],\n",
      "        ...,\n",
      "        [1.7716, 1.7174, 1.6025,  ..., 1.8449, 1.6903, 1.7821],\n",
      "        [1.9778, 2.0217, 1.9022,  ..., 1.9950, 1.7818, 1.8296],\n",
      "        [1.7309, 1.7826, 1.8140,  ..., 1.6886, 1.7443, 1.6394]],\n",
      "       device='cuda:0')\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "Layer ('fc3.weight', 'fc3.weight') shape is torch.Size([10, 30]) and torch.Size([10, 60])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([30, 60])\n",
      "fc_layer0_weight_data.shape = torch.Size([10, 30])\n",
      "T_var.shape = torch.Size([30, 60])\n",
      "aligned_wt shape is torch.Size([10, 60])\n",
      "fc_layer1_weight_data shape is torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.4782, 2.7249, 2.5884, 2.7168, 2.6365, 2.5839, 2.6360, 2.7966, 2.6669,\n",
      "         2.6890],\n",
      "        [3.4139, 3.0677, 3.3318, 3.2686, 3.3752, 3.2672, 3.3015, 3.1254, 3.3914,\n",
      "         3.2746],\n",
      "        [2.2979, 2.2562, 2.0374, 2.2633, 2.3666, 2.4919, 2.2398, 2.3174, 2.3845,\n",
      "         2.4130],\n",
      "        [2.8897, 2.8053, 2.5701, 2.5369, 2.8657, 2.7144, 2.8215, 2.7213, 2.7994,\n",
      "         2.7022],\n",
      "        [2.9463, 2.8629, 2.9614, 3.0010, 2.7050, 2.9129, 2.7779, 2.7687, 2.7405,\n",
      "         2.8636],\n",
      "        [2.3616, 2.4504, 2.5482, 2.3279, 2.4377, 2.2204, 2.3145, 2.6404, 2.3836,\n",
      "         2.3325],\n",
      "        [2.7680, 2.8812, 3.0185, 2.9659, 2.8823, 3.0023, 2.7250, 3.1371, 2.8725,\n",
      "         2.9913],\n",
      "        [2.8205, 2.8179, 2.7757, 2.8384, 2.7316, 2.7474, 2.8511, 2.4193, 2.7806,\n",
      "         2.8431],\n",
      "        [2.4794, 2.6796, 2.6383, 2.5969, 2.6110, 2.6568, 2.6944, 2.8195, 2.5201,\n",
      "         2.5930],\n",
      "        [2.9282, 3.0407, 2.8987, 3.0078, 2.9592, 2.9382, 3.0755, 2.7945, 2.9250,\n",
      "         2.9059]], device='cuda:0')\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned_diff[0].shape == torch.Size([60, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([60, 784])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, N_MODELS):\n",
    "    _, _, T_vars = get_wassersteinized_layers_modularized(args, [models['untrained'][i - 1], models['untrained'][i]])\n",
    "    _, aligned_untrained, _ = get_wassersteinized_layers_modularized(\n",
    "        args, \n",
    "        [models['trained'][i - 1], models['untrained'][i]], \n",
    "        T_vars_pre_computed=[T_var[1] for T_var in T_vars]\n",
    "    )\n",
    "    model = Net(weights=aligned_untrained)\n",
    "    models['aligned_untrained'][i] = model.to('cpu')\n",
    "    print(f'aligned_untrained[0].shape == {aligned_untrained[0].shape}')\n",
    "    print(f\"models['aligned_untrained'][i].fc1.weight.shape == {models['aligned_untrained'][i].fc1.weight.shape}\")\n",
    "\n",
    "    models['trained_short'][i].to(device)\n",
    "    _, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][i - 1], models['trained_short'][i]])\n",
    "    model = Net(weights=aligned)\n",
    "    models['aligned'][i] = model.to('cpu')\n",
    "    print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "    print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "\n",
    "    _, aligned_diff, _ = get_wassersteinized_layers_modularized(\n",
    "        args, \n",
    "        [\n",
    "            models['trained'][i - 1] - models['untrained'][i - 1], \n",
    "            models['untrained'][i]\n",
    "        ], \n",
    "        T_vars_pre_computed=[T_var[1] for T_var in T_vars]\n",
    "    )\n",
    "    model = Net(weights=aligned_diff) + models['untrained'][i]\n",
    "    models['aligned_diff'][i] = model.to('cpu')\n",
    "    print(f'aligned_diff[0].shape == {aligned_diff[0].shape}')\n",
    "    print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== MODEL 0 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3045, Accuracy: 871/10000 (9%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1503, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.4047, Accuracy: 8906/10000 (89%)\n",
      "\n",
      "Aligned untrained:\n",
      "------------\n",
      "Aligned:\n",
      "------------\n",
      "Aligned diff:\n",
      "------------\n",
      "=============== MODEL 1 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3043, Accuracy: 753/10000 (8%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1128, Accuracy: 9666/10000 (97%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3470, Accuracy: 9000/10000 (90%)\n",
      "\n",
      "Aligned untrained:\n",
      "\n",
      "Validation set: Average loss: 35.5676, Accuracy: 73/10000 (1%)\n",
      "\n",
      "Aligned:\n",
      "\n",
      "Validation set: Average loss: 0.3554, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Aligned diff:\n",
      "\n",
      "Validation set: Average loss: 0.3543, Accuracy: 9444/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_MODELS):\n",
    "    print(f'=============== MODEL {i} ==================')\n",
    "    print('Untrained:')\n",
    "    validate(models['untrained'][i].to(device), criterion, [], [])\n",
    "    print('Trained:')\n",
    "    validate(models['trained'][i].to(device), criterion, [], [])\n",
    "    print('Trained short:')\n",
    "    validate(models['trained_short'][i].to(device), criterion, [], [])\n",
    "    print('Aligned untrained:')\n",
    "    if i in models['aligned_untrained']:\n",
    "        validate(models['aligned_untrained'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')\n",
    "    print('Aligned:')\n",
    "    if i in models['aligned']:\n",
    "        validate(models['aligned'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')\n",
    "    print('Aligned diff:')\n",
    "    if i in models['aligned_diff']:\n",
    "        validate(models['aligned_diff'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, aligned, T_vars \u001b[39m=\u001b[39m get_wassersteinized_layers_modularized(args, [models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], models[\u001b[39m'\u001b[39;49m\u001b[39mtrained_short\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m4\u001b[39;49m]])\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m Net(weights\u001b[39m=\u001b[39maligned)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39maligned[0].shape == \u001b[39m\u001b[39m{\u001b[39;00maligned[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "_, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained_short'][4]])\n",
    "model = Net(weights=aligned)\n",
    "print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Net(\n",
       "   (fc1): Linear(in_features=784, out_features=50, bias=False)\n",
       "   (fc1_drop): Dropout(p=0.2, inplace=False)\n",
       "   (fc2): Linear(in_features=50, out_features=50, bias=False)\n",
       "   (fc2_drop): Dropout(p=0.2, inplace=False)\n",
       "   (fc3): Linear(in_features=50, out_features=10, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['aligned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Net(\n",
       "   (fc1): Linear(in_features=784, out_features=50, bias=False)\n",
       "   (fc1_drop): Dropout(p=0.2, inplace=False)\n",
       "   (fc2): Linear(in_features=50, out_features=50, bias=False)\n",
       "   (fc2_drop): Dropout(p=0.2, inplace=False)\n",
       "   (fc3): Linear(in_features=50, out_features=10, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['aligned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071111 torch.Size([30, 784]) torch.Size([60, 784])\n",
      "2.3333333333333335 tensor(1.0000, device='cuda:0') tensor(0.0389, device='cuda:0') tensor(0.0389, device='cuda:0') tensor(-0.0041, device='cuda:0') tensor(-0.0041, device='cuda:0')\n",
      "\n",
      "0.5000061 torch.Size([30, 30]) torch.Size([60, 60])\n",
      "2.3333333333333335 tensor(1.0000, device='cuda:0') tensor(0.2008, device='cuda:0') tensor(0.2008, device='cuda:0') tensor(0.0183, device='cuda:0') tensor(0.0183, device='cuda:0')\n",
      "\n",
      "0.7071117 torch.Size([10, 30]) torch.Size([10, 60])\n",
      "2.3333333333333335 tensor(1.0000, device='cuda:0') tensor(0.3651, device='cuda:0') tensor(0.3651, device='cuda:0') tensor(-0.0095, device='cuda:0') tensor(-0.0095, device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaling = dict()\n",
    "for idx, ((layer0_name, fc_layer0_weight), (layer1_name, fc_layer1_weight)) in \\\n",
    "        enumerate(zip(models['trained'][0].named_parameters(), models['aligned'][1].named_parameters())):\n",
    "    with torch.no_grad():\n",
    "        scaling[layer0_name] = ((fc_layer0_weight.abs() ** 2).sum() ** 0.5 / (fc_layer1_weight.abs() ** 2).sum() ** 0.5).cpu().numpy()\n",
    "        print(scaling[layer0_name], fc_layer0_weight.shape, fc_layer1_weight.shape)\n",
    "        print(\n",
    "            70 / 30,\n",
    "            fc_layer0_weight.abs().mean() /\n",
    "            fc_layer1_weight.abs().mean(),\n",
    "            fc_layer0_weight.abs().mean(), \n",
    "            fc_layer1_weight.abs().mean(),\n",
    "            fc_layer0_weight.mean(), \n",
    "            fc_layer1_weight.mean(),\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.294997\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.847028\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.607638\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.529232\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.483924\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.266716\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.481619\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.339755\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.449372\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.370931\n",
      "\n",
      "Validation set: Average loss: 0.3060, Accuracy: 9156/10000 (92%)\n",
      "\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0010, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([1000, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([1000, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[7.3297, 7.2995, 7.3031,  ..., 7.3083, 7.2919, 7.3267],\n",
      "        [8.3588, 8.3629, 8.2879,  ..., 8.3670, 8.3703, 8.3684],\n",
      "        [7.0789, 7.0735, 7.0433,  ..., 7.0303, 7.0474, 7.0612],\n",
      "        ...,\n",
      "        [9.4452, 9.3633, 9.4030,  ..., 9.3579, 9.3288, 9.4104],\n",
      "        [9.7097, 9.7432, 9.6230,  ..., 9.6819, 9.6902, 9.7332],\n",
      "        [9.0655, 9.1174, 8.9704,  ..., 9.0810, 9.0538, 9.0654]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0010, device='cuda:0')\n",
      "Here, trace is 0.9998999834060669 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([1000, 1000])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[13.5453, 14.7499, 14.1615, 14.4284, 14.5511, 14.1955, 14.2287, 14.2663,\n",
      "         14.4052, 14.3704],\n",
      "        [14.5993, 13.5518, 14.3806, 14.2458, 14.4361, 14.3378, 14.2301, 14.1356,\n",
      "         14.2430, 14.3179],\n",
      "        [11.8683, 11.7628, 11.0733, 11.7441, 12.0601, 12.2338, 11.7702, 11.8221,\n",
      "         11.9232, 12.1498],\n",
      "        [12.4505, 12.0432, 12.0584, 11.5702, 12.6128, 12.2578, 12.7466, 12.1993,\n",
      "         12.3141, 12.3585],\n",
      "        [14.1377, 13.8631, 14.0275, 14.0954, 12.9968, 13.9119, 13.6040, 13.9200,\n",
      "         13.8142, 13.5925],\n",
      "        [11.3239, 11.7454, 11.8089, 11.4441, 11.6948, 10.8723, 11.4139, 11.8319,\n",
      "         11.4959, 11.6063],\n",
      "        [13.4955, 13.7008, 13.6870, 14.0701, 13.4939, 13.4664, 12.9534, 14.0489,\n",
      "         13.6411, 13.8627],\n",
      "        [13.9633, 13.9177, 13.7713, 13.7893, 13.9532, 14.1880, 14.2343, 13.1309,\n",
      "         14.1194, 13.7800],\n",
      "        [12.6808, 12.5479, 12.6994, 12.6668, 12.8520, 12.5280, 12.6590, 13.0206,\n",
      "         12.1209, 12.6964],\n",
      "        [13.2163, 13.3934, 13.5195, 13.1096, 12.9063, 13.1963, 13.5745, 12.8281,\n",
      "         13.0367, 12.5668]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([1000, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "\n",
      "Validation set: Average loss: 98.5679, Accuracy: 9505/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(hidden_size=1000).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, N_EPOCHS_SHORT + 1):\n",
    "    train(model, optimizer, criterion, epoch)\n",
    "    validate(model, criterion, lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0010, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([1000, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([1000, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[11.6144,  7.6671, 12.2352,  ..., 10.9345, 11.1916, 11.2013],\n",
      "        [13.3201, 10.2047, 10.5064,  ..., 15.0106,  9.9551, 12.4098],\n",
      "        [11.0775,  8.9430, 11.1173,  ...,  9.7651,  8.5255, 11.4466],\n",
      "        ...,\n",
      "        [15.5746, 10.6499, 12.4656,  ..., 13.0302,  9.1234, 13.0301],\n",
      "        [14.4431, 12.8314,  8.8450,  ..., 14.8244,  8.7800, 13.8372],\n",
      "        [12.6050, 11.5854,  0.4547,  ..., 14.5230,  7.7296,  9.3355]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0010, device='cuda:0')\n",
      "Here, trace is 0.9998999834060669 and matrix sum is 999.9000244140625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([1000, 1000])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[ 0.7296, 23.7098, 17.4732, 21.3558, 23.1320, 16.8618, 17.1188, 20.1153,\n",
      "         20.6236, 20.9616],\n",
      "        [23.7381,  0.6563, 18.8590, 18.5451, 20.6616, 19.6587, 19.8371, 19.6288,\n",
      "         19.2648, 20.9945],\n",
      "        [17.5090, 18.8355,  0.4540, 16.2160, 19.3809, 20.3023, 17.4286, 16.3006,\n",
      "         18.3245, 21.7112],\n",
      "        [21.3538, 18.5295, 16.1915,  0.5670, 20.7370, 16.0957, 23.3841, 17.8545,\n",
      "         17.2375, 16.7230],\n",
      "        [23.1650, 20.6793, 19.3977, 20.7205,  0.6966, 19.9494, 17.4266, 20.6625,\n",
      "         20.4874, 16.8482],\n",
      "        [16.8583, 19.6519, 20.2922, 16.0814, 19.9503,  0.5636, 16.4839, 21.3138,\n",
      "         16.5706, 18.1402],\n",
      "        [17.1644, 19.8234, 17.4463, 23.3776, 17.3907, 16.4790,  0.6796, 22.8848,\n",
      "         18.7729, 23.6319],\n",
      "        [20.1346, 19.6330, 16.3026, 17.8877, 20.6598, 21.3221, 22.8755,  0.7074,\n",
      "         21.9352, 17.3594],\n",
      "        [20.6154, 19.2907, 18.3294, 17.2280, 20.5196, 16.5568, 18.7661, 21.9439,\n",
      "          0.6802, 17.2181],\n",
      "        [20.9748, 21.0355, 21.7164, 16.7332, 16.8681, 18.1453, 23.6274, 17.3785,\n",
      "         17.1974,  0.6259]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([1000, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "\n",
      "Validation set: Average loss: 99.5590, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "average[0].shape == torch.Size([1000, 784])\n",
      "\n",
      "Validation set: Average loss: 98.8879, Accuracy: 9506/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], model])\n",
    "model = Net(weights=aligned)\n",
    "print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "validate(model.to(device), criterion, [], [])\n",
    "model = Net(weights=average)\n",
    "print(f'average[0].shape == {average[0].shape}')\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],\n",
      "        ...,\n",
      "        [0.0000, 0.0250, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999960064888 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.5175, 1.2648, 1.3460,  ..., 1.9774, 1.6739, 2.0909],\n",
      "        [2.0575, 1.8461, 1.7188,  ..., 1.4778, 1.3777, 2.1506],\n",
      "        [2.6075, 2.3140, 2.0627,  ..., 2.4417, 2.2204, 1.4178],\n",
      "        ...,\n",
      "        [2.2927, 1.7001, 1.6253,  ..., 2.4608, 1.9484, 1.5914],\n",
      "        [1.9489, 1.8061, 2.0693,  ..., 2.3337, 1.8796, 1.9879],\n",
      "        [1.9503, 2.2265, 2.1172,  ..., 1.3048, 1.6413, 1.8783]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0500, device='cuda:0')\n",
      "Here, trace is 1.9999920129776 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.7368, 4.2591, 3.1290, 3.4840, 3.9956, 3.0599, 3.0535, 3.3443, 3.3063,\n",
      "         3.5534],\n",
      "        [3.8920, 2.5985, 2.6589, 2.7885, 2.8614, 3.4302, 3.1944, 3.5126, 3.3150,\n",
      "         3.4946],\n",
      "        [2.7467, 2.9292, 1.7334, 2.7244, 3.2027, 3.5126, 3.4180, 2.4203, 3.1560,\n",
      "         3.5093],\n",
      "        [3.6472, 3.1527, 2.9823, 1.5480, 3.6422, 3.3863, 4.3974, 2.8970, 3.2467,\n",
      "         2.9491],\n",
      "        [3.8923, 3.7429, 3.7614, 4.0709, 1.9277, 3.3013, 3.0625, 4.0600, 3.3201,\n",
      "         3.3029],\n",
      "        [3.2568, 3.1743, 3.4380, 2.5289, 3.5912, 1.8549, 3.2143, 3.3924, 3.1074,\n",
      "         3.5433],\n",
      "        [3.4618, 3.5457, 3.4434, 3.9705, 3.4602, 2.7300, 2.2263, 4.2631, 3.4743,\n",
      "         4.2588],\n",
      "        [3.4181, 3.2413, 3.1034, 3.1516, 3.3882, 3.9777, 3.8931, 2.0674, 4.0258,\n",
      "         2.9565],\n",
      "        [3.5529, 3.2944, 2.8914, 2.6681, 3.2969, 2.9420, 3.6392, 3.7264, 1.9799,\n",
      "         3.4264],\n",
      "        [3.6572, 4.0661, 3.8619, 3.3406, 2.7500, 3.0941, 3.9105, 3.3191, 3.2389,\n",
      "         1.4859]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])\n",
    "\n",
    "for T_var in T_vars:\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(T_var.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.1756, Accuracy: 9445/10000 (94%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.3049, Accuracy: 968/10000 (10%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.3041, Accuracy: 773/10000 (8%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.4477, Accuracy: 1905/10000 (19%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(40)\n",
    "validate(models['trained'][0].to(device), criterion, [], [])\n",
    "validate(models['untrained'][1].to(device), criterion, [], [])\n",
    "validate(model.to(device), criterion, [], [])\n",
    "with torch.no_grad():\n",
    "    model.fc1.weight = nn.Parameter(aligned[0])\n",
    "    model.fc2.weight = nn.Parameter(aligned[1])\n",
    "    model.fc3.weight = nn.Parameter(aligned[2])\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bdc725fb65c21f82e45edbbae76c938f412b1b7259c22cf88bbbf1e62e294f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
