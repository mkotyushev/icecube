{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify MNIST digits using **PyTorch**. \n",
    "\n",
    "First, the needed imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/micromamba/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.0.post200  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next we'll load the MNIST data.  First time we may have to download the data, which can take a while.\n",
    "\n",
    "Note that we are here using the MNIST test data for *validation*, instead of for testing the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transforms.ToTensor())\n",
    "\n",
    "validation_dataset = datasets.MNIST('./data', \n",
    "                                    train=False, \n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. The first element of training data (`X_train`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. `y_train` is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray_r\")\n",
    "    plt.title('Class: '+str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network definition\n",
    "\n",
    "Let's define the network as a Python class.  We have to write the `__init__()` and `forward()` methods, and PyTorch will automatically generate a `backward()` method for computing the gradients for the backward pass.\n",
    "\n",
    "Finally, we define an optimizer to update the model parameters based on the computed gradients.  We select *stochastic gradient descent (with momentum)* as the optimization algorithm, and set *learning rate* to 0.01.  Note that there are [several different options](http://pytorch.org/docs/optim.html#algorithms) for the optimizer in PyTorch that we could use instead of *SGD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_size=50, weights=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden_size, bias=False)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10, bias=False)\n",
    "\n",
    "        if weights is not None:\n",
    "            assert len(weights) == 3\n",
    "            with torch.no_grad():\n",
    "                self.fc1.weight = nn.Parameter(weights[0])\n",
    "                self.fc2.weight = nn.Parameter(weights[1])\n",
    "                self.fc3.weight = nn.Parameter(weights[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        diff_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (first_model_param_name, first_model_param), \\\n",
    "                (second_model_param_name, second_model_param), \\\n",
    "                (diff_model_param_name, diff_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                other.named_parameters(),\n",
    "                diff_model.named_parameters()\n",
    "            ):\n",
    "                assert first_model_param_name == second_model_param_name == diff_model_param_name\n",
    "                assert first_model_param.shape == second_model_param.shape == diff_model_param.shape\n",
    "                diff_model_param.copy_(\n",
    "                    first_model_param +\n",
    "                    second_model_param\n",
    "                )\n",
    "        return diff_model\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        diff_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (first_model_param_name, first_model_param), \\\n",
    "                (second_model_param_name, second_model_param), \\\n",
    "                (diff_model_param_name, diff_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                other.named_parameters(),\n",
    "                diff_model.named_parameters()\n",
    "            ):\n",
    "                assert first_model_param_name == second_model_param_name == diff_model_param_name\n",
    "                assert first_model_param.shape == second_model_param.shape == diff_model_param.shape\n",
    "                diff_model_param.copy_(\n",
    "                    first_model_param -\n",
    "                    second_model_param\n",
    "                )\n",
    "        return diff_model\n",
    "\n",
    "    def __mul__(self, scalar):\n",
    "        result_model = copy.deepcopy(self)\n",
    "        with torch.no_grad():\n",
    "            for \\\n",
    "                (_, model_param), \\\n",
    "                (_, result_model_param) in zip(\n",
    "                self.named_parameters(), \n",
    "                result_model.named_parameters()\n",
    "            ):\n",
    "                result_model_param.copy_(\n",
    "                    model_param * scalar\n",
    "                )\n",
    "        return result_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Let's now define functions to `train()` and `validate()` the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, loss_vector, accuracy_vector):\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in validation_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(validation_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(validation_loader.dataset), accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train our model using the `train()` function.  An *epoch* means one pass through the whole training data. After each epoch, we evaluate the model using `validate()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net().to(device)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# epochs = 10\n",
    "\n",
    "# lossv, accv = [], []\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     train(epoch)\n",
    "#     validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize how the training progressed. \n",
    "\n",
    "* *Loss* is a function of the difference of the network output and the target values.  We are minimizing the loss function during training so it should decrease over time.\n",
    "* *Accuracy* is the classification accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(5,3))\n",
    "# plt.plot(np.arange(1,epochs+1), lossv)\n",
    "# plt.title('validation loss')\n",
    "\n",
    "# plt.figure(figsize=(5,3))\n",
    "# plt.plot(np.arange(1,epochs+1), accv)\n",
    "# plt.title('validation accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "\n",
    "Modify the MLP model.  Try to improve the classification accuracy, or experiment with the effects of different parameters.  If you are interested in the state-of-the-art performance on permutation invariant MNIST, see e.g. this [recent paper](https://arxiv.org/abs/1507.02672) by Aalto University / The Curious AI Company researchers.\n",
    "\n",
    "You can also consult the PyTorch documentation at http://pytorch.org/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train multiple MLPs with different hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299198\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.030619\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.564870\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.244364\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.965074\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.756698\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.878756\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.679140\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.785333\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.757187\n",
      "\n",
      "Validation set: Average loss: 0.3970, Accuracy: 8879/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.307256\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.591349\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.506101\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.588762\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.352742\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.386505\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.763527\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.975834\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.331177\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.613048\n",
      "\n",
      "Validation set: Average loss: 0.2909, Accuracy: 9134/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.330193\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.367857\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.495416\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.512956\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.618295\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.466743\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.556445\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.435932\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.161107\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.345329\n",
      "\n",
      "Validation set: Average loss: 0.2457, Accuracy: 9256/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.243274\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.419409\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.343114\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.317064\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.493260\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.405144\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.230651\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.260980\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.325677\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.389008\n",
      "\n",
      "Validation set: Average loss: 0.2196, Accuracy: 9330/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.430633\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.598699\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.191830\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.324336\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.906066\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.374742\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.246126\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.168072\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.358109\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.211921\n",
      "\n",
      "Validation set: Average loss: 0.2017, Accuracy: 9390/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.220507\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.141015\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.258241\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.258933\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.341938\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.227872\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.223693\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.160730\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.271110\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.502489\n",
      "\n",
      "Validation set: Average loss: 0.1838, Accuracy: 9438/10000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.141107\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.271300\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.333416\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.331738\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.313048\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.358632\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.344255\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.478965\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.235193\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.365659\n",
      "\n",
      "Validation set: Average loss: 0.1759, Accuracy: 9449/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.127713\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.304519\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.278026\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.204407\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.683518\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.289618\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.358867\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.260822\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.252664\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.080305\n",
      "\n",
      "Validation set: Average loss: 0.1681, Accuracy: 9503/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.545239\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.099877\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.361151\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.324670\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.775943\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.305968\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.397356\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.145302\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.461257\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.135722\n",
      "\n",
      "Validation set: Average loss: 0.1614, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.320377\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.254457\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.237638\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.136532\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.150099\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.333112\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.266999\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.528217\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.254143\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.062151\n",
      "\n",
      "Validation set: Average loss: 0.1582, Accuracy: 9505/10000 (95%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307952\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.080915\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.309884\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.261053\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.964486\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.852863\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.572903\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.535871\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.561841\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.464154\n",
      "\n",
      "Validation set: Average loss: 0.3898, Accuracy: 8925/10000 (89%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311315\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.096641\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.528712\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.256807\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.961170\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.695419\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.807799\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.614226\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.777363\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.174197\n",
      "\n",
      "Validation set: Average loss: 0.3841, Accuracy: 8954/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.624441\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.595878\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.457731\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.643517\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.437829\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.202955\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.325743\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.227995\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.163211\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.545622\n",
      "\n",
      "Validation set: Average loss: 0.2679, Accuracy: 9204/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.352192\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.446790\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.338375\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.311748\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.259612\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.590460\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.234877\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.123232\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.410927\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.365317\n",
      "\n",
      "Validation set: Average loss: 0.2225, Accuracy: 9349/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.382282\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.365575\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.332313\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.302048\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.738456\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.374630\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.349453\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.123182\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.096156\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.318987\n",
      "\n",
      "Validation set: Average loss: 0.1941, Accuracy: 9422/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.131761\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.225535\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.167751\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.508981\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.224203\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.411397\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.513712\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.147475\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.252350\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.493719\n",
      "\n",
      "Validation set: Average loss: 0.1786, Accuracy: 9463/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.127121\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.178615\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.598927\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.614924\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.222163\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.256993\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.181574\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.235539\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.333910\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.245129\n",
      "\n",
      "Validation set: Average loss: 0.1681, Accuracy: 9498/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.288146\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.116784\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.527973\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.244078\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.242579\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.427433\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.141289\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.168636\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.203375\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.165556\n",
      "\n",
      "Validation set: Average loss: 0.1567, Accuracy: 9524/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.422599\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.365320\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.324964\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.156382\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.244762\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.505106\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.379382\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.251186\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.150841\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.149363\n",
      "\n",
      "Validation set: Average loss: 0.1505, Accuracy: 9532/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.297787\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.145758\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.184604\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.073259\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.240996\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.189864\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.086552\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.163399\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.058069\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.157936\n",
      "\n",
      "Validation set: Average loss: 0.1436, Accuracy: 9575/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.078305\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.127451\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.217474\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.188224\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.286537\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.148191\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.130883\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.177058\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.250298\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.511181\n",
      "\n",
      "Validation set: Average loss: 0.1369, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304351\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.121834\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.449802\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.705749\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.840890\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.841999\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.689812\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.574676\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.986453\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.591860\n",
      "\n",
      "Validation set: Average loss: 0.3825, Accuracy: 8942/10000 (89%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307874\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.036826\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.495527\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.162982\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.574443\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.570638\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.583804\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.512610\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.467179\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.550883\n",
      "\n",
      "Validation set: Average loss: 0.3547, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.780803\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.282056\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.575888\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.501095\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.487390\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.627608\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.298153\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.291880\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.708000\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.171210\n",
      "\n",
      "Validation set: Average loss: 0.2651, Accuracy: 9220/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.945973\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.345345\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.421214\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.288060\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.311616\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.492463\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.331896\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.328490\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.267017\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.417168\n",
      "\n",
      "Validation set: Average loss: 0.2135, Accuracy: 9355/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.315101\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.161570\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.610004\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.263820\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.350617\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.252947\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.136858\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.212304\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.263522\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.162567\n",
      "\n",
      "Validation set: Average loss: 0.1851, Accuracy: 9449/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.056998\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.423331\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.474057\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.209778\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.343991\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.375263\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.434160\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.319576\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.139393\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.302087\n",
      "\n",
      "Validation set: Average loss: 0.1666, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.098275\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.439271\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.246570\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.262743\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.058541\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.190111\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.132198\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.122896\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.261574\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.229558\n",
      "\n",
      "Validation set: Average loss: 0.1534, Accuracy: 9543/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.355380\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.192162\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.186624\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.075763\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.118774\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.153411\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.268118\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.298613\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.175677\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.540406\n",
      "\n",
      "Validation set: Average loss: 0.1394, Accuracy: 9573/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.138542\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.175728\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.232821\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.259320\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.213448\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.286702\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.196156\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.215603\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.050189\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.261509\n",
      "\n",
      "Validation set: Average loss: 0.1340, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.113969\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.116267\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.098212\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.242360\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.099017\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.154334\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.135209\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.175226\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.089997\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.371204\n",
      "\n",
      "Validation set: Average loss: 0.1303, Accuracy: 9615/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.082328\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.271811\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.146488\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.196785\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.203381\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.186060\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.128441\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.207470\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.052594\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.348608\n",
      "\n",
      "Validation set: Average loss: 0.1234, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302786\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.073962\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.455906\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.053818\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.599750\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.598446\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.423499\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.432508\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.364798\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.532447\n",
      "\n",
      "Validation set: Average loss: 0.3580, Accuracy: 8997/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292607\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.100172\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.298539\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.188912\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.849926\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.799199\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.782177\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.486667\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.311398\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.488080\n",
      "\n",
      "Validation set: Average loss: 0.3461, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.191095\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.551603\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.180382\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.254825\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.465915\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.262170\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.284141\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.660160\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.602708\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.187271\n",
      "\n",
      "Validation set: Average loss: 0.2513, Accuracy: 9260/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.254296\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.415258\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.474892\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.299545\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.118925\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.276825\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.228992\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.570717\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.186069\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.357676\n",
      "\n",
      "Validation set: Average loss: 0.2014, Accuracy: 9381/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.338632\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.230945\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.362141\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.160128\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.259696\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.349087\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.383463\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.254473\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.210520\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.115832\n",
      "\n",
      "Validation set: Average loss: 0.1771, Accuracy: 9455/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.115808\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.661354\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.523877\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.524898\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.299581\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.087061\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.399470\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.254242\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.187813\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.062679\n",
      "\n",
      "Validation set: Average loss: 0.1554, Accuracy: 9518/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.170753\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.263994\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.228802\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.224093\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.114303\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.116087\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.355462\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.109190\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.212220\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.070255\n",
      "\n",
      "Validation set: Average loss: 0.1402, Accuracy: 9566/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.132600\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.034030\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.046594\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.079624\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.183021\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.091388\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.049020\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.157240\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.175704\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.077508\n",
      "\n",
      "Validation set: Average loss: 0.1326, Accuracy: 9577/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.108229\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.040111\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.352931\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.166815\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.084423\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.184269\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.199694\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.127673\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.106058\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.079488\n",
      "\n",
      "Validation set: Average loss: 0.1230, Accuracy: 9618/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.163649\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.258358\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.082430\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.299449\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.140983\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.073004\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.114311\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.019211\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.084483\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.187174\n",
      "\n",
      "Validation set: Average loss: 0.1169, Accuracy: 9632/10000 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.213884\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.148328\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.278769\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.090224\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.275097\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.060473\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.080007\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.259629\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.041644\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.247048\n",
      "\n",
      "Validation set: Average loss: 0.1115, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296052\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.079268\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.333698\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.606006\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.650145\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.515783\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.784457\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.847016\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.462283\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.497849\n",
      "\n",
      "Validation set: Average loss: 0.3481, Accuracy: 8999/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298095\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.009329\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.111612\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.933986\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.689230\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.935475\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.444063\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.521012\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.629788\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.558781\n",
      "\n",
      "Validation set: Average loss: 0.3448, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.583557\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.317906\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.226033\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.399058\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.325525\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.372788\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.199287\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.540865\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.412418\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.324740\n",
      "\n",
      "Validation set: Average loss: 0.2461, Accuracy: 9279/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.194909\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.337681\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.361563\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.259243\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.171981\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.423066\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.578085\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.137502\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.151251\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.171444\n",
      "\n",
      "Validation set: Average loss: 0.2010, Accuracy: 9401/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.314040\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.739856\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.166483\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.224039\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.178546\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.068457\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.639747\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.126686\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.532885\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.373108\n",
      "\n",
      "Validation set: Average loss: 0.1667, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.166579\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.305767\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.096500\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.416911\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.237228\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.184810\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.124274\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.139097\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.175057\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.131727\n",
      "\n",
      "Validation set: Average loss: 0.1483, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.237475\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.242711\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.222871\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.401548\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.126324\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.102236\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.306221\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.269457\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.162979\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.034942\n",
      "\n",
      "Validation set: Average loss: 0.1347, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.221426\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.424438\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.115427\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.332079\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.045454\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.081950\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.227810\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.222764\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.043470\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.122063\n",
      "\n",
      "Validation set: Average loss: 0.1271, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.369160\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.318157\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.201127\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.111451\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.028687\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.265866\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.119546\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.128466\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.063649\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.245833\n",
      "\n",
      "Validation set: Average loss: 0.1159, Accuracy: 9650/10000 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.059226\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.109174\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.359440\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.071692\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.129729\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.153171\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.137303\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.090230\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.297312\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.103775\n",
      "\n",
      "Validation set: Average loss: 0.1112, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.121077\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.054473\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.033152\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.172678\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.232981\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.257550\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.219973\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.337345\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.030314\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.132076\n",
      "\n",
      "Validation set: Average loss: 0.1060, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299292\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.113336\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.223097\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.931516\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.716833\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.740209\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.508317\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.385268\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.536311\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.241258\n",
      "\n",
      "Validation set: Average loss: 0.3412, Accuracy: 9042/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS_SHORT = 1\n",
    "N_EPOCHS = 10\n",
    "N_MODELS = 5\n",
    "models = defaultdict(dict)\n",
    "for i in range(N_MODELS):\n",
    "    model = Net(hidden_size=10 * (i + 3)).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    models['untrained'][i] = copy.deepcopy(model).to('cpu')\n",
    "\n",
    "    lossv, accv = [], []\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        train(model, optimizer, criterion, epoch)\n",
    "        validate(model, criterion, lossv, accv)\n",
    "    \n",
    "    models['trained'][i] = model.to('cpu')\n",
    "\n",
    "    model = copy.deepcopy(models['untrained'][i]).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    lossv, accv = [], []\n",
    "    for epoch in range(1, N_EPOCHS_SHORT + 1):\n",
    "        train(model, optimizer, criterion, epoch)\n",
    "        validate(model, criterion, lossv, accv)\n",
    "    \n",
    "    models['trained_short'][i] = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24720\n",
      "\n",
      "Validation set: Average loss: 2.3028, Accuracy: 914/10000 (9%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1582, Accuracy: 9505/10000 (95%)\n",
      "\n",
      "33360\n",
      "\n",
      "Validation set: Average loss: 2.3038, Accuracy: 826/10000 (8%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1369, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "42200\n",
      "\n",
      "Validation set: Average loss: 2.3001, Accuracy: 1218/10000 (12%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1234, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "51240\n",
      "\n",
      "Validation set: Average loss: 2.2999, Accuracy: 1210/10000 (12%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1115, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "60480\n",
      "\n",
      "Validation set: Average loss: 2.3024, Accuracy: 810/10000 (8%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1060, Accuracy: 9677/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_MODELS):\n",
    "    print(get_n_params(models['untrained'][i]))\n",
    "    validate(models['untrained'][i].to(device), criterion, [], [])\n",
    "    validate(models['trained'][i].to(device), criterion, [], [])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OT fusion to map smaller model to bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspaces/job_application/otfusion/')\n",
    "sys.path.insert(0, '/workspaces/job_application/cifar/')\n",
    "from wasserstein_ensemble import get_wassersteinized_layers_modularized\n",
    "from parameters import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()\n",
    "args = parser.parse_args('--gpu-id 1 --model-name mlpnet --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --exact --correction --ground-metric euclidean --weight-stats \\\n",
    "--activation-histograms --activation-mode raw --geom-ensemble-type acts --sweep-id 21 \\\n",
    "--act-num-samples 200 --ground-metric-normalize none --activation-seed 21 \\\n",
    "--prelu-acts --recheck-acc --load-models ./mnist_models --ckpt-type final \\\n",
    "--past-correction --not-squared --dist-normalize --print-distances --to-download'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.gpu_id = 0\n",
    "args.proper_marginals = True\n",
    "args.skip_last_layer = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic check with two trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0250,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.3333280086517334 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.0681, 2.2005, 1.5773,  ..., 1.3306, 2.1235, 1.5188],\n",
      "        [1.8381, 1.8590, 1.3707,  ..., 1.8699, 2.2082, 1.8303],\n",
      "        [1.4483, 1.4819, 1.4599,  ..., 1.8270, 1.6114, 1.4914],\n",
      "        ...,\n",
      "        [1.9778, 2.5447, 2.0899,  ..., 1.9904, 2.6248, 1.7526],\n",
      "        [1.8577, 2.1451, 1.6796,  ..., 1.7256, 2.3501, 2.0952],\n",
      "        [2.1402, 1.9953, 1.9925,  ..., 1.6596, 2.2325, 1.9417]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0083,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0083, device='cuda:0')\n",
      "Here, trace is 0.33333197236061096 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.8889, 3.9014, 3.0255, 3.6213, 4.0799, 3.3666, 2.9929, 3.2339, 3.7269,\n",
      "         3.4725],\n",
      "        [4.1093, 2.2386, 3.5851, 3.4406, 3.5570, 3.3916, 3.5701, 3.3251, 3.0391,\n",
      "         3.8246],\n",
      "        [3.1341, 3.1805, 1.3682, 3.1476, 3.3546, 3.8020, 3.3256, 2.6820, 3.3240,\n",
      "         3.7015],\n",
      "        [3.5118, 3.3178, 2.8397, 1.4569, 3.8191, 2.8449, 3.9636, 3.2775, 3.2381,\n",
      "         3.0634],\n",
      "        [3.9902, 3.7802, 3.4545, 4.0209, 1.5824, 3.2638, 3.0778, 3.8608, 3.2720,\n",
      "         3.0291],\n",
      "        [2.5545, 3.7321, 3.4729, 2.9361, 3.6750, 1.8687, 2.9387, 3.6026, 3.0168,\n",
      "         3.0204],\n",
      "        [2.7924, 3.7241, 3.2277, 4.0965, 2.9360, 3.1278, 1.6434, 3.9118, 3.2235,\n",
      "         3.9614],\n",
      "        [3.9627, 3.3706, 2.9983, 3.0136, 3.7295, 3.8780, 4.2161, 1.7809, 3.6494,\n",
      "         3.1830],\n",
      "        [3.0414, 3.9472, 3.1211, 3.0591, 3.8194, 2.7387, 3.4988, 3.6025, 2.0426,\n",
      "         3.2927],\n",
      "        [3.6410, 3.7555, 3.7653, 3.2728, 3.3598, 3.1128, 4.0683, 3.0550, 2.9676,\n",
      "         1.9152]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n"
     ]
    }
   ],
   "source": [
    "_, aligned, _ = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 784]), torch.Size([40, 40]), torch.Size([10, 40]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned[0].shape, aligned[1].shape, aligned[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 2.3044, Accuracy: 1079/10000 (11%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 0.1808, Accuracy: 9481/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(30)\n",
    "validate(model.to(device), criterion, [], [])\n",
    "with torch.no_grad():\n",
    "    model.fc1.weight = nn.Parameter(aligned[0])\n",
    "    model.fc2.weight = nn.Parameter(aligned[1])\n",
    "    model.fc3.weight = nn.Parameter(aligned[2])\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0207,  0.0051, -0.0202,  ...,  0.0219, -0.0323, -0.0139],\n",
       "        [ 0.0073, -0.0114, -0.0273,  ..., -0.0283, -0.0110, -0.0046],\n",
       "        [ 0.0340,  0.0251,  0.0038,  ..., -0.0145, -0.0020,  0.0273],\n",
       "        ...,\n",
       "        [ 0.0190,  0.0293,  0.0113,  ..., -0.0142, -0.0200,  0.0269],\n",
       "        [-0.0128, -0.0034,  0.0224,  ...,  0.0225, -0.0146,  0.0101],\n",
       "        [ 0.0270, -0.0141,  0.0069,  ..., -0.0169, -0.0004,  0.0325]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['trained'][0].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0309,  0.0253, -0.0051,  ...,  0.0009,  0.0102,  0.0039],\n",
       "        [ 0.0318,  0.0110, -0.0021,  ..., -0.0198,  0.0022, -0.0010],\n",
       "        [-0.0328,  0.0331, -0.0344,  ...,  0.0220,  0.0088,  0.0129],\n",
       "        ...,\n",
       "        [ 0.0046, -0.0284,  0.0159,  ..., -0.0325,  0.0247, -0.0103],\n",
       "        [-0.0064,  0.0276,  0.0256,  ...,  0.0348,  0.0271,  0.0320],\n",
       "        [ 0.0125, -0.0147, -0.0315,  ..., -0.0075, -0.0270,  0.0183]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['trained'][1].fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 6.9154e-03, -2.2000e-03,  9.9058e-03,  ..., -1.3443e-02,\n",
       "          8.6483e-03,  2.2158e-02],\n",
       "        [ 1.5169e-02,  3.4674e-02, -3.2476e-03,  ..., -1.6691e-02,\n",
       "         -1.4302e-02,  1.8324e-02],\n",
       "        [ 2.0746e-02,  5.0795e-03, -2.0154e-02,  ...,  2.1901e-02,\n",
       "         -3.2274e-02, -1.3933e-02],\n",
       "        ...,\n",
       "        [ 2.7756e-02, -1.3339e-02, -1.3497e-03,  ...,  3.5403e-02,\n",
       "          6.9110e-03, -2.0211e-02],\n",
       "        [ 4.0680e-03,  4.9553e-03,  5.3620e-03,  ..., -1.4040e-03,\n",
       "          7.7146e-03, -5.0417e-05],\n",
       "        [ 6.4863e-04, -2.5920e-02, -7.5890e-03,  ..., -4.3384e-03,\n",
       "          8.5830e-03,  9.0196e-03]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Ts are or to be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0250,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.3333280086517334 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.0681, 2.2005, 1.5773,  ..., 1.3306, 2.1235, 1.5188],\n",
      "        [1.8381, 1.8590, 1.3707,  ..., 1.8699, 2.2082, 1.8303],\n",
      "        [1.4483, 1.4819, 1.4599,  ..., 1.8270, 1.6114, 1.4914],\n",
      "        ...,\n",
      "        [1.9778, 2.5447, 2.0899,  ..., 1.9904, 2.6248, 1.7526],\n",
      "        [1.8577, 2.1451, 1.6796,  ..., 1.7256, 2.3501, 2.0952],\n",
      "        [2.1402, 1.9953, 1.9925,  ..., 1.6596, 2.2325, 1.9417]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0083,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0083, device='cuda:0')\n",
      "Here, trace is 0.33333197236061096 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.8889, 3.9014, 3.0255, 3.6213, 4.0799, 3.3666, 2.9929, 3.2339, 3.7269,\n",
      "         3.4725],\n",
      "        [4.1093, 2.2386, 3.5851, 3.4406, 3.5570, 3.3916, 3.5701, 3.3251, 3.0391,\n",
      "         3.8246],\n",
      "        [3.1341, 3.1805, 1.3682, 3.1476, 3.3546, 3.8020, 3.3256, 2.6820, 3.3240,\n",
      "         3.7015],\n",
      "        [3.5118, 3.3178, 2.8397, 1.4569, 3.8191, 2.8449, 3.9636, 3.2775, 3.2381,\n",
      "         3.0634],\n",
      "        [3.9902, 3.7802, 3.4545, 4.0209, 1.5824, 3.2638, 3.0778, 3.8608, 3.2720,\n",
      "         3.0291],\n",
      "        [2.5545, 3.7321, 3.4729, 2.9361, 3.6750, 1.8687, 2.9387, 3.6026, 3.0168,\n",
      "         3.0204],\n",
      "        [2.7924, 3.7241, 3.2277, 4.0965, 2.9360, 3.1278, 1.6434, 3.9118, 3.2235,\n",
      "         3.9614],\n",
      "        [3.9627, 3.3706, 2.9983, 3.0136, 3.7295, 3.8780, 4.2161, 1.7809, 3.6494,\n",
      "         3.1830],\n",
      "        [3.0414, 3.9472, 3.1211, 3.0591, 3.8194, 2.7387, 3.4988, 3.6025, 2.0426,\n",
      "         3.2927],\n",
      "        [3.6410, 3.7555, 3.7653, 3.2728, 3.3598, 3.1128, 4.0683, 3.0550, 2.9676,\n",
      "         1.9152]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aligned1, T_vars \u001b[39m=\u001b[39m get_wassersteinized_layers_modularized(args, [models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]])\n\u001b[1;32m      2\u001b[0m aligned2, _ \u001b[39m=\u001b[39m get_wassersteinized_layers_modularized(args, [models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m], models[\u001b[39m'\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m]], T_vars_pre_computed\u001b[39m=\u001b[39mT_vars)\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "aligned1, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])\n",
    "aligned2, _ = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]], T_vars_pre_computed=T_vars)\n",
    "with torch.no_grad():\n",
    "    for a1, a2 in zip(aligned1, aligned2):\n",
    "        assert np.allclose(a1.cpu().numpy(), a2.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, -1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['untrained'][0].fc1.weight.get_device(), \\\n",
    "models['trained'][0].fc1.weight.get_device(), \\\n",
    "models['trained_short'][0].fc1.weight.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0250, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.3333280086517334 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.8945, 0.9240, 0.6995,  ..., 0.8888, 0.8086, 0.7528],\n",
      "        [0.8658, 0.7917, 0.5627,  ..., 0.6203, 0.6768, 0.7516],\n",
      "        [0.8063, 0.8640, 0.6984,  ..., 0.7560, 0.7478, 0.7956],\n",
      "        ...,\n",
      "        [0.8560, 0.8429, 0.7011,  ..., 0.8679, 0.8436, 0.9585],\n",
      "        [0.9103, 0.9317, 0.7989,  ..., 0.8956, 0.8398, 0.7310],\n",
      "        [0.8376, 0.9833, 0.7594,  ..., 0.7483, 0.8809, 0.8976]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0083],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999960064888 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9960, 0.8663, 0.9558, 0.7742, 0.9954, 0.7029, 0.8978, 0.7642, 0.7930,\n",
      "         0.9068],\n",
      "        [0.9720, 0.9268, 0.7333, 0.8659, 0.8982, 0.7128, 0.8153, 0.8836, 0.7961,\n",
      "         0.8545],\n",
      "        [0.8924, 0.7945, 0.9077, 0.7947, 0.7760, 0.8220, 0.8183, 0.8469, 0.8730,\n",
      "         0.7817],\n",
      "        [0.7980, 0.6982, 0.6338, 0.7572, 0.7894, 0.7155, 0.7212, 0.8102, 0.7365,\n",
      "         0.7103],\n",
      "        [0.7372, 0.7228, 0.8800, 0.8528, 0.7664, 0.8644, 0.8411, 0.8237, 0.7799,\n",
      "         0.8795],\n",
      "        [0.7650, 0.8746, 0.8646, 0.7769, 0.8248, 0.8868, 0.8455, 0.8113, 0.8322,\n",
      "         0.9249],\n",
      "        [0.8520, 0.9588, 0.8479, 0.7700, 0.9114, 0.7637, 0.7672, 0.8211, 0.9004,\n",
      "         0.9296],\n",
      "        [0.8893, 0.8020, 0.7986, 0.7827, 0.7663, 0.8741, 0.7878, 0.9220, 0.8777,\n",
      "         0.7572],\n",
      "        [0.9305, 0.7996, 0.6753, 0.7558, 0.8391, 0.8579, 0.8887, 0.7636, 0.6955,\n",
      "         0.8247],\n",
      "        [0.7339, 0.7948, 0.9384, 0.7597, 0.7262, 0.7472, 0.7035, 0.8167, 0.7481,\n",
      "         0.7239]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "Ratio of trace to the matrix sum:  tensor(0.0333, device='cuda:0')\n",
      "Here, trace is 1.3333280086517334 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.3754, 1.4542, 1.3266,  ..., 1.5363, 1.4526, 1.3594],\n",
      "        [1.5596, 1.6123, 1.4663,  ..., 1.5469, 1.4650, 1.5523],\n",
      "        [1.4044, 1.2862, 1.2279,  ..., 1.1700, 1.3601, 1.2305],\n",
      "        ...,\n",
      "        [1.7245, 1.7390, 1.6727,  ..., 1.8103, 1.8544, 1.8812],\n",
      "        [1.8072, 1.9747, 1.6852,  ..., 1.8161, 1.7797, 1.7916],\n",
      "        [1.6048, 1.8563, 1.5311,  ..., 1.5779, 1.7261, 1.7067]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999960064888 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.6885, 2.5987, 2.6285, 2.4699, 2.6642, 2.4845, 2.6375, 2.3983, 2.5840,\n",
      "         2.6587],\n",
      "        [2.6779, 2.6410, 2.5228, 2.6284, 2.7156, 2.4616, 2.5381, 2.6973, 2.5315,\n",
      "         2.5565],\n",
      "        [2.2073, 2.0439, 2.1687, 2.1222, 2.1953, 2.1823, 2.0952, 2.1708, 2.3156,\n",
      "         2.2001],\n",
      "        [2.2566, 2.1573, 2.2716, 2.3400, 2.4110, 2.3602, 2.3840, 2.3005, 2.2581,\n",
      "         2.2379],\n",
      "        [2.3574, 2.5626, 2.6263, 2.5386, 2.4369, 2.6149, 2.4918, 2.6226, 2.6285,\n",
      "         2.5687],\n",
      "        [2.0992, 2.1686, 2.0989, 2.1118, 2.1651, 2.1482, 2.1718, 1.9496, 1.9415,\n",
      "         2.2501],\n",
      "        [2.5476, 2.6209, 2.5349, 2.5110, 2.5625, 2.4222, 2.4774, 2.4571, 2.5136,\n",
      "         2.6883],\n",
      "        [2.4398, 2.5026, 2.4337, 2.4525, 2.3719, 2.5525, 2.5338, 2.5604, 2.4398,\n",
      "         2.4406],\n",
      "        [2.4433, 2.1668, 2.1107, 2.2326, 2.1430, 2.1457, 2.1720, 2.2427, 2.2034,\n",
      "         2.0965],\n",
      "        [2.3662, 2.4720, 2.5627, 2.4414, 2.3114, 2.5058, 2.4131, 2.5366, 2.4617,\n",
      "         2.2725]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned_untrained[0].shape == torch.Size([40, 784])\n",
      "models['aligned_untrained'][i].fc1.weight.shape == torch.Size([40, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0250,  ..., 0.0000, 0.0000, 0.0083],\n",
      "        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0417, device='cuda:0')\n",
      "Here, trace is 1.666659951210022 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.7602, 1.5729, 1.4934,  ..., 1.4279, 1.8148, 1.6191],\n",
      "        [1.5787, 1.5243, 1.6083,  ..., 1.7253, 1.7573, 1.7112],\n",
      "        [1.1375, 1.3537, 1.2753,  ..., 1.4798, 1.3638, 1.5217],\n",
      "        ...,\n",
      "        [1.8750, 1.8334, 1.8089,  ..., 1.8877, 2.1927, 1.7240],\n",
      "        [1.8468, 1.7544, 1.8148,  ..., 1.7891, 1.9423, 2.0092],\n",
      "        [1.8801, 1.6690, 1.7756,  ..., 1.6561, 1.7952, 1.7436]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999959468841553 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.8895, 3.6310, 3.0864, 3.0678, 3.3843, 2.7240, 3.1727, 3.0431, 3.0782,\n",
      "         3.1462],\n",
      "        [3.3805, 1.9094, 2.9798, 2.9346, 3.3148, 3.0013, 2.9472, 2.9611, 2.8316,\n",
      "         3.1490],\n",
      "        [2.4089, 2.6521, 1.6618, 2.4685, 2.7129, 2.6938, 2.5671, 2.6015, 2.4489,\n",
      "         2.7693],\n",
      "        [2.7710, 2.3541, 2.5277, 1.6364, 3.0632, 2.3161, 2.9746, 2.6773, 2.3277,\n",
      "         2.6738],\n",
      "        [3.2625, 2.9684, 2.6804, 3.0686, 1.7853, 2.8909, 2.4748, 2.9917, 3.0216,\n",
      "         2.4122],\n",
      "        [2.1862, 2.9468, 2.8584, 2.3068, 2.7060, 1.8988, 2.3073, 2.8427, 2.2731,\n",
      "         2.5170],\n",
      "        [2.5881, 3.0393, 2.6295, 3.0514, 2.5741, 2.6731, 2.0440, 3.1213, 2.8986,\n",
      "         2.9278],\n",
      "        [3.1200, 2.9982, 2.6380, 2.7104, 3.0988, 3.2403, 3.3960, 2.2886, 2.9808,\n",
      "         2.7093],\n",
      "        [2.7249, 2.9244, 2.6602, 2.8138, 2.9093, 2.2623, 2.6371, 2.7058, 2.0825,\n",
      "         2.5806],\n",
      "        [3.0301, 2.9359, 3.1117, 2.7323, 2.5168, 2.5677, 3.2156, 2.1905, 2.6672,\n",
      "         1.8511]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([40, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([40, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "Ratio of trace to the matrix sum:  tensor(0.0417, device='cuda:0')\n",
      "Here, trace is 1.666659951210022 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.5020, 1.3943, 1.2357,  ..., 1.2374, 1.4073, 1.3565],\n",
      "        [1.4344, 1.4625, 1.5486,  ..., 1.5824, 1.5520, 1.6815],\n",
      "        [0.8739, 1.1461, 1.0968,  ..., 1.1257, 1.0992, 1.2376],\n",
      "        ...,\n",
      "        [1.5357, 1.4943, 1.4535,  ..., 1.4034, 1.6549, 1.4206],\n",
      "        [1.5441, 1.6121, 1.5618,  ..., 1.5175, 1.6319, 1.7827],\n",
      "        [1.3417, 1.3253, 1.3075,  ..., 1.1877, 1.4130, 1.4166]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999959468841553 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.0227, 2.4345, 2.4471, 2.3366, 2.3028, 2.3631, 2.3904, 2.2838, 2.3360,\n",
      "         2.4294],\n",
      "        [2.3900, 1.9674, 2.3568, 2.2121, 2.3480, 2.2324, 2.1768, 2.1866, 2.3064,\n",
      "         2.2335],\n",
      "        [1.8746, 1.8164, 1.7050, 1.9486, 1.8522, 2.0708, 1.9258, 1.9398, 1.8999,\n",
      "         1.9915],\n",
      "        [2.0556, 1.9078, 2.0970, 1.7796, 2.2069, 2.0855, 2.1211, 2.0574, 2.0209,\n",
      "         2.1266],\n",
      "        [2.4555, 2.4133, 2.0761, 2.4406, 2.0361, 2.2358, 2.1626, 2.3577, 2.4408,\n",
      "         2.1132],\n",
      "        [1.7723, 2.0141, 2.1958, 1.7425, 2.0507, 1.8782, 1.8493, 2.0685, 1.8770,\n",
      "         1.9255],\n",
      "        [2.0873, 2.1667, 2.0841, 2.1576, 1.9831, 2.0106, 1.9084, 2.2328, 2.1235,\n",
      "         1.9696],\n",
      "        [2.3524, 2.3068, 2.3468, 2.3084, 2.3439, 2.5200, 2.4875, 2.1630, 2.3801,\n",
      "         2.5260],\n",
      "        [2.1747, 2.1971, 2.0110, 2.2169, 2.1690, 1.9745, 2.1007, 2.1842, 1.8728,\n",
      "         1.9661],\n",
      "        [2.1674, 2.1890, 2.2149, 2.1540, 2.1240, 2.0533, 2.2622, 1.9898, 2.1701,\n",
      "         2.1350]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned_diff[0].shape == torch.Size([40, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([40, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([50])\n",
      "inverse marginals beta is  tensor([0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 784])\n",
      "Previous layer shape is  torch.Size([50, 784])\n",
      "shape of layer: model 0 torch.Size([40, 40])\n",
      "shape of layer: model 1 torch.Size([50, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([40, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9088, 0.7837, 0.8664,  ..., 0.8804, 0.8972, 0.8028],\n",
      "        [0.7481, 0.8995, 0.8596,  ..., 0.8651, 0.8577, 0.8504],\n",
      "        [0.8040, 0.8034, 0.6809,  ..., 0.8072, 0.7181, 0.7399],\n",
      "        ...,\n",
      "        [0.7378, 0.6688, 0.7158,  ..., 0.7958, 0.8205, 0.7784],\n",
      "        [0.7397, 0.7603, 0.7364,  ..., 0.8072, 0.7439, 0.8972],\n",
      "        [0.9140, 0.8721, 0.8339,  ..., 0.8100, 0.8583, 0.8832]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0200, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([50])\n",
      "inverse marginals beta is  tensor([0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0300, device='cuda:0')\n",
      "Here, trace is 1.4999923706054688 and matrix sum is 49.9997444152832 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 40])\n",
      "Previous layer shape is  torch.Size([50, 50])\n",
      "shape of layer: model 0 torch.Size([10, 40])\n",
      "shape of layer: model 1 torch.Size([10, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([10, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.8408, 0.8005, 0.7909, 0.7154, 0.7380, 0.6806, 0.8553, 0.9132, 0.8385,\n",
      "         0.6692],\n",
      "        [0.8769, 0.6823, 0.7666, 0.8201, 0.7803, 0.7156, 0.8216, 0.9295, 0.7485,\n",
      "         0.7692],\n",
      "        [0.8529, 0.7436, 0.9460, 0.7931, 0.8572, 0.7829, 0.8092, 0.8806, 0.7690,\n",
      "         0.9330],\n",
      "        [0.7735, 0.7425, 0.7930, 0.6220, 0.8052, 0.7919, 0.7347, 0.7894, 0.7411,\n",
      "         0.6657],\n",
      "        [0.7404, 0.7683, 0.7446, 0.7665, 0.7968, 0.6784, 0.7926, 0.9603, 0.7474,\n",
      "         0.7862],\n",
      "        [0.7378, 0.8453, 0.8105, 0.7597, 0.8575, 0.8308, 0.7900, 0.7456, 0.6780,\n",
      "         0.7487],\n",
      "        [0.6942, 0.8208, 0.7528, 0.7563, 0.8140, 0.8053, 0.7461, 0.7546, 0.7410,\n",
      "         0.7321],\n",
      "        [0.8928, 0.8453, 0.8382, 0.7681, 0.7250, 0.7993, 0.8946, 0.8249, 0.7607,\n",
      "         0.7494],\n",
      "        [0.7379, 0.7526, 0.8161, 0.6266, 0.9106, 0.7425, 0.7738, 0.8474, 0.7422,\n",
      "         0.7686],\n",
      "        [0.8173, 0.7855, 0.8076, 0.6850, 0.8805, 0.7556, 0.8565, 0.8134, 0.7818,\n",
      "         0.8372]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 40])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 784])\n",
      "Previous layer shape is  torch.Size([50, 784])\n",
      "shape of layer: model 0 torch.Size([40, 40])\n",
      "shape of layer: model 1 torch.Size([50, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([40, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.3410, 1.2260, 1.3801,  ..., 1.3359, 1.4086, 1.3722],\n",
      "        [1.2592, 1.2448, 1.3366,  ..., 1.2821, 1.2485, 1.3422],\n",
      "        [1.1430, 1.0673, 1.1423,  ..., 1.2525, 1.2070, 1.1122],\n",
      "        ...,\n",
      "        [1.1317, 1.1907, 1.1550,  ..., 1.1908, 1.2319, 1.1672],\n",
      "        [1.4704, 1.4295, 1.4781,  ..., 1.4247, 1.3700, 1.5994],\n",
      "        [0.9150, 0.8733, 0.8302,  ..., 0.8089, 0.8744, 0.8809]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "Ratio of trace to the matrix sum:  tensor(0.0300, device='cuda:0')\n",
      "Here, trace is 1.4999923706054688 and matrix sum is 49.9997444152832 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 40])\n",
      "Previous layer shape is  torch.Size([50, 50])\n",
      "shape of layer: model 0 torch.Size([10, 40])\n",
      "shape of layer: model 1 torch.Size([10, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([10, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.4207, 2.3651, 2.3926, 2.3561, 2.3182, 2.2275, 2.4468, 2.3538, 2.4338,\n",
      "         2.2474],\n",
      "        [2.7736, 2.6182, 2.6509, 2.7287, 2.7443, 2.6997, 2.7446, 2.8227, 2.6130,\n",
      "         2.6636],\n",
      "        [2.1591, 2.0120, 2.2633, 2.0926, 2.1383, 2.0559, 2.1216, 2.2059, 2.0941,\n",
      "         2.2150],\n",
      "        [2.2634, 2.2558, 2.2140, 2.1436, 2.1543, 2.3295, 2.1707, 2.2600, 2.2215,\n",
      "         2.1946],\n",
      "        [2.4845, 2.5591, 2.4832, 2.5321, 2.5321, 2.4612, 2.5136, 2.6392, 2.5390,\n",
      "         2.6076],\n",
      "        [1.9486, 2.1382, 2.0022, 2.0521, 2.1248, 2.0973, 1.9902, 2.0163, 1.9329,\n",
      "         1.9109],\n",
      "        [2.2835, 2.2797, 2.3495, 2.3773, 2.4326, 2.3067, 2.3112, 2.2534, 2.3139,\n",
      "         2.2792],\n",
      "        [2.3872, 2.3409, 2.3130, 2.2206, 2.2241, 2.3454, 2.3787, 2.3152, 2.2895,\n",
      "         2.2570],\n",
      "        [2.0507, 2.1103, 2.1831, 2.0431, 2.3246, 2.1096, 2.1226, 2.1128, 2.0621,\n",
      "         2.1660],\n",
      "        [2.2071, 2.2720, 2.2046, 2.2247, 2.0733, 2.2484, 2.2221, 2.1682, 2.3497,\n",
      "         2.3523]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 40])\n",
      "aligned_untrained[0].shape == torch.Size([50, 784])\n",
      "models['aligned_untrained'][i].fc1.weight.shape == torch.Size([50, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "the transport map is  tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0000e-02, 3.4694e-18,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.0000e-02,\n",
      "         0.0000e+00]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([50])\n",
      "inverse marginals beta is  tensor([0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0050, device='cuda:0')\n",
      "Here, trace is 0.24999873340129852 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 784])\n",
      "Previous layer shape is  torch.Size([50, 784])\n",
      "shape of layer: model 0 torch.Size([40, 40])\n",
      "shape of layer: model 1 torch.Size([50, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([40, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.3105, 1.5713, 1.6422,  ..., 1.4414, 1.7111, 1.8189],\n",
      "        [1.4357, 1.2172, 1.5052,  ..., 1.6563, 1.6642, 1.7502],\n",
      "        [1.2030, 1.3695, 1.4794,  ..., 1.3706, 1.2620, 1.6161],\n",
      "        ...,\n",
      "        [1.4492, 1.4031, 1.2931,  ..., 1.5417, 1.1499, 1.2775],\n",
      "        [1.6595, 1.2445, 1.6805,  ..., 1.6891, 1.9132, 1.8451],\n",
      "        [0.8511, 1.0336, 1.0107,  ..., 1.0605, 1.1943, 1.1269]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0200, 0.0000],\n",
      "        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([50])\n",
      "inverse marginals beta is  tensor([0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200, 0.0200,\n",
      "        0.0200, 0.0200, 0.0200, 0.0200, 0.0200], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0200, device='cuda:0')\n",
      "Here, trace is 0.9999949336051941 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 40])\n",
      "Previous layer shape is  torch.Size([50, 50])\n",
      "shape of layer: model 0 torch.Size([10, 40])\n",
      "shape of layer: model 1 torch.Size([10, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([10, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.8803, 3.2604, 2.3896, 2.8523, 2.9447, 2.3277, 2.3295, 2.6505, 2.7013,\n",
      "         2.6831],\n",
      "        [3.3671, 1.9615, 2.8995, 2.7538, 3.0570, 2.9240, 2.7523, 2.9569, 2.8984,\n",
      "         3.0973],\n",
      "        [2.5665, 2.6417, 1.6322, 2.4129, 2.6134, 2.7607, 2.5285, 2.5325, 2.4672,\n",
      "         2.6434],\n",
      "        [2.4620, 2.9044, 2.3731, 1.4485, 3.1653, 2.3974, 3.1740, 2.6253, 2.6290,\n",
      "         2.7402],\n",
      "        [3.1368, 3.1175, 2.8620, 3.1832, 1.7448, 2.9014, 2.6373, 3.1068, 2.8016,\n",
      "         2.6060],\n",
      "        [2.1482, 2.9832, 2.7904, 2.3110, 2.6798, 1.7899, 2.6931, 2.7833, 2.3831,\n",
      "         2.4322],\n",
      "        [2.7427, 3.0669, 2.5338, 3.2240, 2.6121, 2.6500, 1.8317, 3.2977, 3.0022,\n",
      "         2.8607],\n",
      "        [3.0819, 2.6415, 2.7724, 2.5338, 2.7569, 2.9868, 3.1684, 1.7415, 2.6920,\n",
      "         2.2259],\n",
      "        [2.7877, 2.2533, 2.3939, 2.4969, 2.7458, 2.2421, 2.6354, 2.8931, 1.7935,\n",
      "         2.6096],\n",
      "        [2.7598, 2.8843, 2.9129, 2.5198, 2.3398, 2.3002, 2.9514, 2.4271, 2.3707,\n",
      "         1.9668]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 40])\n",
      "aligned[0].shape == torch.Size([50, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([50, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "Ratio of trace to the matrix sum:  tensor(0.0050, device='cuda:0')\n",
      "Here, trace is 0.24999873340129852 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 784])\n",
      "Previous layer shape is  torch.Size([50, 784])\n",
      "shape of layer: model 0 torch.Size([40, 40])\n",
      "shape of layer: model 1 torch.Size([50, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([40, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9098, 1.0669, 1.1329,  ..., 0.9069, 1.0848, 1.0346],\n",
      "        [1.2407, 1.0798, 1.1357,  ..., 1.2023, 1.2110, 1.1488],\n",
      "        [0.9792, 1.0816, 1.0672,  ..., 1.0522, 0.8831, 1.1161],\n",
      "        ...,\n",
      "        [1.1436, 1.1505, 0.9696,  ..., 1.1729, 0.9226, 0.9837],\n",
      "        [1.3405, 1.0959, 1.2903,  ..., 1.2915, 1.3660, 1.2878],\n",
      "        [0.6049, 0.6316, 0.5022,  ..., 0.6403, 0.6084, 0.6096]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  40\n",
      "returns a uniform measure of cardinality:  50\n",
      "Ratio of trace to the matrix sum:  tensor(0.0200, device='cuda:0')\n",
      "Here, trace is 0.9999949336051941 and matrix sum is 49.99974822998047 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([40, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([40, 40])\n",
      "Previous layer shape is  torch.Size([50, 50])\n",
      "shape of layer: model 0 torch.Size([10, 40])\n",
      "shape of layer: model 1 torch.Size([10, 50])\n",
      "shape of previous transport map torch.Size([40, 50])\n",
      "torch.Size([10, 40])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.8739, 2.0623, 1.9504, 2.1073, 2.1202, 2.0108, 1.8809, 1.9579, 2.0694,\n",
      "         2.0182],\n",
      "        [2.3921, 2.0511, 2.2659, 2.2289, 2.2305, 2.1567, 2.2960, 2.3210, 2.2362,\n",
      "         2.2963],\n",
      "        [1.6291, 1.6894, 1.4731, 1.7251, 1.7256, 1.8004, 1.7139, 1.7216, 1.7636,\n",
      "         1.7797],\n",
      "        [2.0065, 2.1456, 2.0423, 1.7066, 2.1529, 2.0646, 2.1766, 2.1160, 2.0953,\n",
      "         2.0565],\n",
      "        [2.3602, 2.3043, 2.2798, 2.3451, 1.9939, 2.2076, 2.2233, 2.3042, 2.2197,\n",
      "         2.2809],\n",
      "        [1.7423, 1.9762, 2.0536, 1.8633, 1.9887, 1.9150, 1.9110, 2.0110, 1.8951,\n",
      "         1.9040],\n",
      "        [2.0879, 2.1506, 2.0046, 2.3253, 2.2006, 2.1287, 1.9388, 2.2369, 2.2351,\n",
      "         2.0545],\n",
      "        [2.1220, 1.9407, 2.0946, 1.9284, 1.9072, 2.0922, 2.0598, 1.8094, 1.9784,\n",
      "         1.9770],\n",
      "        [1.8096, 1.6967, 1.6773, 1.7604, 1.8708, 1.6635, 1.8570, 1.8626, 1.6368,\n",
      "         1.7020],\n",
      "        [2.2211, 2.1446, 2.3423, 2.1012, 2.0335, 2.0807, 2.2028, 2.0552, 2.0877,\n",
      "         2.0658]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 50])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 40])\n",
      "aligned_diff[0].shape == torch.Size([50, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([50, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0100, device='cuda:0')\n",
      "Here, trace is 0.5999963283538818 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "shape of layer: model 0 torch.Size([50, 50])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([50, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.8130, 0.7371, 0.7048,  ..., 0.8743, 0.7780, 0.7202],\n",
      "        [0.8814, 0.8459, 0.8290,  ..., 0.8285, 0.8529, 0.7695],\n",
      "        [0.7983, 0.7694, 0.7657,  ..., 0.8203, 0.6979, 0.7475],\n",
      "        ...,\n",
      "        [0.8512, 0.8000, 0.8305,  ..., 0.8445, 0.9158, 0.8867],\n",
      "        [0.8402, 0.8744, 0.8445,  ..., 0.8487, 0.8079, 0.7717],\n",
      "        [0.8435, 0.7939, 0.8530,  ..., 0.7314, 0.7724, 0.7071]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0133, device='cuda:0')\n",
      "Here, trace is 0.7999952435493469 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 50])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([10, 50])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([10, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.7837, 0.8485, 0.8465, 0.6915, 0.8797, 0.7458, 0.7210, 0.7894, 0.7824,\n",
      "         0.8206],\n",
      "        [0.7782, 0.8079, 0.7601, 0.6223, 0.8734, 0.6824, 0.7203, 0.8089, 0.7678,\n",
      "         0.6922],\n",
      "        [0.7888, 0.7713, 0.7959, 0.6902, 0.8899, 0.7846, 0.6730, 0.7558, 0.8070,\n",
      "         0.8239],\n",
      "        [0.8364, 0.7360, 0.7484, 0.7287, 0.8023, 0.7896, 0.7442, 0.6932, 0.8185,\n",
      "         0.8225],\n",
      "        [0.7975, 0.8214, 0.8583, 0.7424, 0.7333, 0.7930, 0.8000, 0.7727, 0.8223,\n",
      "         0.7858],\n",
      "        [0.7947, 0.7739, 0.7175, 0.7026, 0.7524, 0.7494, 0.6826, 0.8397, 0.7834,\n",
      "         0.7188],\n",
      "        [0.7855, 0.9531, 0.8924, 0.6497, 0.8755, 0.8529, 0.7348, 0.8351, 0.8315,\n",
      "         0.7699],\n",
      "        [0.8291, 0.7595, 0.9136, 0.7956, 0.8887, 0.7886, 0.8140, 0.9214, 0.7879,\n",
      "         0.7891],\n",
      "        [0.7824, 0.7269, 0.7921, 0.6729, 0.8124, 0.7461, 0.8481, 0.8642, 0.7552,\n",
      "         0.7197],\n",
      "        [0.7499, 0.7648, 0.7250, 0.6591, 0.8122, 0.7044, 0.6517, 0.7285, 0.7779,\n",
      "         0.7585]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000],\n",
      "        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 50])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0100, device='cuda:0')\n",
      "Here, trace is 0.5999963283538818 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "shape of layer: model 0 torch.Size([50, 50])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([50, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.1949, 1.1291, 1.0690,  ..., 1.2326, 1.1989, 1.1323],\n",
      "        [1.5754, 1.6154, 1.6733,  ..., 1.4969, 1.5424, 1.4984],\n",
      "        [1.1769, 1.2072, 1.1989,  ..., 1.2012, 1.1478, 1.1425],\n",
      "        ...,\n",
      "        [1.3922, 1.2984, 1.3513,  ..., 1.3945, 1.5048, 1.3784],\n",
      "        [1.4568, 1.4873, 1.4503,  ..., 1.4031, 1.4228, 1.4262],\n",
      "        [1.3470, 1.3163, 1.3479,  ..., 1.2914, 1.3758, 1.3407]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0133, device='cuda:0')\n",
      "Here, trace is 0.7999952435493469 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 50])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([10, 50])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([10, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.2911, 2.4022, 2.4314, 2.3585, 2.4572, 2.4329, 2.3510, 2.4918, 2.3747,\n",
      "         2.3698],\n",
      "        [2.6250, 2.4899, 2.4700, 2.4481, 2.6442, 2.4522, 2.5006, 2.4990, 2.5196,\n",
      "         2.4865],\n",
      "        [2.1103, 2.1334, 2.1750, 2.1027, 2.1552, 2.1311, 2.0527, 2.0531, 2.1943,\n",
      "         2.1634],\n",
      "        [2.1584, 2.0190, 2.1322, 2.1964, 2.1182, 2.1195, 2.0784, 1.9520, 2.1440,\n",
      "         2.2237],\n",
      "        [2.4448, 2.5201, 2.4409, 2.3763, 2.4211, 2.4374, 2.4416, 2.3957, 2.4128,\n",
      "         2.4874],\n",
      "        [2.2544, 2.3508, 2.2570, 2.2154, 2.1113, 2.2081, 2.1652, 2.2853, 2.2386,\n",
      "         2.1712],\n",
      "        [2.4143, 2.6683, 2.4656, 2.1980, 2.5305, 2.5003, 2.3533, 2.5428, 2.5323,\n",
      "         2.3588],\n",
      "        [2.3457, 2.1555, 2.3488, 2.3707, 2.3144, 2.3251, 2.3013, 2.3494, 2.3088,\n",
      "         2.2785],\n",
      "        [2.3973, 2.2807, 2.2861, 2.3105, 2.3695, 2.3840, 2.5399, 2.4344, 2.3052,\n",
      "         2.2809],\n",
      "        [2.1541, 2.1635, 2.2587, 2.3358, 2.1995, 2.1145, 2.2340, 2.2008, 2.1800,\n",
      "         2.3258]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(0.1000, device='cuda:0')\n",
      "Here, trace is 0.9999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 50])\n",
      "aligned_untrained[0].shape == torch.Size([60, 784])\n",
      "models['aligned_untrained'][i].fc1.weight.shape == torch.Size([60, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "shape of layer: model 0 torch.Size([50, 50])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([50, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4253, 1.5247, 1.2703,  ..., 1.4594, 1.1861, 1.3398],\n",
      "        [1.6281, 1.5596, 1.7064,  ..., 1.7200, 1.5990, 1.8095],\n",
      "        [1.3620, 1.4755, 1.4019,  ..., 1.1660, 1.1996, 1.2554],\n",
      "        ...,\n",
      "        [1.3286, 1.4580, 1.5784,  ..., 1.0612, 1.5954, 1.7037],\n",
      "        [1.6707, 1.1960, 1.3419,  ..., 1.4881, 1.7091, 1.5549],\n",
      "        [1.3922, 1.4841, 1.6351,  ..., 1.1917, 1.5198, 1.6182]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([60])\n",
      "inverse marginals beta is  tensor([0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "        0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 50])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([10, 50])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([10, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.0257, 3.3910, 2.6220, 2.9935, 3.1663, 2.7250, 2.6833, 2.8311, 2.8580,\n",
      "         2.8625],\n",
      "        [3.2504, 2.1118, 2.7191, 2.7071, 2.9450, 2.9925, 2.9848, 2.8186, 2.9304,\n",
      "         2.9212],\n",
      "        [2.8499, 3.0661, 1.8338, 2.3391, 2.8344, 2.9329, 2.7126, 2.6583, 2.4454,\n",
      "         2.7043],\n",
      "        [2.6495, 2.7336, 2.5291, 1.5705, 3.0542, 2.4708, 3.0544, 2.5340, 2.3542,\n",
      "         2.4018],\n",
      "        [3.0315, 2.8220, 2.8912, 2.9610, 2.0327, 2.6019, 2.5340, 3.1270, 2.8188,\n",
      "         2.5852],\n",
      "        [2.3152, 3.1429, 3.0279, 2.6923, 2.6832, 1.9152, 2.5339, 2.9185, 2.7469,\n",
      "         2.5741],\n",
      "        [2.6252, 3.1833, 2.7658, 3.2661, 2.6997, 2.7265, 1.9087, 3.2423, 2.9448,\n",
      "         3.0577],\n",
      "        [3.0309, 2.6806, 2.4788, 2.5275, 2.7430, 2.9270, 3.1735, 1.7392, 2.7725,\n",
      "         2.2037],\n",
      "        [2.8451, 2.6511, 2.7673, 2.8341, 2.8714, 2.3973, 2.9102, 3.0273, 1.9232,\n",
      "         2.8187],\n",
      "        [2.9080, 2.6515, 2.9971, 2.6378, 2.4956, 2.4974, 2.8074, 2.6452, 2.5239,\n",
      "         2.0758]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 50])\n",
      "aligned[0].shape == torch.Size([60, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([60, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 784])\n",
      "Previous layer shape is  torch.Size([60, 784])\n",
      "shape of layer: model 0 torch.Size([50, 50])\n",
      "shape of layer: model 1 torch.Size([60, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([50, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9771, 0.9983, 0.9709,  ..., 0.9980, 0.9275, 0.9832],\n",
      "        [1.2838, 1.1923, 1.2434,  ..., 1.2054, 1.1917, 1.2331],\n",
      "        [0.8407, 0.9011, 1.0237,  ..., 0.8190, 0.8725, 0.8550],\n",
      "        ...,\n",
      "        [1.0391, 1.0513, 1.0474,  ..., 0.8294, 1.1030, 1.0771],\n",
      "        [1.1279, 0.9753, 1.0482,  ..., 1.1100, 1.2502, 1.0920],\n",
      "        [1.0938, 1.0983, 1.2066,  ..., 1.0223, 1.2063, 1.1202]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  50\n",
      "returns a uniform measure of cardinality:  60\n",
      "Ratio of trace to the matrix sum:  tensor(0.0167, device='cuda:0')\n",
      "Here, trace is 0.9999939203262329 and matrix sum is 59.9996337890625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([50, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([50, 50])\n",
      "Previous layer shape is  torch.Size([60, 60])\n",
      "shape of layer: model 0 torch.Size([10, 50])\n",
      "shape of layer: model 1 torch.Size([10, 60])\n",
      "shape of previous transport map torch.Size([50, 60])\n",
      "torch.Size([10, 50])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.0611, 2.3987, 2.1911, 2.2333, 2.3953, 2.2431, 2.0624, 2.2403, 2.2935,\n",
      "         2.2577],\n",
      "        [2.2764, 2.0346, 2.2226, 2.2399, 2.1711, 2.2451, 2.3492, 2.1841, 2.3274,\n",
      "         2.1716],\n",
      "        [2.1063, 2.0337, 1.8105, 1.7353, 2.0617, 2.0954, 1.9288, 1.9224, 1.9040,\n",
      "         2.0244],\n",
      "        [1.8218, 1.8914, 1.8633, 1.6143, 2.0307, 1.8818, 1.9616, 1.8028, 1.8808,\n",
      "         1.8117],\n",
      "        [2.1321, 2.0696, 2.0367, 2.1329, 1.8682, 2.0769, 2.1074, 2.2846, 2.1224,\n",
      "         2.0582],\n",
      "        [2.0163, 2.2390, 2.2891, 2.2390, 2.1957, 1.9281, 2.0307, 2.1797, 2.2207,\n",
      "         2.1107],\n",
      "        [2.1811, 2.2828, 2.2237, 2.3527, 2.3116, 2.1752, 1.9973, 2.2881, 2.2878,\n",
      "         2.2159],\n",
      "        [2.0997, 1.9016, 1.8882, 1.8380, 1.9577, 2.0620, 2.0609, 1.7419, 1.9984,\n",
      "         1.9302],\n",
      "        [2.0798, 2.0066, 2.1401, 2.1820, 2.0093, 1.9560, 2.1301, 2.1017, 1.7722,\n",
      "         2.2207],\n",
      "        [2.1098, 2.0462, 2.1596, 1.9792, 1.9248, 2.1038, 2.1398, 2.1252, 2.0712,\n",
      "         1.9408]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 60])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 50])\n",
      "aligned_diff[0].shape == torch.Size([60, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([60, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0143, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0143, device='cuda:0')\n",
      "Here, trace is 0.9999930262565613 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 784])\n",
      "Previous layer shape is  torch.Size([70, 784])\n",
      "shape of layer: model 0 torch.Size([60, 60])\n",
      "shape of layer: model 1 torch.Size([70, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.8490, 0.9008, 0.7851,  ..., 0.8348, 0.7784, 0.8614],\n",
      "        [0.8170, 0.7608, 0.8042,  ..., 0.7467, 0.7744, 0.8177],\n",
      "        [0.8086, 0.7942, 0.8346,  ..., 0.8396, 0.7964, 0.7438],\n",
      "        ...,\n",
      "        [0.8394, 0.7662, 0.7552,  ..., 0.8613, 0.7846, 0.7395],\n",
      "        [0.7499, 0.7970, 0.7052,  ..., 0.8798, 0.7920, 0.7153],\n",
      "        [0.7844, 0.6800, 0.7479,  ..., 0.8625, 0.7751, 0.6899]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0048, device='cuda:0')\n",
      "Here, trace is 0.33333098888397217 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 60])\n",
      "Previous layer shape is  torch.Size([70, 70])\n",
      "shape of layer: model 0 torch.Size([10, 60])\n",
      "shape of layer: model 1 torch.Size([10, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.7865, 0.8419, 0.8419, 0.7754, 0.7781, 0.6841, 0.7903, 0.7924, 0.7779,\n",
      "         0.7677],\n",
      "        [0.7383, 0.9346, 0.7292, 0.8025, 0.7803, 0.6819, 0.7224, 0.8839, 0.8000,\n",
      "         0.7451],\n",
      "        [0.8055, 0.7318, 0.7816, 0.8290, 0.8528, 0.6743, 0.7825, 0.8002, 0.8337,\n",
      "         0.7324],\n",
      "        [0.6831, 0.7732, 0.6374, 0.7076, 0.6793, 0.5903, 0.7100, 0.7122, 0.7721,\n",
      "         0.6736],\n",
      "        [0.7970, 0.7875, 0.7935, 0.8938, 0.8629, 0.7800, 0.8132, 0.8641, 0.7999,\n",
      "         0.7513],\n",
      "        [0.7630, 0.7946, 0.7877, 0.8713, 0.7557, 0.6838, 0.7643, 0.8153, 0.8267,\n",
      "         0.7255],\n",
      "        [0.6944, 0.9003, 0.7236, 0.7848, 0.7096, 0.7548, 0.7856, 0.7940, 0.7364,\n",
      "         0.7804],\n",
      "        [0.7760, 0.8754, 0.8317, 0.8706, 0.8765, 0.7619, 0.7021, 0.8651, 0.7605,\n",
      "         0.7678],\n",
      "        [0.7784, 0.8211, 0.7874, 0.9003, 0.7995, 0.7643, 0.8097, 0.9060, 0.8243,\n",
      "         0.7318],\n",
      "        [0.8216, 0.7618, 0.7230, 0.8385, 0.7084, 0.6850, 0.7526, 0.7806, 0.7523,\n",
      "         0.6439]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.2000, device='cuda:0')\n",
      "Here, trace is 1.9999980926513672 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 60])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "Ratio of trace to the matrix sum:  tensor(0.0143, device='cuda:0')\n",
      "Here, trace is 0.9999930262565613 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 784])\n",
      "Previous layer shape is  torch.Size([70, 784])\n",
      "shape of layer: model 0 torch.Size([60, 60])\n",
      "shape of layer: model 1 torch.Size([70, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.2722, 1.3915, 1.2563,  ..., 1.3271, 1.1919, 1.2979],\n",
      "        [1.5451, 1.4249, 1.6278,  ..., 1.3921, 1.4582, 1.4825],\n",
      "        [1.3901, 1.2489, 1.4284,  ..., 1.2694, 1.3003, 1.2437],\n",
      "        ...,\n",
      "        [1.3898, 1.2668, 1.2799,  ..., 1.3390, 1.1405, 1.0911],\n",
      "        [1.1056, 1.1518, 1.0435,  ..., 1.2795, 1.1984, 1.1487],\n",
      "        [1.4140, 1.2675, 1.3875,  ..., 1.3808, 1.3832, 1.2497]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "Ratio of trace to the matrix sum:  tensor(0.0048, device='cuda:0')\n",
      "Here, trace is 0.33333098888397217 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 60])\n",
      "Previous layer shape is  torch.Size([70, 70])\n",
      "shape of layer: model 0 torch.Size([10, 60])\n",
      "shape of layer: model 1 torch.Size([10, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.3472, 2.3209, 2.4129, 2.2664, 2.3050, 2.2961, 2.3595, 2.2233, 2.2644,\n",
      "         2.3107],\n",
      "        [2.2834, 2.4422, 2.2651, 2.2985, 2.3825, 2.2402, 2.2317, 2.4219, 2.3568,\n",
      "         2.3551],\n",
      "        [2.1253, 2.1421, 2.1346, 2.1016, 2.2416, 2.1180, 2.2067, 2.2265, 2.3335,\n",
      "         2.1731],\n",
      "        [2.0314, 2.0493, 1.9261, 2.1041, 1.9890, 1.9318, 2.0028, 2.0220, 2.1042,\n",
      "         2.0369],\n",
      "        [2.3428, 2.3606, 2.3193, 2.3679, 2.3342, 2.4118, 2.4090, 2.3719, 2.3164,\n",
      "         2.2603],\n",
      "        [2.1989, 2.2603, 2.3012, 2.3108, 2.1519, 2.1841, 2.2579, 2.1733, 2.2526,\n",
      "         2.2110],\n",
      "        [2.2352, 2.3775, 2.2535, 2.2222, 2.1995, 2.3529, 2.3476, 2.2628, 2.2371,\n",
      "         2.3684],\n",
      "        [2.3195, 2.3721, 2.3437, 2.2852, 2.4616, 2.2541, 2.2158, 2.3373, 2.2452,\n",
      "         2.2720],\n",
      "        [2.2415, 2.1400, 2.1948, 2.3138, 2.1253, 2.2378, 2.1982, 2.3593, 2.2604,\n",
      "         2.1887],\n",
      "        [2.4794, 2.3443, 2.4415, 2.5557, 2.4587, 2.3819, 2.3738, 2.4087, 2.3352,\n",
      "         2.3312]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(0.2000, device='cuda:0')\n",
      "Here, trace is 1.9999980926513672 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 60])\n",
      "aligned_untrained[0].shape == torch.Size([70, 784])\n",
      "models['aligned_untrained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0071, device='cuda:0')\n",
      "Here, trace is 0.49999648332595825 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 784])\n",
      "Previous layer shape is  torch.Size([70, 784])\n",
      "shape of layer: model 0 torch.Size([60, 60])\n",
      "shape of layer: model 1 torch.Size([70, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.1902, 1.1917, 1.3680,  ..., 1.3907, 1.4417, 1.3715],\n",
      "        [1.5920, 1.3199, 1.5901,  ..., 1.7299, 1.4830, 1.4799],\n",
      "        [1.3649, 1.2803, 1.3049,  ..., 1.6525, 1.2397, 1.1372],\n",
      "        ...,\n",
      "        [1.2005, 1.1542, 1.5485,  ..., 1.1465, 1.3552, 1.3365],\n",
      "        [1.1878, 1.3866, 1.1683,  ..., 1.4057, 1.3651, 1.4276],\n",
      "        [1.4619, 1.4460, 1.4147,  ..., 1.5719, 1.2801, 1.2639]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0048],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0071, device='cuda:0')\n",
      "Here, trace is 0.49999648332595825 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 60])\n",
      "Previous layer shape is  torch.Size([70, 70])\n",
      "shape of layer: model 0 torch.Size([10, 60])\n",
      "shape of layer: model 1 torch.Size([10, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.9373, 3.4421, 2.6229, 2.7455, 2.8508, 2.4391, 2.6799, 2.8020, 2.9093,\n",
      "         2.8027],\n",
      "        [3.1438, 1.9370, 2.6237, 2.7964, 3.2837, 2.8520, 2.8683, 2.7635, 2.7399,\n",
      "         3.0535],\n",
      "        [2.6421, 2.7131, 1.5690, 2.4675, 2.9097, 2.9188, 2.4904, 2.5436, 2.5863,\n",
      "         2.8520],\n",
      "        [2.7070, 2.5047, 2.3874, 1.8589, 3.0686, 2.5114, 2.9489, 2.4106, 2.3188,\n",
      "         2.6363],\n",
      "        [3.0821, 2.7153, 2.9462, 3.1199, 1.8747, 2.8474, 2.7203, 2.8528, 2.8391,\n",
      "         2.4997],\n",
      "        [2.4089, 3.0007, 3.0369, 2.3488, 2.5918, 1.6012, 2.5393, 3.0982, 2.4113,\n",
      "         2.5582],\n",
      "        [2.6028, 2.6811, 2.5798, 2.9925, 2.4508, 2.3901, 1.5144, 3.2304, 2.8519,\n",
      "         2.9569],\n",
      "        [2.7804, 2.8090, 2.5950, 2.7092, 2.8535, 3.0493, 3.0222, 1.8267, 2.9830,\n",
      "         2.4518],\n",
      "        [3.0452, 2.9095, 2.5960, 2.5810, 2.7476, 2.3971, 2.9073, 3.0549, 1.7768,\n",
      "         2.6567],\n",
      "        [3.0780, 3.1500, 3.2866, 2.9717, 2.5950, 2.7532, 3.3746, 2.6171, 2.7323,\n",
      "         1.9917]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 60])\n",
      "aligned[0].shape == torch.Size([70, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "Ratio of trace to the matrix sum:  tensor(0.0071, device='cuda:0')\n",
      "Here, trace is 0.49999648332595825 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 784])\n",
      "Previous layer shape is  torch.Size([70, 784])\n",
      "shape of layer: model 0 torch.Size([60, 60])\n",
      "shape of layer: model 1 torch.Size([70, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([60, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[0.9640, 1.0332, 1.0358,  ..., 1.0242, 1.1971, 1.0485],\n",
      "        [1.2936, 1.1723, 1.1922,  ..., 1.3021, 1.2171, 1.1136],\n",
      "        [1.1267, 1.0642, 0.9994,  ..., 1.2205, 1.0558, 0.9156],\n",
      "        ...,\n",
      "        [0.9896, 1.0342, 1.1427,  ..., 0.9780, 1.0959, 1.0257],\n",
      "        [1.0137, 1.0264, 0.9173,  ..., 1.0703, 1.0669, 0.9813],\n",
      "        [1.2149, 1.1365, 1.1080,  ..., 1.2130, 1.1108, 0.9930]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  60\n",
      "returns a uniform measure of cardinality:  70\n",
      "Ratio of trace to the matrix sum:  tensor(0.0071, device='cuda:0')\n",
      "Here, trace is 0.49999648332595825 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([60, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([60, 60])\n",
      "Previous layer shape is  torch.Size([70, 70])\n",
      "shape of layer: model 0 torch.Size([10, 60])\n",
      "shape of layer: model 1 torch.Size([10, 70])\n",
      "shape of previous transport map torch.Size([60, 70])\n",
      "torch.Size([10, 60])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.9484, 2.3307, 2.0313, 2.1587, 2.0587, 1.9906, 2.0795, 2.0320, 2.2068,\n",
      "         2.0954],\n",
      "        [2.0775, 1.8005, 2.0416, 2.1089, 2.2452, 2.1007, 2.0980, 2.1149, 2.1083,\n",
      "         2.1198],\n",
      "        [1.9265, 2.0919, 1.7118, 1.9325, 2.1653, 2.1238, 2.0335, 1.9694, 2.0199,\n",
      "         2.0944],\n",
      "        [1.8903, 1.9123, 1.9324, 1.6641, 1.9779, 1.9782, 2.0134, 1.8813, 1.7568,\n",
      "         1.9862],\n",
      "        [2.0734, 1.9711, 2.0890, 2.2003, 1.8055, 2.0727, 2.0070, 2.1072, 2.1458,\n",
      "         2.0158],\n",
      "        [1.9263, 2.0380, 2.1588, 1.8960, 1.9233, 1.7284, 1.8542, 2.1436, 1.9265,\n",
      "         1.8632],\n",
      "        [2.0256, 1.9454, 2.0582, 2.1826, 2.0182, 1.9327, 1.6940, 2.1528, 2.2120,\n",
      "         2.0304],\n",
      "        [2.0118, 2.0920, 2.0064, 2.0797, 2.0530, 2.1851, 2.1539, 1.8720, 2.1320,\n",
      "         2.0046],\n",
      "        [2.0406, 2.0438, 1.8917, 1.9117, 2.0162, 1.8650, 1.9995, 2.0295, 1.7047,\n",
      "         1.9973],\n",
      "        [2.4045, 2.3454, 2.4575, 2.3571, 2.1678, 2.2607, 2.4808, 2.2001, 2.2883,\n",
      "         2.1678]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 60])\n",
      "aligned_diff[0].shape == torch.Size([70, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, N_MODELS):\n",
    "    _, _, T_vars = get_wassersteinized_layers_modularized(args, [models['untrained'][i - 1], models['untrained'][i]])\n",
    "    _, aligned_untrained, _ = get_wassersteinized_layers_modularized(\n",
    "        args, \n",
    "        [models['trained'][i - 1], models['untrained'][i]], \n",
    "        T_vars_pre_computed=T_vars\n",
    "    )\n",
    "    model = Net(weights=aligned_untrained)\n",
    "    models['aligned_untrained'][i] = model.to('cpu')\n",
    "    print(f'aligned_untrained[0].shape == {aligned_untrained[0].shape}')\n",
    "    print(f\"models['aligned_untrained'][i].fc1.weight.shape == {models['aligned_untrained'][i].fc1.weight.shape}\")\n",
    "\n",
    "    models['trained_short'][i].to(device)\n",
    "    _, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][i - 1], models['trained_short'][i]])\n",
    "    model = Net(weights=aligned)\n",
    "    models['aligned'][i] = model.to('cpu')\n",
    "    print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "    print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "\n",
    "    _, aligned_diff, _ = get_wassersteinized_layers_modularized(\n",
    "        args, \n",
    "        [\n",
    "            models['trained'][i - 1] - models['untrained'][i - 1], \n",
    "            models['untrained'][i]\n",
    "        ], \n",
    "        T_vars_pre_computed=T_vars\n",
    "    )\n",
    "    model = Net(weights=aligned_diff) + models['untrained'][i]\n",
    "    models['aligned_diff'][i] = model.to('cpu')\n",
    "    print(f'aligned_diff[0].shape == {aligned_diff[0].shape}')\n",
    "    print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== MODEL 0 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3028, Accuracy: 914/10000 (9%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1582, Accuracy: 9505/10000 (95%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3898, Accuracy: 8925/10000 (89%)\n",
      "\n",
      "Aligned untrained:\n",
      "------------\n",
      "Aligned:\n",
      "------------\n",
      "Aligned diff:\n",
      "------------\n",
      "=============== MODEL 1 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3038, Accuracy: 826/10000 (8%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1369, Accuracy: 9582/10000 (96%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3825, Accuracy: 8942/10000 (89%)\n",
      "\n",
      "Aligned untrained:\n",
      "\n",
      "Validation set: Average loss: 9.8564, Accuracy: 956/10000 (10%)\n",
      "\n",
      "Aligned:\n",
      "\n",
      "Validation set: Average loss: 0.1874, Accuracy: 9456/10000 (95%)\n",
      "\n",
      "Aligned diff:\n",
      "\n",
      "Validation set: Average loss: 0.2269, Accuracy: 9324/10000 (93%)\n",
      "\n",
      "=============== MODEL 2 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3001, Accuracy: 1218/10000 (12%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1234, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3580, Accuracy: 8997/10000 (90%)\n",
      "\n",
      "Aligned untrained:\n",
      "\n",
      "Validation set: Average loss: 8.3128, Accuracy: 1180/10000 (12%)\n",
      "\n",
      "Aligned:\n",
      "\n",
      "Validation set: Average loss: 0.1650, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "Aligned diff:\n",
      "\n",
      "Validation set: Average loss: 0.2216, Accuracy: 9322/10000 (93%)\n",
      "\n",
      "=============== MODEL 3 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.2999, Accuracy: 1210/10000 (12%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1115, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3481, Accuracy: 8999/10000 (90%)\n",
      "\n",
      "Aligned untrained:\n",
      "\n",
      "Validation set: Average loss: 8.9730, Accuracy: 960/10000 (10%)\n",
      "\n",
      "Aligned:\n",
      "\n",
      "Validation set: Average loss: 0.1504, Accuracy: 9550/10000 (96%)\n",
      "\n",
      "Aligned diff:\n",
      "\n",
      "Validation set: Average loss: 0.2082, Accuracy: 9355/10000 (94%)\n",
      "\n",
      "=============== MODEL 4 ==================\n",
      "Untrained:\n",
      "\n",
      "Validation set: Average loss: 2.3024, Accuracy: 810/10000 (8%)\n",
      "\n",
      "Trained:\n",
      "\n",
      "Validation set: Average loss: 0.1060, Accuracy: 9677/10000 (97%)\n",
      "\n",
      "Trained short:\n",
      "\n",
      "Validation set: Average loss: 0.3412, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Aligned untrained:\n",
      "\n",
      "Validation set: Average loss: 7.7418, Accuracy: 1817/10000 (18%)\n",
      "\n",
      "Aligned:\n",
      "\n",
      "Validation set: Average loss: 0.1348, Accuracy: 9593/10000 (96%)\n",
      "\n",
      "Aligned diff:\n",
      "\n",
      "Validation set: Average loss: 0.2058, Accuracy: 9375/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_MODELS):\n",
    "    print(f'=============== MODEL {i} ==================')\n",
    "    print('Untrained:')\n",
    "    validate(models['untrained'][i].to(device), criterion, [], [])\n",
    "    print('Trained:')\n",
    "    validate(models['trained'][i].to(device), criterion, [], [])\n",
    "    print('Trained short:')\n",
    "    validate(models['trained_short'][i].to(device), criterion, [], [])\n",
    "    print('Aligned untrained:')\n",
    "    if i in models['aligned_untrained']:\n",
    "        validate(models['aligned_untrained'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')\n",
    "    print('Aligned:')\n",
    "    if i in models['aligned']:\n",
    "        validate(models['aligned'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')\n",
    "    print('Aligned diff:')\n",
    "    if i in models['aligned_diff']:\n",
    "        validate(models['aligned_diff'][i].to(device), criterion, [], [])\n",
    "    else:\n",
    "        print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0143, 0.0048]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0143, device='cuda:0')\n",
      "Here, trace is 0.9999929666519165 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([70, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([70, 70])\n",
      "shape of previous transport map torch.Size([30, 70])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.9133, 1.7866, 1.9405,  ..., 2.1318, 2.0570, 2.0010],\n",
      "        [2.2928, 2.1357, 2.2234,  ..., 2.4168, 2.1229, 2.1353],\n",
      "        [1.8844, 1.9328, 1.7738,  ..., 2.2251, 1.7927, 1.9887],\n",
      "        ...,\n",
      "        [2.5006, 2.4664, 2.5562,  ..., 2.8419, 2.3393, 2.2710],\n",
      "        [2.5521, 2.3163, 2.4896,  ..., 2.7097, 2.4104, 2.3744],\n",
      "        [2.4212, 2.2318, 2.6864,  ..., 2.3370, 2.2996, 2.4441]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  70\n",
      "the transport map is  tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([70])\n",
      "inverse marginals beta is  tensor([0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143,\n",
      "        0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143, 0.0143],\n",
      "       device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0143, device='cuda:0')\n",
      "Here, trace is 0.9999930262565613 and matrix sum is 69.99951171875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([70, 70])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 70])\n",
      "shape of previous transport map torch.Size([30, 70])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[2.9681, 4.5834, 3.7672, 3.9326, 4.0008, 3.7608, 3.7236, 4.0662, 4.1481,\n",
      "         4.1345],\n",
      "        [3.9406, 3.0685, 3.8594, 3.9066, 4.0134, 3.7055, 3.8196, 3.6532, 3.8412,\n",
      "         3.8124],\n",
      "        [3.3598, 3.2340, 2.5275, 3.3556, 3.5619, 3.6779, 2.8517, 3.4055, 3.4207,\n",
      "         3.6725],\n",
      "        [3.6557, 2.9548, 2.9428, 2.7287, 3.8354, 3.5989, 3.7496, 3.2209, 3.2817,\n",
      "         3.4987],\n",
      "        [4.2475, 3.9786, 3.9095, 4.0910, 2.9067, 3.6647, 3.6319, 3.7638, 3.6238,\n",
      "         3.1908],\n",
      "        [2.8728, 3.3663, 3.3949, 2.9098, 3.3211, 2.4572, 3.2667, 3.5286, 3.0630,\n",
      "         3.2234],\n",
      "        [3.4457, 3.9551, 3.8521, 4.0409, 3.4331, 3.1252, 2.7924, 4.3098, 3.7857,\n",
      "         4.0080],\n",
      "        [3.8335, 3.5764, 3.5977, 3.4867, 3.7561, 3.8963, 3.9944, 2.7438, 3.9542,\n",
      "         3.5273],\n",
      "        [3.3601, 3.7399, 3.3287, 3.5233, 3.8119, 3.3212, 3.5436, 3.8458, 2.6605,\n",
      "         3.4244],\n",
      "        [3.9923, 3.9806, 3.8073, 3.4252, 3.4070, 3.6618, 4.3515, 3.0242, 3.3937,\n",
      "         2.8141]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 70])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([70, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "\n",
      "Validation set: Average loss: 0.4389, Accuracy: 9477/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained_short'][4]])\n",
    "model = Net(weights=aligned)\n",
    "print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.294997\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.847028\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.607638\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.529232\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.483924\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.266716\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.481619\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.339755\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.449372\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.370931\n",
      "\n",
      "Validation set: Average loss: 0.3060, Accuracy: 9156/10000 (92%)\n",
      "\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0010, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([1000, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([1000, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[7.3297, 7.2995, 7.3031,  ..., 7.3083, 7.2919, 7.3267],\n",
      "        [8.3588, 8.3629, 8.2879,  ..., 8.3670, 8.3703, 8.3684],\n",
      "        [7.0789, 7.0735, 7.0433,  ..., 7.0303, 7.0474, 7.0612],\n",
      "        ...,\n",
      "        [9.4452, 9.3633, 9.4030,  ..., 9.3579, 9.3288, 9.4104],\n",
      "        [9.7097, 9.7432, 9.6230,  ..., 9.6819, 9.6902, 9.7332],\n",
      "        [9.0655, 9.1174, 8.9704,  ..., 9.0810, 9.0538, 9.0654]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0010, device='cuda:0')\n",
      "Here, trace is 0.9998999834060669 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([1000, 1000])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[13.5453, 14.7499, 14.1615, 14.4284, 14.5511, 14.1955, 14.2287, 14.2663,\n",
      "         14.4052, 14.3704],\n",
      "        [14.5993, 13.5518, 14.3806, 14.2458, 14.4361, 14.3378, 14.2301, 14.1356,\n",
      "         14.2430, 14.3179],\n",
      "        [11.8683, 11.7628, 11.0733, 11.7441, 12.0601, 12.2338, 11.7702, 11.8221,\n",
      "         11.9232, 12.1498],\n",
      "        [12.4505, 12.0432, 12.0584, 11.5702, 12.6128, 12.2578, 12.7466, 12.1993,\n",
      "         12.3141, 12.3585],\n",
      "        [14.1377, 13.8631, 14.0275, 14.0954, 12.9968, 13.9119, 13.6040, 13.9200,\n",
      "         13.8142, 13.5925],\n",
      "        [11.3239, 11.7454, 11.8089, 11.4441, 11.6948, 10.8723, 11.4139, 11.8319,\n",
      "         11.4959, 11.6063],\n",
      "        [13.4955, 13.7008, 13.6870, 14.0701, 13.4939, 13.4664, 12.9534, 14.0489,\n",
      "         13.6411, 13.8627],\n",
      "        [13.9633, 13.9177, 13.7713, 13.7893, 13.9532, 14.1880, 14.2343, 13.1309,\n",
      "         14.1194, 13.7800],\n",
      "        [12.6808, 12.5479, 12.6994, 12.6668, 12.8520, 12.5280, 12.6590, 13.0206,\n",
      "         12.1209, 12.6964],\n",
      "        [13.2163, 13.3934, 13.5195, 13.1096, 12.9063, 13.1963, 13.5745, 12.8281,\n",
      "         13.0367, 12.5668]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([1000, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "\n",
      "Validation set: Average loss: 98.5679, Accuracy: 9505/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(hidden_size=1000).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, N_EPOCHS_SHORT + 1):\n",
    "    train(model, optimizer, criterion, epoch)\n",
    "    validate(model, criterion, lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0010, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 999.8999633789062 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([1000, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([1000, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[11.6144,  7.6671, 12.2352,  ..., 10.9345, 11.1916, 11.2013],\n",
      "        [13.3201, 10.2047, 10.5064,  ..., 15.0106,  9.9551, 12.4098],\n",
      "        [11.0775,  8.9430, 11.1173,  ...,  9.7651,  8.5255, 11.4466],\n",
      "        ...,\n",
      "        [15.5746, 10.6499, 12.4656,  ..., 13.0302,  9.1234, 13.0301],\n",
      "        [14.4431, 12.8314,  8.8450,  ..., 14.8244,  8.7800, 13.8372],\n",
      "        [12.6050, 11.5854,  0.4547,  ..., 14.5230,  7.7296,  9.3355]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  1000\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([1000])\n",
      "inverse marginals beta is  tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010], device='cuda:0')\n",
      "tensor([0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
      "        0.9999], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0010, device='cuda:0')\n",
      "Here, trace is 0.9998999834060669 and matrix sum is 999.9000244140625 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([1000, 1000])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 1000])\n",
      "shape of previous transport map torch.Size([30, 1000])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[ 0.7296, 23.7098, 17.4732, 21.3558, 23.1320, 16.8618, 17.1188, 20.1153,\n",
      "         20.6236, 20.9616],\n",
      "        [23.7381,  0.6563, 18.8590, 18.5451, 20.6616, 19.6587, 19.8371, 19.6288,\n",
      "         19.2648, 20.9945],\n",
      "        [17.5090, 18.8355,  0.4540, 16.2160, 19.3809, 20.3023, 17.4286, 16.3006,\n",
      "         18.3245, 21.7112],\n",
      "        [21.3538, 18.5295, 16.1915,  0.5670, 20.7370, 16.0957, 23.3841, 17.8545,\n",
      "         17.2375, 16.7230],\n",
      "        [23.1650, 20.6793, 19.3977, 20.7205,  0.6966, 19.9494, 17.4266, 20.6625,\n",
      "         20.4874, 16.8482],\n",
      "        [16.8583, 19.6519, 20.2922, 16.0814, 19.9503,  0.5636, 16.4839, 21.3138,\n",
      "         16.5706, 18.1402],\n",
      "        [17.1644, 19.8234, 17.4463, 23.3776, 17.3907, 16.4790,  0.6796, 22.8848,\n",
      "         18.7729, 23.6319],\n",
      "        [20.1346, 19.6330, 16.3026, 17.8877, 20.6598, 21.3221, 22.8755,  0.7074,\n",
      "         21.9352, 17.3594],\n",
      "        [20.6154, 19.2907, 18.3294, 17.2280, 20.5196, 16.5568, 18.7661, 21.9439,\n",
      "          0.6802, 17.2181],\n",
      "        [20.9748, 21.0355, 21.7164, 16.7332, 16.8681, 18.1453, 23.6274, 17.3785,\n",
      "         17.1974,  0.6259]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 1000])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n",
      "aligned[0].shape == torch.Size([1000, 784])\n",
      "models['trained'][i].fc1.weight.shape == torch.Size([70, 784])\n",
      "\n",
      "Validation set: Average loss: 99.5590, Accuracy: 9501/10000 (95%)\n",
      "\n",
      "average[0].shape == torch.Size([1000, 784])\n",
      "\n",
      "Validation set: Average loss: 98.8879, Accuracy: 9506/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average, aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], model])\n",
    "model = Net(weights=aligned)\n",
    "print(f'aligned[0].shape == {aligned[0].shape}')\n",
    "print(f\"models['trained'][i].fc1.weight.shape == {models['trained'][i].fc1.weight.shape}\")\n",
    "validate(model.to(device), criterion, [], [])\n",
    "model = Net(weights=average)\n",
    "print(f'average[0].shape == {average[0].shape}')\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0250],\n",
      "        ...,\n",
      "        [0.0000, 0.0250, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0250, device='cuda:0')\n",
      "Here, trace is 0.9999960064888 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 784])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 784])\n",
      "Previous layer shape is  torch.Size([40, 784])\n",
      "shape of layer: model 0 torch.Size([30, 30])\n",
      "shape of layer: model 1 torch.Size([40, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([30, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.5175, 1.2648, 1.3460,  ..., 1.9774, 1.6739, 2.0909],\n",
      "        [2.0575, 1.8461, 1.7188,  ..., 1.4778, 1.3777, 2.1506],\n",
      "        [2.6075, 2.3140, 2.0627,  ..., 2.4417, 2.2204, 1.4178],\n",
      "        ...,\n",
      "        [2.2927, 1.7001, 1.6253,  ..., 2.4608, 1.9484, 1.5914],\n",
      "        [1.9489, 1.8061, 2.0693,  ..., 2.3337, 1.8796, 1.9879],\n",
      "        [1.9503, 2.2265, 2.1172,  ..., 1.3048, 1.6413, 1.8783]],\n",
      "       device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  30\n",
      "returns a uniform measure of cardinality:  40\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([40])\n",
      "inverse marginals beta is  tensor([0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250,\n",
      "        0.0250, 0.0250, 0.0250, 0.0250], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0500, device='cuda:0')\n",
      "Here, trace is 1.9999920129776 and matrix sum is 39.999839782714844 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([30, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([30, 30])\n",
      "Previous layer shape is  torch.Size([40, 40])\n",
      "shape of layer: model 0 torch.Size([10, 30])\n",
      "shape of layer: model 1 torch.Size([10, 40])\n",
      "shape of previous transport map torch.Size([30, 40])\n",
      "torch.Size([10, 30])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.7368, 4.2591, 3.1290, 3.4840, 3.9956, 3.0599, 3.0535, 3.3443, 3.3063,\n",
      "         3.5534],\n",
      "        [3.8920, 2.5985, 2.6589, 2.7885, 2.8614, 3.4302, 3.1944, 3.5126, 3.3150,\n",
      "         3.4946],\n",
      "        [2.7467, 2.9292, 1.7334, 2.7244, 3.2027, 3.5126, 3.4180, 2.4203, 3.1560,\n",
      "         3.5093],\n",
      "        [3.6472, 3.1527, 2.9823, 1.5480, 3.6422, 3.3863, 4.3974, 2.8970, 3.2467,\n",
      "         2.9491],\n",
      "        [3.8923, 3.7429, 3.7614, 4.0709, 1.9277, 3.3013, 3.0625, 4.0600, 3.3201,\n",
      "         3.3029],\n",
      "        [3.2568, 3.1743, 3.4380, 2.5289, 3.5912, 1.8549, 3.2143, 3.3924, 3.1074,\n",
      "         3.5433],\n",
      "        [3.4618, 3.5457, 3.4434, 3.9705, 3.4602, 2.7300, 2.2263, 4.2631, 3.4743,\n",
      "         4.2588],\n",
      "        [3.4181, 3.2413, 3.1034, 3.1516, 3.3882, 3.9777, 3.8931, 2.0674, 4.0258,\n",
      "         2.9565],\n",
      "        [3.5529, 3.2944, 2.8914, 2.6681, 3.2969, 2.9420, 3.6392, 3.7264, 1.9799,\n",
      "         3.4264],\n",
      "        [3.6572, 4.0661, 3.8619, 3.3406, 2.7500, 3.0941, 3.9105, 3.3191, 3.2389,\n",
      "         1.4859]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "shape of inverse marginals beta is  torch.Size([10])\n",
      "inverse marginals beta is  tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0')\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 40])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 30])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aligned, T_vars = get_wassersteinized_layers_modularized(args, [models['trained'][0], models['trained'][1]])\n",
    "\n",
    "for T_var in T_vars:\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(T_var.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 0.1756, Accuracy: 9445/10000 (94%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.3049, Accuracy: 968/10000 (10%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.3041, Accuracy: 773/10000 (8%)\n",
      "\n",
      "\n",
      "Validation set: Average loss: 2.4477, Accuracy: 1905/10000 (19%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net(40)\n",
    "validate(models['trained'][0].to(device), criterion, [], [])\n",
    "validate(models['untrained'][1].to(device), criterion, [], [])\n",
    "validate(model.to(device), criterion, [], [])\n",
    "with torch.no_grad():\n",
    "    model.fc1.weight = nn.Parameter(aligned[0])\n",
    "    model.fc2.weight = nn.Parameter(aligned[1])\n",
    "    model.fc3.weight = nn.Parameter(aligned[2])\n",
    "validate(model.to(device), criterion, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bdc725fb65c21f82e45edbbae76c938f412b1b7259c22cf88bbbf1e62e294f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
